{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h2>Googlenet Compression</h2>\n\n<p>Training Googlenet on CIFAR-100, and applying Deep Compression.<p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch import optim\nfrom torchvision import datasets,transforms, models\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image\nimport os\nimport sys\nimport argparse\nimport time\nfrom torch.autograd import Variable\nfrom torch.optim.lr_scheduler import _LRScheduler\nimport re\nimport math\nfrom collections import Iterable\nfrom itertools import islice","execution_count":31,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Hyperparameter Settings</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"batchsize = 128\nl_r = 0.1\nw = 1","execution_count":47,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Loading and preprocessing data:</h3>\n<p>Let us calculate the mean and standard deviation of the dataset. We'll use this in the transforms later.</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_mean_std(dataset):\n    \n    data_r = np.dstack([np.array(im)[:, :, 0]/255 for im,label in dataset])\n    data_g = np.dstack([np.array(im)[:, :, 1]/255 for im,label in dataset])\n    data_b = np.dstack([np.array(im)[:, :, 2]/255 for im,label in dataset])\n    \n    mean = [np.asscalar(np.mean(data_r)), np.asscalar(np.mean(data_g)), np.asscalar(np.mean(data_b))]\n    std = [np.asscalar(np.std(data_r)), np.asscalar(np.std(data_g)), np.asscalar(np.std(data_b))]\n    \n    return mean,std","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#cifar_norm_train=datasets.CIFAR100(data_dir, train=True, transform=None, target_transform=None, download=False)\n#cifar_norm_test=datasets.CIFAR100(data_dir, train=False, transform=None, target_transform=None, download=False)\n#train_mean,train_std = compute_mean_std(cifar_norm_train)\n#test_mean, test_std = compute_mean_std(cifar_norm_test)\n#print(train_mean,train_std)\n#print(test_mean,test_std)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#No need to calculate this multiple times, by saving the mean/std values\ntrain_mean,train_std = [0.5070751592371341, 0.48654887331495067, 0.4409178433670344],[0.2673342858792403, 0.2564384629170882, 0.27615047132568393]\ntest_mean, test_std = [0.508796412760417, 0.48739301317401906, 0.4419422112438727],[0.2682515741720801, 0.25736373644781246, 0.2770957707973041]","execution_count":34,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Padding with 4 zeros, taking 32 by 32 crops, random flipping and normalization by values found above."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transforms = transforms.Compose([transforms.Pad(4,fill=0),\n                                     transforms.RandomResizedCrop(32),\n                                     transforms.RandomHorizontalFlip(),\n                                     transforms.ToTensor(),\n                                     transforms.Normalize(train_mean,train_std)])\n\ntest_transforms = transforms.Compose([transforms.ToTensor(),\n                                     transforms.Normalize(test_mean,test_std)])","execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dir='../input/cifar-100-python/'\ncifar_train=datasets.CIFAR100(data_dir, train=True, transform=train_transforms, target_transform=None, download=False)\ncifar_test=datasets.CIFAR100(data_dir, train=False, transform=test_transforms, target_transform=None, download=False)","execution_count":36,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainloader = torch.utils.data.DataLoader(cifar_train, batch_size=batchsize, shuffle=True)\ntestloader = torch.utils.data.DataLoader(cifar_test, batch_size=batchsize, shuffle=True)","execution_count":37,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Visualizing CIFAR</h3>\n<p>Just to see what's actually in there.</p>"},{"metadata":{"trusted":false},"cell_type":"code","source":"#for inputs,labels in testloader:\n#    break\n#images = inputs.numpy()\n#for i in range(3):\n#    plt.figure()\n#    plt.imshow(images[i].T)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Utils</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def iter_str_every(iterable, k):\n    \"\"\"\n    :param iterable:\n    :param k:\n    :return:\n    \"\"\"\n    i = iter(iterable)\n    piece = ''.join(islice(i, k))\n    while piece:\n        yield piece\n        piece = ''.join(islice(i, k))\n\n\ndef get_sparsity(param):\n    \"\"\"\n    :param param:\n    :return:\n    \"\"\"\n    mask = param.eq(0)\n    return float(mask.sum()) / mask.numel()\n\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        if self.count > 0:\n            self.avg = self.sum / self.count\n\n    def accumulate(self, val, n=1):\n        self.sum += val\n        self.count += n\n        if self.count > 0:\n            self.avg = self.sum / self.count\n\n\nclass Logger(object):\n    def __init__(self, file_path):\n        \"\"\"\n        write log to file\n        :param file_path: str, path to the file\n        \"\"\"\n        self.f = open(file_path, 'w')\n        self.fid = self.f.fileno()\n        self.filepath = file_path\n\n    def close(self):\n        \"\"\"\n        close log file\n        :return:\n        \"\"\"\n        return self.f.close()\n\n    def flush(self):\n        self.f.flush()\n        os.fsync(self.fid)\n\n    def write(self, content, wrap=True, flush=False, verbose=False):\n        \"\"\"\n        write file and flush buffer to the disk\n        :param content: str\n        :param wrap: bool, whether to add '\\n' at the end of the content\n        :param flush: bool, whether to flush buffer to the disk, default=False\n        :param verbose: bool, whether to print the content, default=False\n        :return:\n            void\n        \"\"\"\n        if verbose:\n            print(content)\n        if wrap:\n            content += \"\\n\"\n        self.f.write(content)\n        if flush:\n            self.f.flush()\n            os.fsync(self.fid)\n\n\nclass StageScheduler(object):\n\n    def __init__(self, max_num_stage, stage_step=45):\n        \"\"\"\n        :param max_num_stage:\n        :param stage_step:\n        \"\"\"\n        self.max_num_stage = max_num_stage\n\n        self.stage_step = stage_step\n        if isinstance(stage_step, int):\n            self.stage_step = [stage_step] * max_num_stage\n        if isinstance(stage_step, str):\n            self.stage_step = list(map(int, stage_step.split(',')))\n        assert isinstance(self.stage_step, list)\n\n        num_stage = len(self.stage_step)\n        if num_stage < self.max_num_stage:\n            for i in range(self.max_num_stage - num_stage):\n                self.stage_step.append(self.stage_step[num_stage - 1])\n        elif num_stage > self.max_num_stage:\n            self.max_num_stage = num_stage\n        assert len(self.stage_step) == self.max_num_stage\n\n        for i in range(1, self.max_num_stage):\n            self.stage_step[i] += self.stage_step[i - 1]\n\n    def step(self, epoch):\n        \"\"\"\n        :param epoch:\n        :return:\n        \"\"\"\n        stage = self.max_num_stage - 1\n        for i, max_epoch in enumerate(self.stage_step):\n            if epoch < max_epoch:\n                stage = i\n                break\n        if stage > 0:\n            epoch -= self.stage_step[stage - 1]\n        return stage, epoch","execution_count":38,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Pruning</h2>"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"def prune_vanilla_elementwise(param, sparsity, fn_importance=lambda x: x.abs()):\n    \"\"\"\n    element-wise vanilla pruning\n    :param param: torch.(cuda.)Tensor, weight of conv/fc layer\n    :param sparsity: float, pruning sparsity\n    :param fn_importance: function, inputs 'param' and returns the importance of\n                                    each position in 'param',\n                                    default=lambda x: x.abs()\n    :return:\n        torch.(cuda.)ByteTensor, mask for zeros\n    \"\"\"\n    sparsity = min(max(0.0, sparsity), 1.0)\n    if sparsity == 1.0:\n        return torch.zeros_like(param).byte()\n    num_el = param.numel()\n    importance = fn_importance(param)\n    num_pruned = int(math.ceil(num_el * sparsity))\n    num_stayed = num_el - num_pruned\n    if sparsity <= 0.5:\n        _, topk_indices = torch.topk(importance.view(num_el), k=num_pruned,\n                                     dim=0, largest=False, sorted=False)\n        mask = torch.zeros_like(param).byte()\n        param.view(num_el).index_fill_(0, topk_indices, 0)\n        mask.view(num_el).index_fill_(0, topk_indices, 1)\n    else:\n        thr = torch.min(torch.topk(importance.view(num_el), k=num_stayed,\n                                   dim=0, largest=True, sorted=False)[0])\n        mask = torch.lt(importance, thr)\n        param.masked_fill_(mask, 0)\n    return mask\n\n\ndef prune_vanilla_kernelwise(param, sparsity, fn_importance=lambda x: x.norm(1, -1)):\n    \"\"\"\n    kernel-wise vanilla pruning, the importance determined by L1 norm\n    :param param: torch.(cuda.)Tensor, weight of conv/fc layer\n    :param sparsity: float, pruning sparsity\n    :param fn_importance: function, inputs 'param' as size (param.size(0) * param.size(1), -1) and\n                                    returns the importance of each kernel in 'param',\n                                    default=lambda x: x.norm(1, -1)\n    :return:\n        torch.(cuda.)ByteTensor, mask for zeros\n    \"\"\"\n    assert param.dim() >= 3\n    sparsity = min(max(0.0, sparsity), 1.0)\n    if sparsity == 1.0:\n        return torch.zeros_like(param).byte()\n    num_kernels = param.size(0) * param.size(1)\n    param_k = param.view(num_kernels, -1)\n    param_importance = fn_importance(param_k)\n    num_pruned = int(math.ceil(num_kernels * sparsity))\n    _, topk_indices = torch.topk(param_importance, k=num_pruned,\n                                 dim=0, largest=False, sorted=False)\n    mask = torch.zeros_like(param).byte()\n    mask_k = mask.view(num_kernels, -1)\n    param_k.index_fill_(0, topk_indices, 0)\n    mask_k.index_fill_(0, topk_indices, 1)\n    return mask\n\n\ndef prune_vanilla_filterwise(sparsity, param, fn_importance=lambda x: x.norm(1, -1)):\n    \"\"\"\n    filter-wise vanilla pruning, the importance determined by L1 norm\n    :param param: torch.(cuda.)Tensor, weight of conv/fc layer\n    :param sparsity: float, pruning sparsity\n    :param fn_importance: function, inputs 'param' as size (param.size(0), -1) and\n                                returns the importance of each filter in 'param',\n                                default=lambda x: x.norm(1, -1)\n    :return:\n        torch.(cuda.)ByteTensor, mask for zeros\n    \"\"\"\n    assert param.dim() >= 3\n    sparsity = min(max(0.0, sparsity), 1.0)\n    if sparsity == 1.0:\n        return torch.zeros_like(param).byte()\n    num_filters = param.size(0)\n    param_k = param.view(num_filters, -1)\n    param_importance = fn_importance(param_k)\n    num_pruned = int(math.ceil(num_filters * sparsity))\n    _, topk_indices = torch.topk(param_importance, k=num_pruned,\n                                 dim=0, largest=False, sorted=False)\n    mask = torch.zeros_like(param).byte()\n    mask_k = mask.view(num_filters, -1)\n    param_k.index_fill_(0, topk_indices, 0)\n    mask_k.index_fill_(0, topk_indices, 1)\n    return mask\n\n\nclass VanillaPruner(object):\n\n    def __init__(self, rule=None):\n        \"\"\"\n        Pruner Class for Vanilla Pruning Method\n        :param rule: str, path to the rule file, each line formats\n                          'param_name granularity sparsity_stage_0, sparstiy_stage_1, ...'\n                     list of tuple, [(param_name(str), granularity(str),\n                                      sparsity(float) or [sparsity_stage_0(float), sparstiy_stage_1,],\n                                      fn_importance(optional, str or function))]\n                     'granularity': str, choose from ['element', 'kernel', 'filter']\n                     'fn_importance': str, choose from ['abs', 'l1norm', 'l2norm']\n        \"\"\"\n        if rule:\n            if isinstance(rule, str):\n                content = map(lambda x: x.split(), open(rule).readlines())\n                content = filter(lambda x: len(x) == 3, content)\n                rule = list(map(lambda x: (x[0], x[1], list(map(float, x[2].split(',')))), content))\n            for r in rule:\n                if not isinstance(r[2], Iterable):\n                    assert isinstance(r[2], float) or isinstance(r[2], int)\n                    r[2] = [float(r[2])]\n                if len(r) == 3:\n                    r.append('default')\n                granularity = r[1]\n                if granularity == 'element':\n                    r.append(prune_vanilla_elementwise)\n                elif granularity == 'kernel':\n                    r.append(prune_vanilla_kernelwise)\n                elif granularity == 'filter':\n                    r.append(prune_vanilla_filterwise)\n                else:\n                    raise NotImplementedError\n\n        self.rule = rule\n\n        self.masks = dict()\n\n        print(\"=\" * 89)\n        if self.rule:\n            print(\"Initializing Vanilla Pruner with rules:\")\n            for r in self.rule:\n                print(r[:-1])\n        else:\n            print(\"Initializing Vanilla Pruner WITHOUT rules\")\n        print(\"=\" * 89)\n\n    def load_state_dict(self, state_dict, replace_rule=True):\n        \"\"\"\n        Recover Pruner\n        :param state_dict: dict, a dictionary containing a whole state of the Pruner\n        :param replace_rule: bool, whether to use rule settings in 'state_dict'\n        :return: VanillaPruner\n        \"\"\"\n        if replace_rule:\n            self.rule = state_dict['rule']\n            for r in self.rule:\n                granularity = r[1]\n                if granularity == 'element':\n                    r.append(prune_vanilla_elementwise)\n                elif granularity == 'kernel':\n                    r.append(prune_vanilla_kernelwise)\n                elif granularity == 'filter':\n                    r.append(prune_vanilla_filterwise)\n                else:\n                    raise NotImplementedError\n        self.masks = state_dict['masks']\n        print(\"=\" * 89)\n        print(\"Customizing Vanilla Pruner with rules:\")\n        for r in self.rule:\n            print(r[:-1])\n        print(\"=\" * 89)\n\n    def state_dict(self):\n        \"\"\"\n        Returns a dictionary containing a whole state of the Pruner\n        :return: dict, a dictionary containing a whole state of the Pruner\n        \"\"\"\n        state_dict = dict()\n        state_dict['rule'] = [r[:-1] for r in self.rule]\n        state_dict['masks'] = self.masks\n        return state_dict\n\n    def prune_param(self, param, param_name, stage=0, verbose=False):\n        \"\"\"\n        prune parameter\n        :param param: torch.(cuda.)tensor\n        :param param_name: str, name of param\n        :param stage: int, the pruning stage, default=0\n        :param verbose: bool, whether to print the pruning details\n        :return:\n            torch.(cuda.)ByteTensor, mask for zeros\n        \"\"\"\n        rule_id = -1\n        #for idx, r in enumerate(self.rule):\n        #    m = re.match(r[0], param_name)\n        #    if m is not None and len(param_name) == m.span()[1]:\n        #        rule_id = idx\n        #        break\n        if rule_id > -1:\n            sparsity = self.rule[rule_id][2][stage]\n            fn_prune = self.rule[rule_id][-1]\n            fn_importance = self.rule[rule_id][3]\n            if verbose:\n                print(\"{param_name:^30} | {stage:5d} | {spars:.3f}\".\n                      format(param_name=param_name, stage=stage, spars=sparsity))\n            if fn_importance is None or fn_importance == 'default':\n                mask = fn_prune(param=param, sparsity=sparsity)\n            elif fn_importance == 'abs':\n                mask = fn_prune(param=param, sparsity=sparsity, fn_importance=lambda x: x.abs())\n            elif fn_importance == 'l1norm':\n                mask = fn_prune(param=param, sparsity=sparsity, fn_importance=lambda x: x.norm(1, -1))\n            elif fn_importance == 'l2norm':\n                mask = fn_prune(param=param, sparsity=sparsity, fn_importance=lambda x: x.norm(2, -1))\n            else:\n                mask = fn_prune(param=param, sparsity=sparsity, fn_importance=fn_importance)\n            return mask\n        else:\n            if verbose:\n                print(\"{param_name:^30} | skipping\".format(param_name=param_name))\n            return None\n\n    def prune(self, model, stage=0, update_masks=False, verbose=False):\n        \"\"\"\n        prune models\n        :param model: torch.nn.Module\n        :param stage: int, the pruning stage, default=0\n        :param update_masks: bool, whether update masks\n        :param verbose: bool, whether to print the pruning details\n        :return:\n            void\n        \"\"\"\n        update_masks = True if update_masks or len(self.masks) == 0 else False\n        if verbose:\n            print(\"=\" * 89)\n            print(\"Pruning Models\")\n            if len(self.masks) == 0:\n                print(\"Initializing Masks\")\n            elif update_masks:\n                print(\"Updating Masks\")\n            print(\"=\" * 89)\n            print(\"{name:^30} | stage | sparsity\".format(name='param_name'))\n        for param_name, param in model.named_parameters():\n            if 'AuxLogits' not in param_name:\n                # deal with googlenet\n                if param.dim() > 1:\n                    if update_masks:\n                        mask = self.prune_param(param=param.data, param_name=param_name,\n                                                stage=stage, verbose=verbose)\n                        if mask is not None:\n                            self.masks[param_name] = mask\n                    else:\n                        if param_name in self.masks:\n                            mask = self.masks[param_name]\n                            param.data.masked_fill_(mask, 0)\n        if verbose:\n            print(\"=\" * 89)","execution_count":53,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Googlenet</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"net = models.googlenet()","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_prec1 = 0\nnet = torch.nn.DataParallel(net, device_ids=list(range(1))).cuda()","execution_count":41,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" criterion = nn.CrossEntropyLoss().cuda()","execution_count":42,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\npruner = VanillaPruner()\ninput_size = 224","execution_count":55,"outputs":[{"output_type":"stream","text":"=========================================================================================\nInitializing Vanilla Pruner WITHOUT rules\n=========================================================================================\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def validate(val_loader, model, criterion, epoch):\n    # switch to evaluate mode\n    model.eval()\n    print(\"=\" * 89)\n\n    with torch.no_grad():\n        for i, (input, target) in enumerate(val_loader):\n            target = target.cuda(non_blocking=True)\n\n            # compute output\n            output = model(input)\n            loss = criterion(output.logits, target)\n            test_loss += loss.item()\n            # measure accuracy and record loss\n            _, preds = output.max(1)\n            del output\n            x= preds.eq(labels).sum()\n            print(x)\n            correct = correct+x\n            \n            losses.update(loss.item(), input.size(0))\n            top1.update(prec1[0], input.size(0))\n        k=k+1\n        if k%10 == 0:\n            print(k)\n    print(\"=\" * 89)\n    print('Test set: Average loss: {:.4f}, Accuracy: {:.4f}'.format(\n        test_loss / len(trainloader.dataset),\n        correct.float() / len(testloader.dataset)\n    ))\n    return correct.float() / len(testloader.dataset)","execution_count":79,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(train_loader, model, criterion, optimizer, pruner, epoch,stage):\n    # switch to train mode\n    model.train()\n    print(\"=\" * 89)\n    \n    for batch_index, (input, target) in enumerate(train_loader):\n        # measure data loading time\n        target = target.cuda(non_blocking=True)\n\n        # compute output\n        output = model(input)\n        loss = criterion(output.logits, target)\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        # pruning\n        pruner.prune(model=model, stage=stage, update_masks=False)\n        \n        print('Training Epoch: {epoch} [{trained_samples}/{total_samples}]\\tLoss: {:0.4f}\\tLR: {:0.6f}'.format(\n            loss.item(),\n            optimizer.param_groups[0]['lr'],\n            epoch=epoch,\n            trained_samples=batch_index * 128 + len(input),\n            total_samples=len(train_loader.dataset)\n        ))","execution_count":81,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[60, 120, 160], gamma=0.2)\nprev_epoch = 10\nmax_epoch = 20\nstage = 0\nstart = time.time()\nfor epoch in range(prev_epoch,max_epoch):\n    if epoch == 0:\n        pruner.prune(model=net, stage=stage, update_masks=True)\n    train(train_loader=trainloader, model=net, criterion=criterion, optimizer=optimizer,\n              pruner=pruner, epoch=epoch,stage=stage)\nend = time.time()\nprint('Finished in',end-start)","execution_count":83,"outputs":[{"output_type":"stream","text":"=========================================================================================\nTraining Epoch: 10 [128/50000]\tLoss: 3.6612\tLR: 0.100000\nTraining Epoch: 10 [256/50000]\tLoss: 3.5402\tLR: 0.100000\nTraining Epoch: 10 [384/50000]\tLoss: 3.8542\tLR: 0.100000\nTraining Epoch: 10 [512/50000]\tLoss: 3.6381\tLR: 0.100000\nTraining Epoch: 10 [640/50000]\tLoss: 3.5999\tLR: 0.100000\nTraining Epoch: 10 [768/50000]\tLoss: 3.6518\tLR: 0.100000\nTraining Epoch: 10 [896/50000]\tLoss: 3.6314\tLR: 0.100000\nTraining Epoch: 10 [1024/50000]\tLoss: 3.7106\tLR: 0.100000\nTraining Epoch: 10 [1152/50000]\tLoss: 3.6113\tLR: 0.100000\nTraining Epoch: 10 [1280/50000]\tLoss: 3.4598\tLR: 0.100000\nTraining Epoch: 10 [1408/50000]\tLoss: 3.5032\tLR: 0.100000\nTraining Epoch: 10 [1536/50000]\tLoss: 3.4323\tLR: 0.100000\nTraining Epoch: 10 [1664/50000]\tLoss: 3.7716\tLR: 0.100000\nTraining Epoch: 10 [1792/50000]\tLoss: 3.5260\tLR: 0.100000\nTraining Epoch: 10 [1920/50000]\tLoss: 3.5874\tLR: 0.100000\nTraining Epoch: 10 [2048/50000]\tLoss: 3.6199\tLR: 0.100000\nTraining Epoch: 10 [2176/50000]\tLoss: 3.4904\tLR: 0.100000\nTraining Epoch: 10 [2304/50000]\tLoss: 3.7433\tLR: 0.100000\nTraining Epoch: 10 [2432/50000]\tLoss: 3.6171\tLR: 0.100000\nTraining Epoch: 10 [2560/50000]\tLoss: 3.4140\tLR: 0.100000\nTraining Epoch: 10 [2688/50000]\tLoss: 3.6371\tLR: 0.100000\nTraining Epoch: 10 [2816/50000]\tLoss: 3.8385\tLR: 0.100000\nTraining Epoch: 10 [2944/50000]\tLoss: 3.6651\tLR: 0.100000\nTraining Epoch: 10 [3072/50000]\tLoss: 3.6892\tLR: 0.100000\nTraining Epoch: 10 [3200/50000]\tLoss: 3.6815\tLR: 0.100000\nTraining Epoch: 10 [3328/50000]\tLoss: 3.8491\tLR: 0.100000\nTraining Epoch: 10 [3456/50000]\tLoss: 3.5434\tLR: 0.100000\nTraining Epoch: 10 [3584/50000]\tLoss: 3.5962\tLR: 0.100000\nTraining Epoch: 10 [3712/50000]\tLoss: 3.6013\tLR: 0.100000\nTraining Epoch: 10 [3840/50000]\tLoss: 3.6711\tLR: 0.100000\nTraining Epoch: 10 [3968/50000]\tLoss: 3.6218\tLR: 0.100000\nTraining Epoch: 10 [4096/50000]\tLoss: 3.6665\tLR: 0.100000\nTraining Epoch: 10 [4224/50000]\tLoss: 3.5101\tLR: 0.100000\nTraining Epoch: 10 [4352/50000]\tLoss: 3.4824\tLR: 0.100000\nTraining Epoch: 10 [4480/50000]\tLoss: 3.6854\tLR: 0.100000\nTraining Epoch: 10 [4608/50000]\tLoss: 3.6500\tLR: 0.100000\nTraining Epoch: 10 [4736/50000]\tLoss: 3.5476\tLR: 0.100000\nTraining Epoch: 10 [4864/50000]\tLoss: 3.6011\tLR: 0.100000\nTraining Epoch: 10 [4992/50000]\tLoss: 3.6805\tLR: 0.100000\nTraining Epoch: 10 [5120/50000]\tLoss: 3.6446\tLR: 0.100000\nTraining Epoch: 10 [5248/50000]\tLoss: 3.7335\tLR: 0.100000\nTraining Epoch: 10 [5376/50000]\tLoss: 3.5132\tLR: 0.100000\nTraining Epoch: 10 [5504/50000]\tLoss: 3.5731\tLR: 0.100000\nTraining Epoch: 10 [5632/50000]\tLoss: 3.6606\tLR: 0.100000\nTraining Epoch: 10 [5760/50000]\tLoss: 3.6515\tLR: 0.100000\nTraining Epoch: 10 [5888/50000]\tLoss: 3.6725\tLR: 0.100000\nTraining Epoch: 10 [6016/50000]\tLoss: 3.6599\tLR: 0.100000\nTraining Epoch: 10 [6144/50000]\tLoss: 3.6405\tLR: 0.100000\nTraining Epoch: 10 [6272/50000]\tLoss: 3.6823\tLR: 0.100000\nTraining Epoch: 10 [6400/50000]\tLoss: 3.5149\tLR: 0.100000\nTraining Epoch: 10 [6528/50000]\tLoss: 3.6642\tLR: 0.100000\nTraining Epoch: 10 [6656/50000]\tLoss: 3.5924\tLR: 0.100000\nTraining Epoch: 10 [6784/50000]\tLoss: 3.5622\tLR: 0.100000\nTraining Epoch: 10 [6912/50000]\tLoss: 3.7244\tLR: 0.100000\nTraining Epoch: 10 [7040/50000]\tLoss: 3.5255\tLR: 0.100000\nTraining Epoch: 10 [7168/50000]\tLoss: 3.9240\tLR: 0.100000\nTraining Epoch: 10 [7296/50000]\tLoss: 3.7548\tLR: 0.100000\nTraining Epoch: 10 [7424/50000]\tLoss: 3.4980\tLR: 0.100000\nTraining Epoch: 10 [7552/50000]\tLoss: 3.5523\tLR: 0.100000\nTraining Epoch: 10 [7680/50000]\tLoss: 3.6465\tLR: 0.100000\nTraining Epoch: 10 [7808/50000]\tLoss: 3.5590\tLR: 0.100000\nTraining Epoch: 10 [7936/50000]\tLoss: 3.8644\tLR: 0.100000\nTraining Epoch: 10 [8064/50000]\tLoss: 3.6813\tLR: 0.100000\nTraining Epoch: 10 [8192/50000]\tLoss: 3.5333\tLR: 0.100000\nTraining Epoch: 10 [8320/50000]\tLoss: 3.4172\tLR: 0.100000\nTraining Epoch: 10 [8448/50000]\tLoss: 3.5664\tLR: 0.100000\nTraining Epoch: 10 [8576/50000]\tLoss: 3.6084\tLR: 0.100000\nTraining Epoch: 10 [8704/50000]\tLoss: 3.7004\tLR: 0.100000\nTraining Epoch: 10 [8832/50000]\tLoss: 3.6035\tLR: 0.100000\nTraining Epoch: 10 [8960/50000]\tLoss: 3.5355\tLR: 0.100000\nTraining Epoch: 10 [9088/50000]\tLoss: 3.5669\tLR: 0.100000\nTraining Epoch: 10 [9216/50000]\tLoss: 3.5442\tLR: 0.100000\nTraining Epoch: 10 [9344/50000]\tLoss: 3.4826\tLR: 0.100000\nTraining Epoch: 10 [9472/50000]\tLoss: 3.4933\tLR: 0.100000\nTraining Epoch: 10 [9600/50000]\tLoss: 3.6646\tLR: 0.100000\nTraining Epoch: 10 [9728/50000]\tLoss: 3.4447\tLR: 0.100000\nTraining Epoch: 10 [9856/50000]\tLoss: 3.8375\tLR: 0.100000\nTraining Epoch: 10 [9984/50000]\tLoss: 3.4293\tLR: 0.100000\nTraining Epoch: 10 [10112/50000]\tLoss: 3.2421\tLR: 0.100000\nTraining Epoch: 10 [10240/50000]\tLoss: 3.5319\tLR: 0.100000\nTraining Epoch: 10 [10368/50000]\tLoss: 3.8038\tLR: 0.100000\nTraining Epoch: 10 [10496/50000]\tLoss: 3.6470\tLR: 0.100000\nTraining Epoch: 10 [10624/50000]\tLoss: 3.5367\tLR: 0.100000\nTraining Epoch: 10 [10752/50000]\tLoss: 3.7458\tLR: 0.100000\nTraining Epoch: 10 [10880/50000]\tLoss: 3.5273\tLR: 0.100000\nTraining Epoch: 10 [11008/50000]\tLoss: 3.4607\tLR: 0.100000\nTraining Epoch: 10 [11136/50000]\tLoss: 3.7333\tLR: 0.100000\nTraining Epoch: 10 [11264/50000]\tLoss: 3.4352\tLR: 0.100000\nTraining Epoch: 10 [11392/50000]\tLoss: 3.5722\tLR: 0.100000\nTraining Epoch: 10 [11520/50000]\tLoss: 3.5994\tLR: 0.100000\nTraining Epoch: 10 [11648/50000]\tLoss: 3.7535\tLR: 0.100000\nTraining Epoch: 10 [11776/50000]\tLoss: 3.5856\tLR: 0.100000\nTraining Epoch: 10 [11904/50000]\tLoss: 3.5713\tLR: 0.100000\nTraining Epoch: 10 [12032/50000]\tLoss: 3.6885\tLR: 0.100000\nTraining Epoch: 10 [12160/50000]\tLoss: 3.5633\tLR: 0.100000\nTraining Epoch: 10 [12288/50000]\tLoss: 3.7189\tLR: 0.100000\nTraining Epoch: 10 [12416/50000]\tLoss: 3.6995\tLR: 0.100000\nTraining Epoch: 10 [12544/50000]\tLoss: 3.5312\tLR: 0.100000\nTraining Epoch: 10 [12672/50000]\tLoss: 3.6077\tLR: 0.100000\nTraining Epoch: 10 [12800/50000]\tLoss: 3.7347\tLR: 0.100000\nTraining Epoch: 10 [12928/50000]\tLoss: 3.7215\tLR: 0.100000\nTraining Epoch: 10 [13056/50000]\tLoss: 3.5974\tLR: 0.100000\nTraining Epoch: 10 [13184/50000]\tLoss: 3.5782\tLR: 0.100000\nTraining Epoch: 10 [13312/50000]\tLoss: 3.6595\tLR: 0.100000\nTraining Epoch: 10 [13440/50000]\tLoss: 3.3708\tLR: 0.100000\nTraining Epoch: 10 [13568/50000]\tLoss: 3.5643\tLR: 0.100000\nTraining Epoch: 10 [13696/50000]\tLoss: 3.6310\tLR: 0.100000\nTraining Epoch: 10 [13824/50000]\tLoss: 3.6719\tLR: 0.100000\nTraining Epoch: 10 [13952/50000]\tLoss: 3.5277\tLR: 0.100000\nTraining Epoch: 10 [14080/50000]\tLoss: 3.5669\tLR: 0.100000\nTraining Epoch: 10 [14208/50000]\tLoss: 3.5793\tLR: 0.100000\nTraining Epoch: 10 [14336/50000]\tLoss: 3.7794\tLR: 0.100000\nTraining Epoch: 10 [14464/50000]\tLoss: 3.4505\tLR: 0.100000\nTraining Epoch: 10 [14592/50000]\tLoss: 3.5552\tLR: 0.100000\nTraining Epoch: 10 [14720/50000]\tLoss: 3.6011\tLR: 0.100000\nTraining Epoch: 10 [14848/50000]\tLoss: 3.6703\tLR: 0.100000\nTraining Epoch: 10 [14976/50000]\tLoss: 3.7300\tLR: 0.100000\nTraining Epoch: 10 [15104/50000]\tLoss: 3.8573\tLR: 0.100000\nTraining Epoch: 10 [15232/50000]\tLoss: 3.7355\tLR: 0.100000\nTraining Epoch: 10 [15360/50000]\tLoss: 3.6503\tLR: 0.100000\nTraining Epoch: 10 [15488/50000]\tLoss: 3.6347\tLR: 0.100000\nTraining Epoch: 10 [15616/50000]\tLoss: 3.6674\tLR: 0.100000\nTraining Epoch: 10 [15744/50000]\tLoss: 3.5784\tLR: 0.100000\nTraining Epoch: 10 [15872/50000]\tLoss: 3.4904\tLR: 0.100000\nTraining Epoch: 10 [16000/50000]\tLoss: 3.6038\tLR: 0.100000\nTraining Epoch: 10 [16128/50000]\tLoss: 3.7220\tLR: 0.100000\nTraining Epoch: 10 [16256/50000]\tLoss: 3.8429\tLR: 0.100000\nTraining Epoch: 10 [16384/50000]\tLoss: 3.6866\tLR: 0.100000\nTraining Epoch: 10 [16512/50000]\tLoss: 3.7097\tLR: 0.100000\nTraining Epoch: 10 [16640/50000]\tLoss: 3.6582\tLR: 0.100000\nTraining Epoch: 10 [16768/50000]\tLoss: 3.5816\tLR: 0.100000\nTraining Epoch: 10 [16896/50000]\tLoss: 3.4780\tLR: 0.100000\nTraining Epoch: 10 [17024/50000]\tLoss: 3.5900\tLR: 0.100000\nTraining Epoch: 10 [17152/50000]\tLoss: 3.6148\tLR: 0.100000\nTraining Epoch: 10 [17280/50000]\tLoss: 3.8254\tLR: 0.100000\nTraining Epoch: 10 [17408/50000]\tLoss: 3.6030\tLR: 0.100000\nTraining Epoch: 10 [17536/50000]\tLoss: 3.6959\tLR: 0.100000\nTraining Epoch: 10 [17664/50000]\tLoss: 3.8331\tLR: 0.100000\nTraining Epoch: 10 [17792/50000]\tLoss: 3.6668\tLR: 0.100000\n","name":"stdout"},{"output_type":"stream","text":"Training Epoch: 10 [17920/50000]\tLoss: 3.5132\tLR: 0.100000\nTraining Epoch: 10 [18048/50000]\tLoss: 3.7002\tLR: 0.100000\nTraining Epoch: 10 [18176/50000]\tLoss: 3.6346\tLR: 0.100000\nTraining Epoch: 10 [18304/50000]\tLoss: 3.5236\tLR: 0.100000\nTraining Epoch: 10 [18432/50000]\tLoss: 3.6210\tLR: 0.100000\nTraining Epoch: 10 [18560/50000]\tLoss: 3.7660\tLR: 0.100000\nTraining Epoch: 10 [18688/50000]\tLoss: 3.5301\tLR: 0.100000\nTraining Epoch: 10 [18816/50000]\tLoss: 3.5936\tLR: 0.100000\nTraining Epoch: 10 [18944/50000]\tLoss: 3.8357\tLR: 0.100000\nTraining Epoch: 10 [19072/50000]\tLoss: 3.6361\tLR: 0.100000\nTraining Epoch: 10 [19200/50000]\tLoss: 3.4482\tLR: 0.100000\nTraining Epoch: 10 [19328/50000]\tLoss: 3.7320\tLR: 0.100000\nTraining Epoch: 10 [19456/50000]\tLoss: 3.7101\tLR: 0.100000\nTraining Epoch: 10 [19584/50000]\tLoss: 3.6969\tLR: 0.100000\nTraining Epoch: 10 [19712/50000]\tLoss: 3.6388\tLR: 0.100000\nTraining Epoch: 10 [19840/50000]\tLoss: 3.7326\tLR: 0.100000\nTraining Epoch: 10 [19968/50000]\tLoss: 3.5642\tLR: 0.100000\nTraining Epoch: 10 [20096/50000]\tLoss: 3.7397\tLR: 0.100000\nTraining Epoch: 10 [20224/50000]\tLoss: 3.8721\tLR: 0.100000\nTraining Epoch: 10 [20352/50000]\tLoss: 3.4272\tLR: 0.100000\nTraining Epoch: 10 [20480/50000]\tLoss: 3.5709\tLR: 0.100000\nTraining Epoch: 10 [20608/50000]\tLoss: 3.6552\tLR: 0.100000\nTraining Epoch: 10 [20736/50000]\tLoss: 3.6208\tLR: 0.100000\nTraining Epoch: 10 [20864/50000]\tLoss: 3.2441\tLR: 0.100000\nTraining Epoch: 10 [20992/50000]\tLoss: 3.6717\tLR: 0.100000\nTraining Epoch: 10 [21120/50000]\tLoss: 3.6271\tLR: 0.100000\nTraining Epoch: 10 [21248/50000]\tLoss: 3.6925\tLR: 0.100000\nTraining Epoch: 10 [21376/50000]\tLoss: 3.6672\tLR: 0.100000\nTraining Epoch: 10 [21504/50000]\tLoss: 3.5208\tLR: 0.100000\nTraining Epoch: 10 [21632/50000]\tLoss: 3.5084\tLR: 0.100000\nTraining Epoch: 10 [21760/50000]\tLoss: 3.5045\tLR: 0.100000\nTraining Epoch: 10 [21888/50000]\tLoss: 3.7893\tLR: 0.100000\nTraining Epoch: 10 [22016/50000]\tLoss: 3.7649\tLR: 0.100000\nTraining Epoch: 10 [22144/50000]\tLoss: 3.5772\tLR: 0.100000\nTraining Epoch: 10 [22272/50000]\tLoss: 3.5695\tLR: 0.100000\nTraining Epoch: 10 [22400/50000]\tLoss: 3.5378\tLR: 0.100000\nTraining Epoch: 10 [22528/50000]\tLoss: 3.7520\tLR: 0.100000\nTraining Epoch: 10 [22656/50000]\tLoss: 3.7597\tLR: 0.100000\nTraining Epoch: 10 [22784/50000]\tLoss: 3.5641\tLR: 0.100000\nTraining Epoch: 10 [22912/50000]\tLoss: 3.6802\tLR: 0.100000\nTraining Epoch: 10 [23040/50000]\tLoss: 3.7287\tLR: 0.100000\nTraining Epoch: 10 [23168/50000]\tLoss: 3.6864\tLR: 0.100000\nTraining Epoch: 10 [23296/50000]\tLoss: 3.6335\tLR: 0.100000\nTraining Epoch: 10 [23424/50000]\tLoss: 3.6237\tLR: 0.100000\nTraining Epoch: 10 [23552/50000]\tLoss: 3.8763\tLR: 0.100000\nTraining Epoch: 10 [23680/50000]\tLoss: 3.6272\tLR: 0.100000\nTraining Epoch: 10 [23808/50000]\tLoss: 3.6826\tLR: 0.100000\nTraining Epoch: 10 [23936/50000]\tLoss: 3.6364\tLR: 0.100000\nTraining Epoch: 10 [24064/50000]\tLoss: 3.3425\tLR: 0.100000\nTraining Epoch: 10 [24192/50000]\tLoss: 3.7476\tLR: 0.100000\nTraining Epoch: 10 [24320/50000]\tLoss: 3.5792\tLR: 0.100000\nTraining Epoch: 10 [24448/50000]\tLoss: 3.7013\tLR: 0.100000\nTraining Epoch: 10 [24576/50000]\tLoss: 3.6830\tLR: 0.100000\nTraining Epoch: 10 [24704/50000]\tLoss: 3.6313\tLR: 0.100000\nTraining Epoch: 10 [24832/50000]\tLoss: 3.8864\tLR: 0.100000\nTraining Epoch: 10 [24960/50000]\tLoss: 3.6791\tLR: 0.100000\nTraining Epoch: 10 [25088/50000]\tLoss: 3.5237\tLR: 0.100000\nTraining Epoch: 10 [25216/50000]\tLoss: 3.8040\tLR: 0.100000\nTraining Epoch: 10 [25344/50000]\tLoss: 3.5050\tLR: 0.100000\nTraining Epoch: 10 [25472/50000]\tLoss: 3.5714\tLR: 0.100000\nTraining Epoch: 10 [25600/50000]\tLoss: 3.5519\tLR: 0.100000\nTraining Epoch: 10 [25728/50000]\tLoss: 3.6753\tLR: 0.100000\nTraining Epoch: 10 [25856/50000]\tLoss: 3.8811\tLR: 0.100000\nTraining Epoch: 10 [25984/50000]\tLoss: 3.5290\tLR: 0.100000\nTraining Epoch: 10 [26112/50000]\tLoss: 3.6350\tLR: 0.100000\nTraining Epoch: 10 [26240/50000]\tLoss: 3.6919\tLR: 0.100000\nTraining Epoch: 10 [26368/50000]\tLoss: 3.7387\tLR: 0.100000\nTraining Epoch: 10 [26496/50000]\tLoss: 3.5957\tLR: 0.100000\nTraining Epoch: 10 [26624/50000]\tLoss: 3.5329\tLR: 0.100000\nTraining Epoch: 10 [26752/50000]\tLoss: 3.7430\tLR: 0.100000\nTraining Epoch: 10 [26880/50000]\tLoss: 3.5036\tLR: 0.100000\nTraining Epoch: 10 [27008/50000]\tLoss: 3.5941\tLR: 0.100000\nTraining Epoch: 10 [27136/50000]\tLoss: 3.6339\tLR: 0.100000\nTraining Epoch: 10 [27264/50000]\tLoss: 3.8350\tLR: 0.100000\nTraining Epoch: 10 [27392/50000]\tLoss: 3.7028\tLR: 0.100000\nTraining Epoch: 10 [27520/50000]\tLoss: 3.6530\tLR: 0.100000\nTraining Epoch: 10 [27648/50000]\tLoss: 3.4883\tLR: 0.100000\nTraining Epoch: 10 [27776/50000]\tLoss: 3.6957\tLR: 0.100000\nTraining Epoch: 10 [27904/50000]\tLoss: 3.5683\tLR: 0.100000\nTraining Epoch: 10 [28032/50000]\tLoss: 3.6908\tLR: 0.100000\nTraining Epoch: 10 [28160/50000]\tLoss: 3.7886\tLR: 0.100000\nTraining Epoch: 10 [28288/50000]\tLoss: 3.7006\tLR: 0.100000\nTraining Epoch: 10 [28416/50000]\tLoss: 3.6835\tLR: 0.100000\nTraining Epoch: 10 [28544/50000]\tLoss: 3.6376\tLR: 0.100000\nTraining Epoch: 10 [28672/50000]\tLoss: 3.6727\tLR: 0.100000\nTraining Epoch: 10 [28800/50000]\tLoss: 3.7013\tLR: 0.100000\nTraining Epoch: 10 [28928/50000]\tLoss: 3.6613\tLR: 0.100000\nTraining Epoch: 10 [29056/50000]\tLoss: 3.5217\tLR: 0.100000\nTraining Epoch: 10 [29184/50000]\tLoss: 3.8110\tLR: 0.100000\nTraining Epoch: 10 [29312/50000]\tLoss: 3.5504\tLR: 0.100000\nTraining Epoch: 10 [29440/50000]\tLoss: 3.8466\tLR: 0.100000\nTraining Epoch: 10 [29568/50000]\tLoss: 3.5093\tLR: 0.100000\nTraining Epoch: 10 [29696/50000]\tLoss: 3.5142\tLR: 0.100000\nTraining Epoch: 10 [29824/50000]\tLoss: 3.6424\tLR: 0.100000\nTraining Epoch: 10 [29952/50000]\tLoss: 3.4895\tLR: 0.100000\nTraining Epoch: 10 [30080/50000]\tLoss: 3.6335\tLR: 0.100000\nTraining Epoch: 10 [30208/50000]\tLoss: 3.6266\tLR: 0.100000\nTraining Epoch: 10 [30336/50000]\tLoss: 3.3973\tLR: 0.100000\nTraining Epoch: 10 [30464/50000]\tLoss: 3.6503\tLR: 0.100000\nTraining Epoch: 10 [30592/50000]\tLoss: 3.3508\tLR: 0.100000\nTraining Epoch: 10 [30720/50000]\tLoss: 3.5162\tLR: 0.100000\nTraining Epoch: 10 [30848/50000]\tLoss: 3.7008\tLR: 0.100000\nTraining Epoch: 10 [30976/50000]\tLoss: 3.9198\tLR: 0.100000\nTraining Epoch: 10 [31104/50000]\tLoss: 3.6119\tLR: 0.100000\nTraining Epoch: 10 [31232/50000]\tLoss: 3.3278\tLR: 0.100000\nTraining Epoch: 10 [31360/50000]\tLoss: 3.6056\tLR: 0.100000\nTraining Epoch: 10 [31488/50000]\tLoss: 3.3286\tLR: 0.100000\nTraining Epoch: 10 [31616/50000]\tLoss: 3.6271\tLR: 0.100000\nTraining Epoch: 10 [31744/50000]\tLoss: 3.4546\tLR: 0.100000\nTraining Epoch: 10 [31872/50000]\tLoss: 3.6070\tLR: 0.100000\nTraining Epoch: 10 [32000/50000]\tLoss: 3.8097\tLR: 0.100000\nTraining Epoch: 10 [32128/50000]\tLoss: 3.7311\tLR: 0.100000\nTraining Epoch: 10 [32256/50000]\tLoss: 3.7096\tLR: 0.100000\nTraining Epoch: 10 [32384/50000]\tLoss: 3.7289\tLR: 0.100000\nTraining Epoch: 10 [32512/50000]\tLoss: 3.6500\tLR: 0.100000\nTraining Epoch: 10 [32640/50000]\tLoss: 3.5536\tLR: 0.100000\nTraining Epoch: 10 [32768/50000]\tLoss: 3.7096\tLR: 0.100000\nTraining Epoch: 10 [32896/50000]\tLoss: 3.7419\tLR: 0.100000\nTraining Epoch: 10 [33024/50000]\tLoss: 3.7607\tLR: 0.100000\nTraining Epoch: 10 [33152/50000]\tLoss: 3.5390\tLR: 0.100000\nTraining Epoch: 10 [33280/50000]\tLoss: 3.7096\tLR: 0.100000\nTraining Epoch: 10 [33408/50000]\tLoss: 3.6680\tLR: 0.100000\nTraining Epoch: 10 [33536/50000]\tLoss: 3.6338\tLR: 0.100000\nTraining Epoch: 10 [33664/50000]\tLoss: 3.4804\tLR: 0.100000\nTraining Epoch: 10 [33792/50000]\tLoss: 3.6121\tLR: 0.100000\nTraining Epoch: 10 [33920/50000]\tLoss: 3.6753\tLR: 0.100000\nTraining Epoch: 10 [34048/50000]\tLoss: 3.5227\tLR: 0.100000\nTraining Epoch: 10 [34176/50000]\tLoss: 3.4741\tLR: 0.100000\nTraining Epoch: 10 [34304/50000]\tLoss: 3.6882\tLR: 0.100000\nTraining Epoch: 10 [34432/50000]\tLoss: 3.7493\tLR: 0.100000\nTraining Epoch: 10 [34560/50000]\tLoss: 3.6815\tLR: 0.100000\nTraining Epoch: 10 [34688/50000]\tLoss: 3.6934\tLR: 0.100000\nTraining Epoch: 10 [34816/50000]\tLoss: 3.7685\tLR: 0.100000\nTraining Epoch: 10 [34944/50000]\tLoss: 3.6737\tLR: 0.100000\nTraining Epoch: 10 [35072/50000]\tLoss: 3.4620\tLR: 0.100000\nTraining Epoch: 10 [35200/50000]\tLoss: 3.9520\tLR: 0.100000\nTraining Epoch: 10 [35328/50000]\tLoss: 3.7242\tLR: 0.100000\nTraining Epoch: 10 [35456/50000]\tLoss: 3.6153\tLR: 0.100000\nTraining Epoch: 10 [35584/50000]\tLoss: 3.5409\tLR: 0.100000\nTraining Epoch: 10 [35712/50000]\tLoss: 3.6429\tLR: 0.100000\nTraining Epoch: 10 [35840/50000]\tLoss: 3.3458\tLR: 0.100000\n","name":"stdout"},{"output_type":"stream","text":"Training Epoch: 10 [35968/50000]\tLoss: 3.8116\tLR: 0.100000\nTraining Epoch: 10 [36096/50000]\tLoss: 3.4502\tLR: 0.100000\nTraining Epoch: 10 [36224/50000]\tLoss: 3.6222\tLR: 0.100000\nTraining Epoch: 10 [36352/50000]\tLoss: 3.6648\tLR: 0.100000\nTraining Epoch: 10 [36480/50000]\tLoss: 3.7872\tLR: 0.100000\nTraining Epoch: 10 [36608/50000]\tLoss: 3.5892\tLR: 0.100000\nTraining Epoch: 10 [36736/50000]\tLoss: 3.7615\tLR: 0.100000\nTraining Epoch: 10 [36864/50000]\tLoss: 3.5437\tLR: 0.100000\nTraining Epoch: 10 [36992/50000]\tLoss: 3.7191\tLR: 0.100000\nTraining Epoch: 10 [37120/50000]\tLoss: 3.5297\tLR: 0.100000\nTraining Epoch: 10 [37248/50000]\tLoss: 3.4678\tLR: 0.100000\nTraining Epoch: 10 [37376/50000]\tLoss: 3.4733\tLR: 0.100000\nTraining Epoch: 10 [37504/50000]\tLoss: 3.6786\tLR: 0.100000\nTraining Epoch: 10 [37632/50000]\tLoss: 3.6427\tLR: 0.100000\nTraining Epoch: 10 [37760/50000]\tLoss: 3.5790\tLR: 0.100000\nTraining Epoch: 10 [37888/50000]\tLoss: 3.7788\tLR: 0.100000\nTraining Epoch: 10 [38016/50000]\tLoss: 3.6915\tLR: 0.100000\nTraining Epoch: 10 [38144/50000]\tLoss: 3.3964\tLR: 0.100000\nTraining Epoch: 10 [38272/50000]\tLoss: 3.4344\tLR: 0.100000\nTraining Epoch: 10 [38400/50000]\tLoss: 3.6261\tLR: 0.100000\nTraining Epoch: 10 [38528/50000]\tLoss: 3.6701\tLR: 0.100000\nTraining Epoch: 10 [38656/50000]\tLoss: 3.7511\tLR: 0.100000\nTraining Epoch: 10 [38784/50000]\tLoss: 3.6093\tLR: 0.100000\nTraining Epoch: 10 [38912/50000]\tLoss: 3.5069\tLR: 0.100000\nTraining Epoch: 10 [39040/50000]\tLoss: 3.5689\tLR: 0.100000\nTraining Epoch: 10 [39168/50000]\tLoss: 3.7160\tLR: 0.100000\nTraining Epoch: 10 [39296/50000]\tLoss: 3.5375\tLR: 0.100000\nTraining Epoch: 10 [39424/50000]\tLoss: 3.6295\tLR: 0.100000\nTraining Epoch: 10 [39552/50000]\tLoss: 3.6975\tLR: 0.100000\nTraining Epoch: 10 [39680/50000]\tLoss: 3.6938\tLR: 0.100000\nTraining Epoch: 10 [39808/50000]\tLoss: 3.6382\tLR: 0.100000\nTraining Epoch: 10 [39936/50000]\tLoss: 3.6481\tLR: 0.100000\nTraining Epoch: 10 [40064/50000]\tLoss: 3.8062\tLR: 0.100000\nTraining Epoch: 10 [40192/50000]\tLoss: 3.7123\tLR: 0.100000\nTraining Epoch: 10 [40320/50000]\tLoss: 3.5844\tLR: 0.100000\nTraining Epoch: 10 [40448/50000]\tLoss: 3.7180\tLR: 0.100000\nTraining Epoch: 10 [40576/50000]\tLoss: 3.7685\tLR: 0.100000\nTraining Epoch: 10 [40704/50000]\tLoss: 3.9092\tLR: 0.100000\nTraining Epoch: 10 [40832/50000]\tLoss: 3.9089\tLR: 0.100000\nTraining Epoch: 10 [40960/50000]\tLoss: 3.7332\tLR: 0.100000\nTraining Epoch: 10 [41088/50000]\tLoss: 3.6171\tLR: 0.100000\nTraining Epoch: 10 [41216/50000]\tLoss: 3.8213\tLR: 0.100000\nTraining Epoch: 10 [41344/50000]\tLoss: 3.8014\tLR: 0.100000\nTraining Epoch: 10 [41472/50000]\tLoss: 3.6044\tLR: 0.100000\nTraining Epoch: 10 [41600/50000]\tLoss: 3.5234\tLR: 0.100000\nTraining Epoch: 10 [41728/50000]\tLoss: 3.6411\tLR: 0.100000\nTraining Epoch: 10 [41856/50000]\tLoss: 3.5057\tLR: 0.100000\nTraining Epoch: 10 [41984/50000]\tLoss: 3.6714\tLR: 0.100000\nTraining Epoch: 10 [42112/50000]\tLoss: 3.5009\tLR: 0.100000\nTraining Epoch: 10 [42240/50000]\tLoss: 3.6075\tLR: 0.100000\nTraining Epoch: 10 [42368/50000]\tLoss: 3.6407\tLR: 0.100000\nTraining Epoch: 10 [42496/50000]\tLoss: 3.6923\tLR: 0.100000\nTraining Epoch: 10 [42624/50000]\tLoss: 3.5148\tLR: 0.100000\nTraining Epoch: 10 [42752/50000]\tLoss: 3.5163\tLR: 0.100000\nTraining Epoch: 10 [42880/50000]\tLoss: 3.6166\tLR: 0.100000\nTraining Epoch: 10 [43008/50000]\tLoss: 3.4556\tLR: 0.100000\nTraining Epoch: 10 [43136/50000]\tLoss: 3.5460\tLR: 0.100000\nTraining Epoch: 10 [43264/50000]\tLoss: 3.5767\tLR: 0.100000\nTraining Epoch: 10 [43392/50000]\tLoss: 3.6329\tLR: 0.100000\nTraining Epoch: 10 [43520/50000]\tLoss: 3.4981\tLR: 0.100000\nTraining Epoch: 10 [43648/50000]\tLoss: 3.5237\tLR: 0.100000\nTraining Epoch: 10 [43776/50000]\tLoss: 3.5628\tLR: 0.100000\nTraining Epoch: 10 [43904/50000]\tLoss: 3.7045\tLR: 0.100000\nTraining Epoch: 10 [44032/50000]\tLoss: 3.7648\tLR: 0.100000\nTraining Epoch: 10 [44160/50000]\tLoss: 3.6620\tLR: 0.100000\nTraining Epoch: 10 [44288/50000]\tLoss: 3.8117\tLR: 0.100000\nTraining Epoch: 10 [44416/50000]\tLoss: 3.7574\tLR: 0.100000\nTraining Epoch: 10 [44544/50000]\tLoss: 3.7232\tLR: 0.100000\nTraining Epoch: 10 [44672/50000]\tLoss: 3.3987\tLR: 0.100000\nTraining Epoch: 10 [44800/50000]\tLoss: 3.5783\tLR: 0.100000\nTraining Epoch: 10 [44928/50000]\tLoss: 3.7619\tLR: 0.100000\nTraining Epoch: 10 [45056/50000]\tLoss: 3.5896\tLR: 0.100000\nTraining Epoch: 10 [45184/50000]\tLoss: 3.4199\tLR: 0.100000\nTraining Epoch: 10 [45312/50000]\tLoss: 3.7206\tLR: 0.100000\nTraining Epoch: 10 [45440/50000]\tLoss: 3.6349\tLR: 0.100000\nTraining Epoch: 10 [45568/50000]\tLoss: 3.6668\tLR: 0.100000\nTraining Epoch: 10 [45696/50000]\tLoss: 3.7941\tLR: 0.100000\nTraining Epoch: 10 [45824/50000]\tLoss: 3.7128\tLR: 0.100000\nTraining Epoch: 10 [45952/50000]\tLoss: 3.4874\tLR: 0.100000\nTraining Epoch: 10 [46080/50000]\tLoss: 3.4574\tLR: 0.100000\nTraining Epoch: 10 [46208/50000]\tLoss: 3.5393\tLR: 0.100000\nTraining Epoch: 10 [46336/50000]\tLoss: 3.5796\tLR: 0.100000\nTraining Epoch: 10 [46464/50000]\tLoss: 3.5068\tLR: 0.100000\nTraining Epoch: 10 [46592/50000]\tLoss: 3.7691\tLR: 0.100000\nTraining Epoch: 10 [46720/50000]\tLoss: 3.4538\tLR: 0.100000\nTraining Epoch: 10 [46848/50000]\tLoss: 3.7436\tLR: 0.100000\nTraining Epoch: 10 [46976/50000]\tLoss: 3.5247\tLR: 0.100000\nTraining Epoch: 10 [47104/50000]\tLoss: 3.6956\tLR: 0.100000\nTraining Epoch: 10 [47232/50000]\tLoss: 3.4707\tLR: 0.100000\nTraining Epoch: 10 [47360/50000]\tLoss: 3.5010\tLR: 0.100000\nTraining Epoch: 10 [47488/50000]\tLoss: 3.4597\tLR: 0.100000\nTraining Epoch: 10 [47616/50000]\tLoss: 3.3750\tLR: 0.100000\nTraining Epoch: 10 [47744/50000]\tLoss: 3.3913\tLR: 0.100000\nTraining Epoch: 10 [47872/50000]\tLoss: 3.5326\tLR: 0.100000\nTraining Epoch: 10 [48000/50000]\tLoss: 3.5623\tLR: 0.100000\nTraining Epoch: 10 [48128/50000]\tLoss: 3.7352\tLR: 0.100000\nTraining Epoch: 10 [48256/50000]\tLoss: 3.6394\tLR: 0.100000\nTraining Epoch: 10 [48384/50000]\tLoss: 3.7793\tLR: 0.100000\nTraining Epoch: 10 [48512/50000]\tLoss: 3.5010\tLR: 0.100000\nTraining Epoch: 10 [48640/50000]\tLoss: 3.6306\tLR: 0.100000\nTraining Epoch: 10 [48768/50000]\tLoss: 3.4202\tLR: 0.100000\nTraining Epoch: 10 [48896/50000]\tLoss: 3.3963\tLR: 0.100000\nTraining Epoch: 10 [49024/50000]\tLoss: 3.6851\tLR: 0.100000\nTraining Epoch: 10 [49152/50000]\tLoss: 3.7617\tLR: 0.100000\nTraining Epoch: 10 [49280/50000]\tLoss: 3.5502\tLR: 0.100000\nTraining Epoch: 10 [49408/50000]\tLoss: 3.8234\tLR: 0.100000\nTraining Epoch: 10 [49536/50000]\tLoss: 3.4377\tLR: 0.100000\nTraining Epoch: 10 [49664/50000]\tLoss: 3.6458\tLR: 0.100000\nTraining Epoch: 10 [49792/50000]\tLoss: 3.4232\tLR: 0.100000\nTraining Epoch: 10 [49920/50000]\tLoss: 3.5759\tLR: 0.100000\nTraining Epoch: 10 [50000/50000]\tLoss: 3.4552\tLR: 0.100000\n=========================================================================================\nTraining Epoch: 11 [128/50000]\tLoss: 3.6939\tLR: 0.100000\nTraining Epoch: 11 [256/50000]\tLoss: 3.5197\tLR: 0.100000\nTraining Epoch: 11 [384/50000]\tLoss: 3.6677\tLR: 0.100000\nTraining Epoch: 11 [512/50000]\tLoss: 3.7305\tLR: 0.100000\nTraining Epoch: 11 [640/50000]\tLoss: 3.4570\tLR: 0.100000\nTraining Epoch: 11 [768/50000]\tLoss: 3.5647\tLR: 0.100000\nTraining Epoch: 11 [896/50000]\tLoss: 3.6278\tLR: 0.100000\nTraining Epoch: 11 [1024/50000]\tLoss: 3.6362\tLR: 0.100000\nTraining Epoch: 11 [1152/50000]\tLoss: 3.6924\tLR: 0.100000\nTraining Epoch: 11 [1280/50000]\tLoss: 3.4363\tLR: 0.100000\nTraining Epoch: 11 [1408/50000]\tLoss: 3.6390\tLR: 0.100000\nTraining Epoch: 11 [1536/50000]\tLoss: 3.7836\tLR: 0.100000\nTraining Epoch: 11 [1664/50000]\tLoss: 3.3365\tLR: 0.100000\nTraining Epoch: 11 [1792/50000]\tLoss: 3.6556\tLR: 0.100000\nTraining Epoch: 11 [1920/50000]\tLoss: 3.7302\tLR: 0.100000\nTraining Epoch: 11 [2048/50000]\tLoss: 3.5583\tLR: 0.100000\nTraining Epoch: 11 [2176/50000]\tLoss: 3.5330\tLR: 0.100000\nTraining Epoch: 11 [2304/50000]\tLoss: 3.6505\tLR: 0.100000\nTraining Epoch: 11 [2432/50000]\tLoss: 3.5236\tLR: 0.100000\nTraining Epoch: 11 [2560/50000]\tLoss: 3.5793\tLR: 0.100000\nTraining Epoch: 11 [2688/50000]\tLoss: 3.6592\tLR: 0.100000\nTraining Epoch: 11 [2816/50000]\tLoss: 3.4971\tLR: 0.100000\nTraining Epoch: 11 [2944/50000]\tLoss: 3.4404\tLR: 0.100000\nTraining Epoch: 11 [3072/50000]\tLoss: 3.7656\tLR: 0.100000\nTraining Epoch: 11 [3200/50000]\tLoss: 3.5963\tLR: 0.100000\nTraining Epoch: 11 [3328/50000]\tLoss: 3.6381\tLR: 0.100000\nTraining Epoch: 11 [3456/50000]\tLoss: 3.6509\tLR: 0.100000\n","name":"stdout"},{"output_type":"stream","text":"Training Epoch: 11 [3584/50000]\tLoss: 3.6921\tLR: 0.100000\nTraining Epoch: 11 [3712/50000]\tLoss: 3.6447\tLR: 0.100000\nTraining Epoch: 11 [3840/50000]\tLoss: 3.6057\tLR: 0.100000\nTraining Epoch: 11 [3968/50000]\tLoss: 3.5995\tLR: 0.100000\nTraining Epoch: 11 [4096/50000]\tLoss: 3.5663\tLR: 0.100000\nTraining Epoch: 11 [4224/50000]\tLoss: 3.6077\tLR: 0.100000\nTraining Epoch: 11 [4352/50000]\tLoss: 3.5401\tLR: 0.100000\nTraining Epoch: 11 [4480/50000]\tLoss: 3.3848\tLR: 0.100000\nTraining Epoch: 11 [4608/50000]\tLoss: 3.4720\tLR: 0.100000\nTraining Epoch: 11 [4736/50000]\tLoss: 3.6819\tLR: 0.100000\nTraining Epoch: 11 [4864/50000]\tLoss: 3.6387\tLR: 0.100000\nTraining Epoch: 11 [4992/50000]\tLoss: 3.4576\tLR: 0.100000\nTraining Epoch: 11 [5120/50000]\tLoss: 3.6641\tLR: 0.100000\nTraining Epoch: 11 [5248/50000]\tLoss: 3.4544\tLR: 0.100000\nTraining Epoch: 11 [5376/50000]\tLoss: 3.6846\tLR: 0.100000\nTraining Epoch: 11 [5504/50000]\tLoss: 3.4869\tLR: 0.100000\nTraining Epoch: 11 [5632/50000]\tLoss: 3.6785\tLR: 0.100000\nTraining Epoch: 11 [5760/50000]\tLoss: 3.6511\tLR: 0.100000\nTraining Epoch: 11 [5888/50000]\tLoss: 3.6360\tLR: 0.100000\nTraining Epoch: 11 [6016/50000]\tLoss: 3.3818\tLR: 0.100000\nTraining Epoch: 11 [6144/50000]\tLoss: 3.7115\tLR: 0.100000\nTraining Epoch: 11 [6272/50000]\tLoss: 3.5693\tLR: 0.100000\nTraining Epoch: 11 [6400/50000]\tLoss: 3.6088\tLR: 0.100000\nTraining Epoch: 11 [6528/50000]\tLoss: 3.4005\tLR: 0.100000\nTraining Epoch: 11 [6656/50000]\tLoss: 3.5711\tLR: 0.100000\nTraining Epoch: 11 [6784/50000]\tLoss: 3.6835\tLR: 0.100000\nTraining Epoch: 11 [6912/50000]\tLoss: 3.5730\tLR: 0.100000\nTraining Epoch: 11 [7040/50000]\tLoss: 3.5498\tLR: 0.100000\nTraining Epoch: 11 [7168/50000]\tLoss: 3.6512\tLR: 0.100000\nTraining Epoch: 11 [7296/50000]\tLoss: 3.5418\tLR: 0.100000\nTraining Epoch: 11 [7424/50000]\tLoss: 3.5299\tLR: 0.100000\nTraining Epoch: 11 [7552/50000]\tLoss: 3.7511\tLR: 0.100000\nTraining Epoch: 11 [7680/50000]\tLoss: 3.5927\tLR: 0.100000\nTraining Epoch: 11 [7808/50000]\tLoss: 3.5222\tLR: 0.100000\nTraining Epoch: 11 [7936/50000]\tLoss: 3.7650\tLR: 0.100000\nTraining Epoch: 11 [8064/50000]\tLoss: 3.4642\tLR: 0.100000\nTraining Epoch: 11 [8192/50000]\tLoss: 3.7248\tLR: 0.100000\nTraining Epoch: 11 [8320/50000]\tLoss: 3.5367\tLR: 0.100000\nTraining Epoch: 11 [8448/50000]\tLoss: 3.5028\tLR: 0.100000\nTraining Epoch: 11 [8576/50000]\tLoss: 3.4467\tLR: 0.100000\nTraining Epoch: 11 [8704/50000]\tLoss: 3.4779\tLR: 0.100000\nTraining Epoch: 11 [8832/50000]\tLoss: 3.5556\tLR: 0.100000\nTraining Epoch: 11 [8960/50000]\tLoss: 3.5372\tLR: 0.100000\nTraining Epoch: 11 [9088/50000]\tLoss: 3.6368\tLR: 0.100000\nTraining Epoch: 11 [9216/50000]\tLoss: 3.6045\tLR: 0.100000\nTraining Epoch: 11 [9344/50000]\tLoss: 3.6893\tLR: 0.100000\nTraining Epoch: 11 [9472/50000]\tLoss: 3.8612\tLR: 0.100000\nTraining Epoch: 11 [9600/50000]\tLoss: 3.5398\tLR: 0.100000\nTraining Epoch: 11 [9728/50000]\tLoss: 3.6519\tLR: 0.100000\nTraining Epoch: 11 [9856/50000]\tLoss: 3.3725\tLR: 0.100000\nTraining Epoch: 11 [9984/50000]\tLoss: 3.7817\tLR: 0.100000\nTraining Epoch: 11 [10112/50000]\tLoss: 3.5506\tLR: 0.100000\nTraining Epoch: 11 [10240/50000]\tLoss: 3.4319\tLR: 0.100000\nTraining Epoch: 11 [10368/50000]\tLoss: 3.4631\tLR: 0.100000\nTraining Epoch: 11 [10496/50000]\tLoss: 3.5598\tLR: 0.100000\nTraining Epoch: 11 [10624/50000]\tLoss: 3.5830\tLR: 0.100000\nTraining Epoch: 11 [10752/50000]\tLoss: 3.8545\tLR: 0.100000\nTraining Epoch: 11 [10880/50000]\tLoss: 3.7395\tLR: 0.100000\nTraining Epoch: 11 [11008/50000]\tLoss: 3.6231\tLR: 0.100000\nTraining Epoch: 11 [11136/50000]\tLoss: 3.5892\tLR: 0.100000\nTraining Epoch: 11 [11264/50000]\tLoss: 3.7438\tLR: 0.100000\nTraining Epoch: 11 [11392/50000]\tLoss: 3.7733\tLR: 0.100000\nTraining Epoch: 11 [11520/50000]\tLoss: 3.5940\tLR: 0.100000\nTraining Epoch: 11 [11648/50000]\tLoss: 3.5966\tLR: 0.100000\nTraining Epoch: 11 [11776/50000]\tLoss: 3.5525\tLR: 0.100000\nTraining Epoch: 11 [11904/50000]\tLoss: 3.7288\tLR: 0.100000\nTraining Epoch: 11 [12032/50000]\tLoss: 3.6112\tLR: 0.100000\nTraining Epoch: 11 [12160/50000]\tLoss: 3.5743\tLR: 0.100000\nTraining Epoch: 11 [12288/50000]\tLoss: 3.7565\tLR: 0.100000\nTraining Epoch: 11 [12416/50000]\tLoss: 3.6966\tLR: 0.100000\nTraining Epoch: 11 [12544/50000]\tLoss: 3.8499\tLR: 0.100000\nTraining Epoch: 11 [12672/50000]\tLoss: 3.7599\tLR: 0.100000\nTraining Epoch: 11 [12800/50000]\tLoss: 3.4371\tLR: 0.100000\nTraining Epoch: 11 [12928/50000]\tLoss: 3.5874\tLR: 0.100000\nTraining Epoch: 11 [13056/50000]\tLoss: 3.6299\tLR: 0.100000\nTraining Epoch: 11 [13184/50000]\tLoss: 3.5927\tLR: 0.100000\nTraining Epoch: 11 [13312/50000]\tLoss: 3.2849\tLR: 0.100000\nTraining Epoch: 11 [13440/50000]\tLoss: 3.6466\tLR: 0.100000\nTraining Epoch: 11 [13568/50000]\tLoss: 3.6662\tLR: 0.100000\nTraining Epoch: 11 [13696/50000]\tLoss: 3.6718\tLR: 0.100000\nTraining Epoch: 11 [13824/50000]\tLoss: 3.7337\tLR: 0.100000\nTraining Epoch: 11 [13952/50000]\tLoss: 3.5239\tLR: 0.100000\nTraining Epoch: 11 [14080/50000]\tLoss: 3.6298\tLR: 0.100000\nTraining Epoch: 11 [14208/50000]\tLoss: 3.8499\tLR: 0.100000\nTraining Epoch: 11 [14336/50000]\tLoss: 3.5742\tLR: 0.100000\nTraining Epoch: 11 [14464/50000]\tLoss: 3.5325\tLR: 0.100000\nTraining Epoch: 11 [14592/50000]\tLoss: 3.4611\tLR: 0.100000\nTraining Epoch: 11 [14720/50000]\tLoss: 3.3217\tLR: 0.100000\nTraining Epoch: 11 [14848/50000]\tLoss: 3.5349\tLR: 0.100000\nTraining Epoch: 11 [14976/50000]\tLoss: 3.6149\tLR: 0.100000\nTraining Epoch: 11 [15104/50000]\tLoss: 3.7684\tLR: 0.100000\nTraining Epoch: 11 [15232/50000]\tLoss: 3.4258\tLR: 0.100000\nTraining Epoch: 11 [15360/50000]\tLoss: 3.5818\tLR: 0.100000\nTraining Epoch: 11 [15488/50000]\tLoss: 3.6215\tLR: 0.100000\nTraining Epoch: 11 [15616/50000]\tLoss: 3.5488\tLR: 0.100000\nTraining Epoch: 11 [15744/50000]\tLoss: 3.6072\tLR: 0.100000\nTraining Epoch: 11 [15872/50000]\tLoss: 3.5522\tLR: 0.100000\nTraining Epoch: 11 [16000/50000]\tLoss: 3.5267\tLR: 0.100000\nTraining Epoch: 11 [16128/50000]\tLoss: 3.6203\tLR: 0.100000\nTraining Epoch: 11 [16256/50000]\tLoss: 3.6884\tLR: 0.100000\nTraining Epoch: 11 [16384/50000]\tLoss: 3.5019\tLR: 0.100000\nTraining Epoch: 11 [16512/50000]\tLoss: 3.3858\tLR: 0.100000\nTraining Epoch: 11 [16640/50000]\tLoss: 3.6269\tLR: 0.100000\nTraining Epoch: 11 [16768/50000]\tLoss: 3.5432\tLR: 0.100000\nTraining Epoch: 11 [16896/50000]\tLoss: 3.8426\tLR: 0.100000\nTraining Epoch: 11 [17024/50000]\tLoss: 3.5042\tLR: 0.100000\nTraining Epoch: 11 [17152/50000]\tLoss: 3.7911\tLR: 0.100000\nTraining Epoch: 11 [17280/50000]\tLoss: 3.6484\tLR: 0.100000\nTraining Epoch: 11 [17408/50000]\tLoss: 3.7164\tLR: 0.100000\nTraining Epoch: 11 [17536/50000]\tLoss: 3.6406\tLR: 0.100000\nTraining Epoch: 11 [17664/50000]\tLoss: 3.3816\tLR: 0.100000\nTraining Epoch: 11 [17792/50000]\tLoss: 3.5585\tLR: 0.100000\nTraining Epoch: 11 [17920/50000]\tLoss: 3.3496\tLR: 0.100000\nTraining Epoch: 11 [18048/50000]\tLoss: 3.6435\tLR: 0.100000\nTraining Epoch: 11 [18176/50000]\tLoss: 3.3841\tLR: 0.100000\nTraining Epoch: 11 [18304/50000]\tLoss: 3.6192\tLR: 0.100000\nTraining Epoch: 11 [18432/50000]\tLoss: 3.6018\tLR: 0.100000\nTraining Epoch: 11 [18560/50000]\tLoss: 3.8037\tLR: 0.100000\nTraining Epoch: 11 [18688/50000]\tLoss: 3.7285\tLR: 0.100000\nTraining Epoch: 11 [18816/50000]\tLoss: 3.5507\tLR: 0.100000\nTraining Epoch: 11 [18944/50000]\tLoss: 3.7673\tLR: 0.100000\nTraining Epoch: 11 [19072/50000]\tLoss: 3.4698\tLR: 0.100000\nTraining Epoch: 11 [19200/50000]\tLoss: 3.4963\tLR: 0.100000\nTraining Epoch: 11 [19328/50000]\tLoss: 3.6262\tLR: 0.100000\nTraining Epoch: 11 [19456/50000]\tLoss: 3.4950\tLR: 0.100000\nTraining Epoch: 11 [19584/50000]\tLoss: 3.6325\tLR: 0.100000\nTraining Epoch: 11 [19712/50000]\tLoss: 3.4033\tLR: 0.100000\nTraining Epoch: 11 [19840/50000]\tLoss: 3.4415\tLR: 0.100000\nTraining Epoch: 11 [19968/50000]\tLoss: 3.4074\tLR: 0.100000\nTraining Epoch: 11 [20096/50000]\tLoss: 3.6993\tLR: 0.100000\nTraining Epoch: 11 [20224/50000]\tLoss: 3.4957\tLR: 0.100000\nTraining Epoch: 11 [20352/50000]\tLoss: 3.7146\tLR: 0.100000\nTraining Epoch: 11 [20480/50000]\tLoss: 3.6331\tLR: 0.100000\nTraining Epoch: 11 [20608/50000]\tLoss: 3.6767\tLR: 0.100000\nTraining Epoch: 11 [20736/50000]\tLoss: 3.4981\tLR: 0.100000\nTraining Epoch: 11 [20864/50000]\tLoss: 3.5264\tLR: 0.100000\nTraining Epoch: 11 [20992/50000]\tLoss: 3.6599\tLR: 0.100000\nTraining Epoch: 11 [21120/50000]\tLoss: 3.5608\tLR: 0.100000\nTraining Epoch: 11 [21248/50000]\tLoss: 3.6235\tLR: 0.100000\nTraining Epoch: 11 [21376/50000]\tLoss: 3.5461\tLR: 0.100000\nTraining Epoch: 11 [21504/50000]\tLoss: 3.4182\tLR: 0.100000\n","name":"stdout"},{"output_type":"stream","text":"Training Epoch: 11 [21632/50000]\tLoss: 3.6293\tLR: 0.100000\nTraining Epoch: 11 [21760/50000]\tLoss: 3.4215\tLR: 0.100000\nTraining Epoch: 11 [21888/50000]\tLoss: 3.4900\tLR: 0.100000\nTraining Epoch: 11 [22016/50000]\tLoss: 3.5548\tLR: 0.100000\nTraining Epoch: 11 [22144/50000]\tLoss: 3.6513\tLR: 0.100000\nTraining Epoch: 11 [22272/50000]\tLoss: 3.4390\tLR: 0.100000\nTraining Epoch: 11 [22400/50000]\tLoss: 3.7226\tLR: 0.100000\nTraining Epoch: 11 [22528/50000]\tLoss: 3.6482\tLR: 0.100000\nTraining Epoch: 11 [22656/50000]\tLoss: 3.6195\tLR: 0.100000\nTraining Epoch: 11 [22784/50000]\tLoss: 3.4092\tLR: 0.100000\nTraining Epoch: 11 [22912/50000]\tLoss: 3.6089\tLR: 0.100000\nTraining Epoch: 11 [23040/50000]\tLoss: 3.3694\tLR: 0.100000\nTraining Epoch: 11 [23168/50000]\tLoss: 3.7022\tLR: 0.100000\nTraining Epoch: 11 [23296/50000]\tLoss: 3.5209\tLR: 0.100000\nTraining Epoch: 11 [23424/50000]\tLoss: 3.7107\tLR: 0.100000\nTraining Epoch: 11 [23552/50000]\tLoss: 3.6250\tLR: 0.100000\nTraining Epoch: 11 [23680/50000]\tLoss: 3.8485\tLR: 0.100000\nTraining Epoch: 11 [23808/50000]\tLoss: 3.7817\tLR: 0.100000\nTraining Epoch: 11 [23936/50000]\tLoss: 3.5233\tLR: 0.100000\nTraining Epoch: 11 [24064/50000]\tLoss: 3.5182\tLR: 0.100000\nTraining Epoch: 11 [24192/50000]\tLoss: 3.5907\tLR: 0.100000\nTraining Epoch: 11 [24320/50000]\tLoss: 3.3839\tLR: 0.100000\nTraining Epoch: 11 [24448/50000]\tLoss: 3.5790\tLR: 0.100000\nTraining Epoch: 11 [24576/50000]\tLoss: 3.4786\tLR: 0.100000\nTraining Epoch: 11 [24704/50000]\tLoss: 3.6024\tLR: 0.100000\nTraining Epoch: 11 [24832/50000]\tLoss: 3.4413\tLR: 0.100000\nTraining Epoch: 11 [24960/50000]\tLoss: 3.8043\tLR: 0.100000\nTraining Epoch: 11 [25088/50000]\tLoss: 3.5589\tLR: 0.100000\nTraining Epoch: 11 [25216/50000]\tLoss: 3.7455\tLR: 0.100000\nTraining Epoch: 11 [25344/50000]\tLoss: 3.3347\tLR: 0.100000\nTraining Epoch: 11 [25472/50000]\tLoss: 3.6449\tLR: 0.100000\nTraining Epoch: 11 [25600/50000]\tLoss: 3.4872\tLR: 0.100000\nTraining Epoch: 11 [25728/50000]\tLoss: 3.4310\tLR: 0.100000\nTraining Epoch: 11 [25856/50000]\tLoss: 3.5659\tLR: 0.100000\nTraining Epoch: 11 [25984/50000]\tLoss: 3.6771\tLR: 0.100000\nTraining Epoch: 11 [26112/50000]\tLoss: 3.6123\tLR: 0.100000\nTraining Epoch: 11 [26240/50000]\tLoss: 3.5391\tLR: 0.100000\nTraining Epoch: 11 [26368/50000]\tLoss: 3.5387\tLR: 0.100000\nTraining Epoch: 11 [26496/50000]\tLoss: 3.6331\tLR: 0.100000\nTraining Epoch: 11 [26624/50000]\tLoss: 3.5864\tLR: 0.100000\nTraining Epoch: 11 [26752/50000]\tLoss: 3.5830\tLR: 0.100000\nTraining Epoch: 11 [26880/50000]\tLoss: 3.5007\tLR: 0.100000\nTraining Epoch: 11 [27008/50000]\tLoss: 3.3970\tLR: 0.100000\nTraining Epoch: 11 [27136/50000]\tLoss: 3.5766\tLR: 0.100000\nTraining Epoch: 11 [27264/50000]\tLoss: 3.7017\tLR: 0.100000\nTraining Epoch: 11 [27392/50000]\tLoss: 3.5289\tLR: 0.100000\nTraining Epoch: 11 [27520/50000]\tLoss: 3.6487\tLR: 0.100000\nTraining Epoch: 11 [27648/50000]\tLoss: 3.7192\tLR: 0.100000\nTraining Epoch: 11 [27776/50000]\tLoss: 3.5551\tLR: 0.100000\nTraining Epoch: 11 [27904/50000]\tLoss: 3.6569\tLR: 0.100000\nTraining Epoch: 11 [28032/50000]\tLoss: 3.4768\tLR: 0.100000\nTraining Epoch: 11 [28160/50000]\tLoss: 3.6227\tLR: 0.100000\nTraining Epoch: 11 [28288/50000]\tLoss: 3.6941\tLR: 0.100000\nTraining Epoch: 11 [28416/50000]\tLoss: 3.6122\tLR: 0.100000\nTraining Epoch: 11 [28544/50000]\tLoss: 3.3566\tLR: 0.100000\nTraining Epoch: 11 [28672/50000]\tLoss: 3.4723\tLR: 0.100000\nTraining Epoch: 11 [28800/50000]\tLoss: 3.6806\tLR: 0.100000\nTraining Epoch: 11 [28928/50000]\tLoss: 3.7622\tLR: 0.100000\nTraining Epoch: 11 [29056/50000]\tLoss: 3.6142\tLR: 0.100000\nTraining Epoch: 11 [29184/50000]\tLoss: 3.8245\tLR: 0.100000\nTraining Epoch: 11 [29312/50000]\tLoss: 3.7088\tLR: 0.100000\nTraining Epoch: 11 [29440/50000]\tLoss: 3.5026\tLR: 0.100000\nTraining Epoch: 11 [29568/50000]\tLoss: 3.5543\tLR: 0.100000\nTraining Epoch: 11 [29696/50000]\tLoss: 3.5576\tLR: 0.100000\nTraining Epoch: 11 [29824/50000]\tLoss: 3.4612\tLR: 0.100000\nTraining Epoch: 11 [29952/50000]\tLoss: 3.7651\tLR: 0.100000\nTraining Epoch: 11 [30080/50000]\tLoss: 3.5406\tLR: 0.100000\nTraining Epoch: 11 [30208/50000]\tLoss: 3.6265\tLR: 0.100000\nTraining Epoch: 11 [30336/50000]\tLoss: 3.7094\tLR: 0.100000\nTraining Epoch: 11 [30464/50000]\tLoss: 3.5661\tLR: 0.100000\nTraining Epoch: 11 [30592/50000]\tLoss: 3.6587\tLR: 0.100000\nTraining Epoch: 11 [30720/50000]\tLoss: 3.5204\tLR: 0.100000\nTraining Epoch: 11 [30848/50000]\tLoss: 3.4929\tLR: 0.100000\nTraining Epoch: 11 [30976/50000]\tLoss: 3.4306\tLR: 0.100000\nTraining Epoch: 11 [31104/50000]\tLoss: 3.6080\tLR: 0.100000\nTraining Epoch: 11 [31232/50000]\tLoss: 3.4595\tLR: 0.100000\nTraining Epoch: 11 [31360/50000]\tLoss: 3.6075\tLR: 0.100000\nTraining Epoch: 11 [31488/50000]\tLoss: 3.5399\tLR: 0.100000\nTraining Epoch: 11 [31616/50000]\tLoss: 3.4278\tLR: 0.100000\nTraining Epoch: 11 [31744/50000]\tLoss: 3.6375\tLR: 0.100000\nTraining Epoch: 11 [31872/50000]\tLoss: 3.4746\tLR: 0.100000\nTraining Epoch: 11 [32000/50000]\tLoss: 3.5502\tLR: 0.100000\nTraining Epoch: 11 [32128/50000]\tLoss: 3.7568\tLR: 0.100000\nTraining Epoch: 11 [32256/50000]\tLoss: 3.4891\tLR: 0.100000\nTraining Epoch: 11 [32384/50000]\tLoss: 3.5919\tLR: 0.100000\nTraining Epoch: 11 [32512/50000]\tLoss: 3.4079\tLR: 0.100000\nTraining Epoch: 11 [32640/50000]\tLoss: 3.4320\tLR: 0.100000\nTraining Epoch: 11 [32768/50000]\tLoss: 3.5596\tLR: 0.100000\nTraining Epoch: 11 [32896/50000]\tLoss: 3.5716\tLR: 0.100000\nTraining Epoch: 11 [33024/50000]\tLoss: 3.5163\tLR: 0.100000\nTraining Epoch: 11 [33152/50000]\tLoss: 3.4310\tLR: 0.100000\nTraining Epoch: 11 [33280/50000]\tLoss: 3.3944\tLR: 0.100000\nTraining Epoch: 11 [33408/50000]\tLoss: 3.6928\tLR: 0.100000\nTraining Epoch: 11 [33536/50000]\tLoss: 3.5532\tLR: 0.100000\nTraining Epoch: 11 [33664/50000]\tLoss: 3.2865\tLR: 0.100000\nTraining Epoch: 11 [33792/50000]\tLoss: 3.4634\tLR: 0.100000\nTraining Epoch: 11 [33920/50000]\tLoss: 3.2754\tLR: 0.100000\nTraining Epoch: 11 [34048/50000]\tLoss: 3.5910\tLR: 0.100000\nTraining Epoch: 11 [34176/50000]\tLoss: 3.5594\tLR: 0.100000\nTraining Epoch: 11 [34304/50000]\tLoss: 3.9225\tLR: 0.100000\nTraining Epoch: 11 [34432/50000]\tLoss: 3.5709\tLR: 0.100000\nTraining Epoch: 11 [34560/50000]\tLoss: 3.5709\tLR: 0.100000\nTraining Epoch: 11 [34688/50000]\tLoss: 3.3240\tLR: 0.100000\nTraining Epoch: 11 [34816/50000]\tLoss: 3.4082\tLR: 0.100000\nTraining Epoch: 11 [34944/50000]\tLoss: 3.5671\tLR: 0.100000\nTraining Epoch: 11 [35072/50000]\tLoss: 3.3017\tLR: 0.100000\nTraining Epoch: 11 [35200/50000]\tLoss: 3.5118\tLR: 0.100000\nTraining Epoch: 11 [35328/50000]\tLoss: 3.7214\tLR: 0.100000\nTraining Epoch: 11 [35456/50000]\tLoss: 3.7031\tLR: 0.100000\nTraining Epoch: 11 [35584/50000]\tLoss: 3.5905\tLR: 0.100000\nTraining Epoch: 11 [35712/50000]\tLoss: 3.5652\tLR: 0.100000\nTraining Epoch: 11 [35840/50000]\tLoss: 3.5688\tLR: 0.100000\nTraining Epoch: 11 [35968/50000]\tLoss: 3.5663\tLR: 0.100000\nTraining Epoch: 11 [36096/50000]\tLoss: 3.4307\tLR: 0.100000\nTraining Epoch: 11 [36224/50000]\tLoss: 3.4325\tLR: 0.100000\nTraining Epoch: 11 [36352/50000]\tLoss: 3.6267\tLR: 0.100000\nTraining Epoch: 11 [36480/50000]\tLoss: 3.4631\tLR: 0.100000\nTraining Epoch: 11 [36608/50000]\tLoss: 3.5820\tLR: 0.100000\nTraining Epoch: 11 [36736/50000]\tLoss: 3.3704\tLR: 0.100000\nTraining Epoch: 11 [36864/50000]\tLoss: 3.4014\tLR: 0.100000\nTraining Epoch: 11 [36992/50000]\tLoss: 3.4966\tLR: 0.100000\nTraining Epoch: 11 [37120/50000]\tLoss: 3.3567\tLR: 0.100000\nTraining Epoch: 11 [37248/50000]\tLoss: 3.5455\tLR: 0.100000\nTraining Epoch: 11 [37376/50000]\tLoss: 3.5723\tLR: 0.100000\nTraining Epoch: 11 [37504/50000]\tLoss: 3.6073\tLR: 0.100000\nTraining Epoch: 11 [37632/50000]\tLoss: 3.6550\tLR: 0.100000\nTraining Epoch: 11 [37760/50000]\tLoss: 3.7503\tLR: 0.100000\nTraining Epoch: 11 [37888/50000]\tLoss: 3.6103\tLR: 0.100000\nTraining Epoch: 11 [38016/50000]\tLoss: 3.5963\tLR: 0.100000\nTraining Epoch: 11 [38144/50000]\tLoss: 3.4847\tLR: 0.100000\nTraining Epoch: 11 [38272/50000]\tLoss: 3.7339\tLR: 0.100000\nTraining Epoch: 11 [38400/50000]\tLoss: 3.6584\tLR: 0.100000\nTraining Epoch: 11 [38528/50000]\tLoss: 3.4956\tLR: 0.100000\nTraining Epoch: 11 [38656/50000]\tLoss: 3.5725\tLR: 0.100000\nTraining Epoch: 11 [38784/50000]\tLoss: 3.3890\tLR: 0.100000\nTraining Epoch: 11 [38912/50000]\tLoss: 3.5968\tLR: 0.100000\nTraining Epoch: 11 [39040/50000]\tLoss: 3.3076\tLR: 0.100000\nTraining Epoch: 11 [39168/50000]\tLoss: 3.4983\tLR: 0.100000\nTraining Epoch: 11 [39296/50000]\tLoss: 3.6263\tLR: 0.100000\nTraining Epoch: 11 [39424/50000]\tLoss: 3.5624\tLR: 0.100000\nTraining Epoch: 11 [39552/50000]\tLoss: 3.6892\tLR: 0.100000\n","name":"stdout"},{"output_type":"stream","text":"Training Epoch: 11 [39680/50000]\tLoss: 3.7741\tLR: 0.100000\nTraining Epoch: 11 [39808/50000]\tLoss: 3.6564\tLR: 0.100000\nTraining Epoch: 11 [39936/50000]\tLoss: 3.6421\tLR: 0.100000\nTraining Epoch: 11 [40064/50000]\tLoss: 3.3350\tLR: 0.100000\nTraining Epoch: 11 [40192/50000]\tLoss: 3.3188\tLR: 0.100000\nTraining Epoch: 11 [40320/50000]\tLoss: 3.4138\tLR: 0.100000\nTraining Epoch: 11 [40448/50000]\tLoss: 3.4784\tLR: 0.100000\nTraining Epoch: 11 [40576/50000]\tLoss: 3.6864\tLR: 0.100000\nTraining Epoch: 11 [40704/50000]\tLoss: 3.2788\tLR: 0.100000\nTraining Epoch: 11 [40832/50000]\tLoss: 3.3089\tLR: 0.100000\nTraining Epoch: 11 [40960/50000]\tLoss: 3.5313\tLR: 0.100000\nTraining Epoch: 11 [41088/50000]\tLoss: 3.5541\tLR: 0.100000\nTraining Epoch: 11 [41216/50000]\tLoss: 3.7470\tLR: 0.100000\nTraining Epoch: 11 [41344/50000]\tLoss: 3.4345\tLR: 0.100000\nTraining Epoch: 11 [41472/50000]\tLoss: 3.5608\tLR: 0.100000\nTraining Epoch: 11 [41600/50000]\tLoss: 3.6093\tLR: 0.100000\nTraining Epoch: 11 [41728/50000]\tLoss: 3.7978\tLR: 0.100000\nTraining Epoch: 11 [41856/50000]\tLoss: 3.8039\tLR: 0.100000\nTraining Epoch: 11 [41984/50000]\tLoss: 3.4862\tLR: 0.100000\nTraining Epoch: 11 [42112/50000]\tLoss: 3.6272\tLR: 0.100000\nTraining Epoch: 11 [42240/50000]\tLoss: 3.3758\tLR: 0.100000\nTraining Epoch: 11 [42368/50000]\tLoss: 3.6320\tLR: 0.100000\nTraining Epoch: 11 [42496/50000]\tLoss: 3.4901\tLR: 0.100000\nTraining Epoch: 11 [42624/50000]\tLoss: 3.6545\tLR: 0.100000\nTraining Epoch: 11 [42752/50000]\tLoss: 3.8051\tLR: 0.100000\nTraining Epoch: 11 [42880/50000]\tLoss: 3.4376\tLR: 0.100000\nTraining Epoch: 11 [43008/50000]\tLoss: 3.7377\tLR: 0.100000\nTraining Epoch: 11 [43136/50000]\tLoss: 3.6610\tLR: 0.100000\nTraining Epoch: 11 [43264/50000]\tLoss: 3.7457\tLR: 0.100000\nTraining Epoch: 11 [43392/50000]\tLoss: 3.5386\tLR: 0.100000\nTraining Epoch: 11 [43520/50000]\tLoss: 3.6087\tLR: 0.100000\nTraining Epoch: 11 [43648/50000]\tLoss: 3.6628\tLR: 0.100000\nTraining Epoch: 11 [43776/50000]\tLoss: 3.4670\tLR: 0.100000\nTraining Epoch: 11 [43904/50000]\tLoss: 3.6476\tLR: 0.100000\nTraining Epoch: 11 [44032/50000]\tLoss: 3.7864\tLR: 0.100000\nTraining Epoch: 11 [44160/50000]\tLoss: 3.6612\tLR: 0.100000\nTraining Epoch: 11 [44288/50000]\tLoss: 3.6238\tLR: 0.100000\nTraining Epoch: 11 [44416/50000]\tLoss: 3.6899\tLR: 0.100000\nTraining Epoch: 11 [44544/50000]\tLoss: 3.3705\tLR: 0.100000\nTraining Epoch: 11 [44672/50000]\tLoss: 3.3804\tLR: 0.100000\nTraining Epoch: 11 [44800/50000]\tLoss: 3.6004\tLR: 0.100000\nTraining Epoch: 11 [44928/50000]\tLoss: 3.5306\tLR: 0.100000\nTraining Epoch: 11 [45056/50000]\tLoss: 3.6042\tLR: 0.100000\nTraining Epoch: 11 [45184/50000]\tLoss: 3.4946\tLR: 0.100000\nTraining Epoch: 11 [45312/50000]\tLoss: 3.7249\tLR: 0.100000\nTraining Epoch: 11 [45440/50000]\tLoss: 3.6519\tLR: 0.100000\nTraining Epoch: 11 [45568/50000]\tLoss: 3.5427\tLR: 0.100000\nTraining Epoch: 11 [45696/50000]\tLoss: 3.3969\tLR: 0.100000\nTraining Epoch: 11 [45824/50000]\tLoss: 3.3570\tLR: 0.100000\nTraining Epoch: 11 [45952/50000]\tLoss: 3.5188\tLR: 0.100000\nTraining Epoch: 11 [46080/50000]\tLoss: 3.7522\tLR: 0.100000\nTraining Epoch: 11 [46208/50000]\tLoss: 3.5059\tLR: 0.100000\nTraining Epoch: 11 [46336/50000]\tLoss: 3.3974\tLR: 0.100000\nTraining Epoch: 11 [46464/50000]\tLoss: 3.8408\tLR: 0.100000\nTraining Epoch: 11 [46592/50000]\tLoss: 3.5160\tLR: 0.100000\nTraining Epoch: 11 [46720/50000]\tLoss: 3.4942\tLR: 0.100000\nTraining Epoch: 11 [46848/50000]\tLoss: 3.4268\tLR: 0.100000\nTraining Epoch: 11 [46976/50000]\tLoss: 3.4654\tLR: 0.100000\nTraining Epoch: 11 [47104/50000]\tLoss: 3.3789\tLR: 0.100000\nTraining Epoch: 11 [47232/50000]\tLoss: 3.8241\tLR: 0.100000\nTraining Epoch: 11 [47360/50000]\tLoss: 3.5250\tLR: 0.100000\nTraining Epoch: 11 [47488/50000]\tLoss: 3.4313\tLR: 0.100000\nTraining Epoch: 11 [47616/50000]\tLoss: 3.7001\tLR: 0.100000\nTraining Epoch: 11 [47744/50000]\tLoss: 3.5172\tLR: 0.100000\nTraining Epoch: 11 [47872/50000]\tLoss: 3.7282\tLR: 0.100000\nTraining Epoch: 11 [48000/50000]\tLoss: 3.7238\tLR: 0.100000\nTraining Epoch: 11 [48128/50000]\tLoss: 3.7168\tLR: 0.100000\nTraining Epoch: 11 [48256/50000]\tLoss: 3.6595\tLR: 0.100000\nTraining Epoch: 11 [48384/50000]\tLoss: 3.7937\tLR: 0.100000\nTraining Epoch: 11 [48512/50000]\tLoss: 3.5466\tLR: 0.100000\nTraining Epoch: 11 [48640/50000]\tLoss: 3.5680\tLR: 0.100000\nTraining Epoch: 11 [48768/50000]\tLoss: 3.2997\tLR: 0.100000\nTraining Epoch: 11 [48896/50000]\tLoss: 3.6145\tLR: 0.100000\nTraining Epoch: 11 [49024/50000]\tLoss: 3.5430\tLR: 0.100000\nTraining Epoch: 11 [49152/50000]\tLoss: 3.6090\tLR: 0.100000\nTraining Epoch: 11 [49280/50000]\tLoss: 3.6708\tLR: 0.100000\nTraining Epoch: 11 [49408/50000]\tLoss: 3.4919\tLR: 0.100000\nTraining Epoch: 11 [49536/50000]\tLoss: 3.4927\tLR: 0.100000\nTraining Epoch: 11 [49664/50000]\tLoss: 3.5413\tLR: 0.100000\nTraining Epoch: 11 [49792/50000]\tLoss: 3.4742\tLR: 0.100000\nTraining Epoch: 11 [49920/50000]\tLoss: 3.6867\tLR: 0.100000\nTraining Epoch: 11 [50000/50000]\tLoss: 3.5642\tLR: 0.100000\n=========================================================================================\nTraining Epoch: 12 [128/50000]\tLoss: 3.4252\tLR: 0.100000\nTraining Epoch: 12 [256/50000]\tLoss: 3.6235\tLR: 0.100000\nTraining Epoch: 12 [384/50000]\tLoss: 3.6480\tLR: 0.100000\nTraining Epoch: 12 [512/50000]\tLoss: 3.3671\tLR: 0.100000\nTraining Epoch: 12 [640/50000]\tLoss: 3.6576\tLR: 0.100000\nTraining Epoch: 12 [768/50000]\tLoss: 3.5760\tLR: 0.100000\nTraining Epoch: 12 [896/50000]\tLoss: 3.7926\tLR: 0.100000\nTraining Epoch: 12 [1024/50000]\tLoss: 3.5799\tLR: 0.100000\nTraining Epoch: 12 [1152/50000]\tLoss: 3.4976\tLR: 0.100000\nTraining Epoch: 12 [1280/50000]\tLoss: 3.5877\tLR: 0.100000\nTraining Epoch: 12 [1408/50000]\tLoss: 3.7041\tLR: 0.100000\nTraining Epoch: 12 [1536/50000]\tLoss: 3.6185\tLR: 0.100000\nTraining Epoch: 12 [1664/50000]\tLoss: 3.5680\tLR: 0.100000\nTraining Epoch: 12 [1792/50000]\tLoss: 3.5462\tLR: 0.100000\nTraining Epoch: 12 [1920/50000]\tLoss: 3.5272\tLR: 0.100000\nTraining Epoch: 12 [2048/50000]\tLoss: 3.5104\tLR: 0.100000\nTraining Epoch: 12 [2176/50000]\tLoss: 3.7195\tLR: 0.100000\nTraining Epoch: 12 [2304/50000]\tLoss: 3.6233\tLR: 0.100000\nTraining Epoch: 12 [2432/50000]\tLoss: 3.5067\tLR: 0.100000\nTraining Epoch: 12 [2560/50000]\tLoss: 3.6665\tLR: 0.100000\nTraining Epoch: 12 [2688/50000]\tLoss: 3.5667\tLR: 0.100000\nTraining Epoch: 12 [2816/50000]\tLoss: 3.4311\tLR: 0.100000\nTraining Epoch: 12 [2944/50000]\tLoss: 3.5879\tLR: 0.100000\nTraining Epoch: 12 [3072/50000]\tLoss: 3.6536\tLR: 0.100000\nTraining Epoch: 12 [3200/50000]\tLoss: 3.6488\tLR: 0.100000\nTraining Epoch: 12 [3328/50000]\tLoss: 3.5808\tLR: 0.100000\nTraining Epoch: 12 [3456/50000]\tLoss: 3.5375\tLR: 0.100000\nTraining Epoch: 12 [3584/50000]\tLoss: 3.5535\tLR: 0.100000\nTraining Epoch: 12 [3712/50000]\tLoss: 3.5111\tLR: 0.100000\nTraining Epoch: 12 [3840/50000]\tLoss: 3.7065\tLR: 0.100000\nTraining Epoch: 12 [3968/50000]\tLoss: 3.4612\tLR: 0.100000\nTraining Epoch: 12 [4096/50000]\tLoss: 3.4721\tLR: 0.100000\nTraining Epoch: 12 [4224/50000]\tLoss: 3.4879\tLR: 0.100000\nTraining Epoch: 12 [4352/50000]\tLoss: 3.6574\tLR: 0.100000\nTraining Epoch: 12 [4480/50000]\tLoss: 3.5512\tLR: 0.100000\nTraining Epoch: 12 [4608/50000]\tLoss: 3.6344\tLR: 0.100000\nTraining Epoch: 12 [4736/50000]\tLoss: 3.4699\tLR: 0.100000\nTraining Epoch: 12 [4864/50000]\tLoss: 3.4201\tLR: 0.100000\nTraining Epoch: 12 [4992/50000]\tLoss: 3.6871\tLR: 0.100000\nTraining Epoch: 12 [5120/50000]\tLoss: 3.5721\tLR: 0.100000\nTraining Epoch: 12 [5248/50000]\tLoss: 3.6187\tLR: 0.100000\nTraining Epoch: 12 [5376/50000]\tLoss: 3.5171\tLR: 0.100000\nTraining Epoch: 12 [5504/50000]\tLoss: 3.3505\tLR: 0.100000\nTraining Epoch: 12 [5632/50000]\tLoss: 3.6596\tLR: 0.100000\nTraining Epoch: 12 [5760/50000]\tLoss: 3.6370\tLR: 0.100000\nTraining Epoch: 12 [5888/50000]\tLoss: 3.6206\tLR: 0.100000\nTraining Epoch: 12 [6016/50000]\tLoss: 3.5877\tLR: 0.100000\nTraining Epoch: 12 [6144/50000]\tLoss: 3.5296\tLR: 0.100000\nTraining Epoch: 12 [6272/50000]\tLoss: 3.6416\tLR: 0.100000\nTraining Epoch: 12 [6400/50000]\tLoss: 3.4754\tLR: 0.100000\nTraining Epoch: 12 [6528/50000]\tLoss: 3.4602\tLR: 0.100000\nTraining Epoch: 12 [6656/50000]\tLoss: 3.6550\tLR: 0.100000\nTraining Epoch: 12 [6784/50000]\tLoss: 3.4525\tLR: 0.100000\nTraining Epoch: 12 [6912/50000]\tLoss: 3.5533\tLR: 0.100000\nTraining Epoch: 12 [7040/50000]\tLoss: 3.5380\tLR: 0.100000\nTraining Epoch: 12 [7168/50000]\tLoss: 3.3335\tLR: 0.100000\nTraining Epoch: 12 [7296/50000]\tLoss: 3.6728\tLR: 0.100000\nTraining Epoch: 12 [7424/50000]\tLoss: 3.4459\tLR: 0.100000\nTraining Epoch: 12 [7552/50000]\tLoss: 3.3830\tLR: 0.100000\n","name":"stdout"},{"output_type":"stream","text":"Training Epoch: 12 [7680/50000]\tLoss: 3.6682\tLR: 0.100000\nTraining Epoch: 12 [7808/50000]\tLoss: 3.5102\tLR: 0.100000\nTraining Epoch: 12 [7936/50000]\tLoss: 3.6902\tLR: 0.100000\nTraining Epoch: 12 [8064/50000]\tLoss: 3.4903\tLR: 0.100000\nTraining Epoch: 12 [8192/50000]\tLoss: 3.4816\tLR: 0.100000\nTraining Epoch: 12 [8320/50000]\tLoss: 3.2170\tLR: 0.100000\nTraining Epoch: 12 [8448/50000]\tLoss: 3.3582\tLR: 0.100000\nTraining Epoch: 12 [8576/50000]\tLoss: 3.4668\tLR: 0.100000\nTraining Epoch: 12 [8704/50000]\tLoss: 3.4042\tLR: 0.100000\nTraining Epoch: 12 [8832/50000]\tLoss: 3.5927\tLR: 0.100000\nTraining Epoch: 12 [8960/50000]\tLoss: 3.6088\tLR: 0.100000\nTraining Epoch: 12 [9088/50000]\tLoss: 3.5000\tLR: 0.100000\nTraining Epoch: 12 [9216/50000]\tLoss: 3.4415\tLR: 0.100000\nTraining Epoch: 12 [9344/50000]\tLoss: 3.7633\tLR: 0.100000\nTraining Epoch: 12 [9472/50000]\tLoss: 3.6426\tLR: 0.100000\nTraining Epoch: 12 [9600/50000]\tLoss: 3.7827\tLR: 0.100000\nTraining Epoch: 12 [9728/50000]\tLoss: 3.5159\tLR: 0.100000\nTraining Epoch: 12 [9856/50000]\tLoss: 3.6982\tLR: 0.100000\nTraining Epoch: 12 [9984/50000]\tLoss: 3.4843\tLR: 0.100000\nTraining Epoch: 12 [10112/50000]\tLoss: 3.4508\tLR: 0.100000\nTraining Epoch: 12 [10240/50000]\tLoss: 3.5221\tLR: 0.100000\nTraining Epoch: 12 [10368/50000]\tLoss: 3.3356\tLR: 0.100000\nTraining Epoch: 12 [10496/50000]\tLoss: 3.6335\tLR: 0.100000\nTraining Epoch: 12 [10624/50000]\tLoss: 3.6720\tLR: 0.100000\nTraining Epoch: 12 [10752/50000]\tLoss: 3.6978\tLR: 0.100000\nTraining Epoch: 12 [10880/50000]\tLoss: 3.7953\tLR: 0.100000\nTraining Epoch: 12 [11008/50000]\tLoss: 3.7866\tLR: 0.100000\nTraining Epoch: 12 [11136/50000]\tLoss: 3.9018\tLR: 0.100000\nTraining Epoch: 12 [11264/50000]\tLoss: 3.5271\tLR: 0.100000\nTraining Epoch: 12 [11392/50000]\tLoss: 3.6724\tLR: 0.100000\nTraining Epoch: 12 [11520/50000]\tLoss: 3.7146\tLR: 0.100000\nTraining Epoch: 12 [11648/50000]\tLoss: 3.6054\tLR: 0.100000\nTraining Epoch: 12 [11776/50000]\tLoss: 3.7557\tLR: 0.100000\nTraining Epoch: 12 [11904/50000]\tLoss: 3.6481\tLR: 0.100000\nTraining Epoch: 12 [12032/50000]\tLoss: 3.7682\tLR: 0.100000\nTraining Epoch: 12 [12160/50000]\tLoss: 3.5799\tLR: 0.100000\nTraining Epoch: 12 [12288/50000]\tLoss: 3.5301\tLR: 0.100000\nTraining Epoch: 12 [12416/50000]\tLoss: 3.6108\tLR: 0.100000\nTraining Epoch: 12 [12544/50000]\tLoss: 3.7782\tLR: 0.100000\nTraining Epoch: 12 [12672/50000]\tLoss: 3.9283\tLR: 0.100000\nTraining Epoch: 12 [12800/50000]\tLoss: 3.5113\tLR: 0.100000\nTraining Epoch: 12 [12928/50000]\tLoss: 3.6332\tLR: 0.100000\nTraining Epoch: 12 [13056/50000]\tLoss: 3.6644\tLR: 0.100000\nTraining Epoch: 12 [13184/50000]\tLoss: 3.5776\tLR: 0.100000\nTraining Epoch: 12 [13312/50000]\tLoss: 3.6126\tLR: 0.100000\nTraining Epoch: 12 [13440/50000]\tLoss: 3.7911\tLR: 0.100000\nTraining Epoch: 12 [13568/50000]\tLoss: 3.6753\tLR: 0.100000\nTraining Epoch: 12 [13696/50000]\tLoss: 3.5851\tLR: 0.100000\nTraining Epoch: 12 [13824/50000]\tLoss: 3.4118\tLR: 0.100000\nTraining Epoch: 12 [13952/50000]\tLoss: 3.6136\tLR: 0.100000\nTraining Epoch: 12 [14080/50000]\tLoss: 3.4583\tLR: 0.100000\nTraining Epoch: 12 [14208/50000]\tLoss: 3.4725\tLR: 0.100000\nTraining Epoch: 12 [14336/50000]\tLoss: 3.5013\tLR: 0.100000\nTraining Epoch: 12 [14464/50000]\tLoss: 3.5894\tLR: 0.100000\nTraining Epoch: 12 [14592/50000]\tLoss: 3.5884\tLR: 0.100000\nTraining Epoch: 12 [14720/50000]\tLoss: 3.5443\tLR: 0.100000\nTraining Epoch: 12 [14848/50000]\tLoss: 3.5001\tLR: 0.100000\nTraining Epoch: 12 [14976/50000]\tLoss: 3.5702\tLR: 0.100000\nTraining Epoch: 12 [15104/50000]\tLoss: 3.1590\tLR: 0.100000\nTraining Epoch: 12 [15232/50000]\tLoss: 3.5776\tLR: 0.100000\nTraining Epoch: 12 [15360/50000]\tLoss: 3.4390\tLR: 0.100000\nTraining Epoch: 12 [15488/50000]\tLoss: 3.5268\tLR: 0.100000\nTraining Epoch: 12 [15616/50000]\tLoss: 3.2988\tLR: 0.100000\nTraining Epoch: 12 [15744/50000]\tLoss: 3.3890\tLR: 0.100000\nTraining Epoch: 12 [15872/50000]\tLoss: 3.3383\tLR: 0.100000\nTraining Epoch: 12 [16000/50000]\tLoss: 3.7266\tLR: 0.100000\nTraining Epoch: 12 [16128/50000]\tLoss: 3.4993\tLR: 0.100000\nTraining Epoch: 12 [16256/50000]\tLoss: 3.5524\tLR: 0.100000\nTraining Epoch: 12 [16384/50000]\tLoss: 3.6131\tLR: 0.100000\nTraining Epoch: 12 [16512/50000]\tLoss: 3.3798\tLR: 0.100000\nTraining Epoch: 12 [16640/50000]\tLoss: 3.5256\tLR: 0.100000\nTraining Epoch: 12 [16768/50000]\tLoss: 3.5715\tLR: 0.100000\nTraining Epoch: 12 [16896/50000]\tLoss: 3.5832\tLR: 0.100000\nTraining Epoch: 12 [17024/50000]\tLoss: 3.4595\tLR: 0.100000\nTraining Epoch: 12 [17152/50000]\tLoss: 3.2575\tLR: 0.100000\nTraining Epoch: 12 [17280/50000]\tLoss: 3.4820\tLR: 0.100000\nTraining Epoch: 12 [17408/50000]\tLoss: 3.2779\tLR: 0.100000\nTraining Epoch: 12 [17536/50000]\tLoss: 3.5702\tLR: 0.100000\nTraining Epoch: 12 [17664/50000]\tLoss: 3.5229\tLR: 0.100000\nTraining Epoch: 12 [17792/50000]\tLoss: 3.5590\tLR: 0.100000\nTraining Epoch: 12 [17920/50000]\tLoss: 3.8286\tLR: 0.100000\nTraining Epoch: 12 [18048/50000]\tLoss: 3.6327\tLR: 0.100000\nTraining Epoch: 12 [18176/50000]\tLoss: 3.4883\tLR: 0.100000\nTraining Epoch: 12 [18304/50000]\tLoss: 3.5609\tLR: 0.100000\nTraining Epoch: 12 [18432/50000]\tLoss: 3.5317\tLR: 0.100000\nTraining Epoch: 12 [18560/50000]\tLoss: 3.5604\tLR: 0.100000\nTraining Epoch: 12 [18688/50000]\tLoss: 3.5117\tLR: 0.100000\nTraining Epoch: 12 [18816/50000]\tLoss: 3.4827\tLR: 0.100000\nTraining Epoch: 12 [18944/50000]\tLoss: 3.7188\tLR: 0.100000\nTraining Epoch: 12 [19072/50000]\tLoss: 3.5370\tLR: 0.100000\nTraining Epoch: 12 [19200/50000]\tLoss: 3.6861\tLR: 0.100000\nTraining Epoch: 12 [19328/50000]\tLoss: 3.6936\tLR: 0.100000\nTraining Epoch: 12 [19456/50000]\tLoss: 3.6750\tLR: 0.100000\nTraining Epoch: 12 [19584/50000]\tLoss: 3.5532\tLR: 0.100000\nTraining Epoch: 12 [19712/50000]\tLoss: 3.5675\tLR: 0.100000\nTraining Epoch: 12 [19840/50000]\tLoss: 3.5983\tLR: 0.100000\nTraining Epoch: 12 [19968/50000]\tLoss: 3.6428\tLR: 0.100000\nTraining Epoch: 12 [20096/50000]\tLoss: 3.7157\tLR: 0.100000\nTraining Epoch: 12 [20224/50000]\tLoss: 3.6240\tLR: 0.100000\nTraining Epoch: 12 [20352/50000]\tLoss: 3.7563\tLR: 0.100000\nTraining Epoch: 12 [20480/50000]\tLoss: 3.4735\tLR: 0.100000\nTraining Epoch: 12 [20608/50000]\tLoss: 3.7077\tLR: 0.100000\nTraining Epoch: 12 [20736/50000]\tLoss: 3.5454\tLR: 0.100000\nTraining Epoch: 12 [20864/50000]\tLoss: 3.6572\tLR: 0.100000\nTraining Epoch: 12 [20992/50000]\tLoss: 3.5510\tLR: 0.100000\nTraining Epoch: 12 [21120/50000]\tLoss: 3.7323\tLR: 0.100000\nTraining Epoch: 12 [21248/50000]\tLoss: 3.5371\tLR: 0.100000\nTraining Epoch: 12 [21376/50000]\tLoss: 3.5538\tLR: 0.100000\nTraining Epoch: 12 [21504/50000]\tLoss: 3.4354\tLR: 0.100000\nTraining Epoch: 12 [21632/50000]\tLoss: 3.5740\tLR: 0.100000\nTraining Epoch: 12 [21760/50000]\tLoss: 3.6036\tLR: 0.100000\nTraining Epoch: 12 [21888/50000]\tLoss: 3.6630\tLR: 0.100000\nTraining Epoch: 12 [22016/50000]\tLoss: 3.8615\tLR: 0.100000\nTraining Epoch: 12 [22144/50000]\tLoss: 3.5272\tLR: 0.100000\nTraining Epoch: 12 [22272/50000]\tLoss: 3.5825\tLR: 0.100000\nTraining Epoch: 12 [22400/50000]\tLoss: 3.6422\tLR: 0.100000\nTraining Epoch: 12 [22528/50000]\tLoss: 3.5250\tLR: 0.100000\nTraining Epoch: 12 [22656/50000]\tLoss: 3.5766\tLR: 0.100000\nTraining Epoch: 12 [22784/50000]\tLoss: 3.2669\tLR: 0.100000\nTraining Epoch: 12 [22912/50000]\tLoss: 3.4040\tLR: 0.100000\nTraining Epoch: 12 [23040/50000]\tLoss: 3.5614\tLR: 0.100000\nTraining Epoch: 12 [23168/50000]\tLoss: 3.8471\tLR: 0.100000\nTraining Epoch: 12 [23296/50000]\tLoss: 3.5005\tLR: 0.100000\nTraining Epoch: 12 [23424/50000]\tLoss: 3.6427\tLR: 0.100000\nTraining Epoch: 12 [23552/50000]\tLoss: 3.4717\tLR: 0.100000\nTraining Epoch: 12 [23680/50000]\tLoss: 3.4477\tLR: 0.100000\nTraining Epoch: 12 [23808/50000]\tLoss: 3.6594\tLR: 0.100000\nTraining Epoch: 12 [23936/50000]\tLoss: 3.3978\tLR: 0.100000\nTraining Epoch: 12 [24064/50000]\tLoss: 3.4828\tLR: 0.100000\nTraining Epoch: 12 [24192/50000]\tLoss: 3.4323\tLR: 0.100000\nTraining Epoch: 12 [24320/50000]\tLoss: 3.6084\tLR: 0.100000\nTraining Epoch: 12 [24448/50000]\tLoss: 3.3437\tLR: 0.100000\nTraining Epoch: 12 [24576/50000]\tLoss: 3.5517\tLR: 0.100000\nTraining Epoch: 12 [24704/50000]\tLoss: 3.4955\tLR: 0.100000\nTraining Epoch: 12 [24832/50000]\tLoss: 3.5454\tLR: 0.100000\nTraining Epoch: 12 [24960/50000]\tLoss: 3.6181\tLR: 0.100000\nTraining Epoch: 12 [25088/50000]\tLoss: 3.5462\tLR: 0.100000\nTraining Epoch: 12 [25216/50000]\tLoss: 3.6045\tLR: 0.100000\nTraining Epoch: 12 [25344/50000]\tLoss: 3.5659\tLR: 0.100000\nTraining Epoch: 12 [25472/50000]\tLoss: 3.4508\tLR: 0.100000\nTraining Epoch: 12 [25600/50000]\tLoss: 3.4961\tLR: 0.100000\n","name":"stdout"},{"output_type":"stream","text":"Training Epoch: 12 [25728/50000]\tLoss: 3.7068\tLR: 0.100000\nTraining Epoch: 12 [25856/50000]\tLoss: 3.6580\tLR: 0.100000\nTraining Epoch: 12 [25984/50000]\tLoss: 3.3667\tLR: 0.100000\nTraining Epoch: 12 [26112/50000]\tLoss: 3.7266\tLR: 0.100000\nTraining Epoch: 12 [26240/50000]\tLoss: 3.5036\tLR: 0.100000\nTraining Epoch: 12 [26368/50000]\tLoss: 3.5411\tLR: 0.100000\nTraining Epoch: 12 [26496/50000]\tLoss: 3.3991\tLR: 0.100000\nTraining Epoch: 12 [26624/50000]\tLoss: 3.4280\tLR: 0.100000\nTraining Epoch: 12 [26752/50000]\tLoss: 3.6187\tLR: 0.100000\nTraining Epoch: 12 [26880/50000]\tLoss: 3.4627\tLR: 0.100000\nTraining Epoch: 12 [27008/50000]\tLoss: 3.5445\tLR: 0.100000\nTraining Epoch: 12 [27136/50000]\tLoss: 3.5799\tLR: 0.100000\nTraining Epoch: 12 [27264/50000]\tLoss: 3.7497\tLR: 0.100000\nTraining Epoch: 12 [27392/50000]\tLoss: 3.4250\tLR: 0.100000\nTraining Epoch: 12 [27520/50000]\tLoss: 3.4937\tLR: 0.100000\nTraining Epoch: 12 [27648/50000]\tLoss: 3.7339\tLR: 0.100000\nTraining Epoch: 12 [27776/50000]\tLoss: 3.6536\tLR: 0.100000\nTraining Epoch: 12 [27904/50000]\tLoss: 3.5699\tLR: 0.100000\nTraining Epoch: 12 [28032/50000]\tLoss: 3.6488\tLR: 0.100000\nTraining Epoch: 12 [28160/50000]\tLoss: 3.6814\tLR: 0.100000\nTraining Epoch: 12 [28288/50000]\tLoss: 3.4230\tLR: 0.100000\nTraining Epoch: 12 [28416/50000]\tLoss: 3.7741\tLR: 0.100000\nTraining Epoch: 12 [28544/50000]\tLoss: 3.4567\tLR: 0.100000\nTraining Epoch: 12 [28672/50000]\tLoss: 3.5738\tLR: 0.100000\nTraining Epoch: 12 [28800/50000]\tLoss: 3.3789\tLR: 0.100000\nTraining Epoch: 12 [28928/50000]\tLoss: 3.5274\tLR: 0.100000\nTraining Epoch: 12 [29056/50000]\tLoss: 3.6827\tLR: 0.100000\nTraining Epoch: 12 [29184/50000]\tLoss: 3.5869\tLR: 0.100000\nTraining Epoch: 12 [29312/50000]\tLoss: 3.6402\tLR: 0.100000\nTraining Epoch: 12 [29440/50000]\tLoss: 3.7945\tLR: 0.100000\nTraining Epoch: 12 [29568/50000]\tLoss: 3.7963\tLR: 0.100000\nTraining Epoch: 12 [29696/50000]\tLoss: 3.6329\tLR: 0.100000\nTraining Epoch: 12 [29824/50000]\tLoss: 3.7070\tLR: 0.100000\nTraining Epoch: 12 [29952/50000]\tLoss: 3.4681\tLR: 0.100000\nTraining Epoch: 12 [30080/50000]\tLoss: 3.5098\tLR: 0.100000\nTraining Epoch: 12 [30208/50000]\tLoss: 3.5014\tLR: 0.100000\nTraining Epoch: 12 [30336/50000]\tLoss: 3.4339\tLR: 0.100000\nTraining Epoch: 12 [30464/50000]\tLoss: 3.6167\tLR: 0.100000\nTraining Epoch: 12 [30592/50000]\tLoss: 3.5135\tLR: 0.100000\nTraining Epoch: 12 [30720/50000]\tLoss: 3.1910\tLR: 0.100000\nTraining Epoch: 12 [30848/50000]\tLoss: 3.6636\tLR: 0.100000\nTraining Epoch: 12 [30976/50000]\tLoss: 3.5917\tLR: 0.100000\nTraining Epoch: 12 [31104/50000]\tLoss: 3.4100\tLR: 0.100000\nTraining Epoch: 12 [31232/50000]\tLoss: 3.3269\tLR: 0.100000\nTraining Epoch: 12 [31360/50000]\tLoss: 3.4392\tLR: 0.100000\nTraining Epoch: 12 [31488/50000]\tLoss: 3.8018\tLR: 0.100000\nTraining Epoch: 12 [31616/50000]\tLoss: 3.5003\tLR: 0.100000\nTraining Epoch: 12 [31744/50000]\tLoss: 3.3405\tLR: 0.100000\nTraining Epoch: 12 [31872/50000]\tLoss: 3.5387\tLR: 0.100000\nTraining Epoch: 12 [32000/50000]\tLoss: 3.7194\tLR: 0.100000\nTraining Epoch: 12 [32128/50000]\tLoss: 3.7060\tLR: 0.100000\nTraining Epoch: 12 [32256/50000]\tLoss: 3.3591\tLR: 0.100000\nTraining Epoch: 12 [32384/50000]\tLoss: 3.5705\tLR: 0.100000\nTraining Epoch: 12 [32512/50000]\tLoss: 3.4601\tLR: 0.100000\nTraining Epoch: 12 [32640/50000]\tLoss: 3.6801\tLR: 0.100000\nTraining Epoch: 12 [32768/50000]\tLoss: 3.5972\tLR: 0.100000\nTraining Epoch: 12 [32896/50000]\tLoss: 3.7591\tLR: 0.100000\nTraining Epoch: 12 [33024/50000]\tLoss: 3.4675\tLR: 0.100000\nTraining Epoch: 12 [33152/50000]\tLoss: 3.5619\tLR: 0.100000\nTraining Epoch: 12 [33280/50000]\tLoss: 3.4798\tLR: 0.100000\nTraining Epoch: 12 [33408/50000]\tLoss: 3.4738\tLR: 0.100000\nTraining Epoch: 12 [33536/50000]\tLoss: 3.4167\tLR: 0.100000\nTraining Epoch: 12 [33664/50000]\tLoss: 3.4540\tLR: 0.100000\nTraining Epoch: 12 [33792/50000]\tLoss: 3.5447\tLR: 0.100000\nTraining Epoch: 12 [33920/50000]\tLoss: 3.8062\tLR: 0.100000\nTraining Epoch: 12 [34048/50000]\tLoss: 3.7117\tLR: 0.100000\nTraining Epoch: 12 [34176/50000]\tLoss: 3.3816\tLR: 0.100000\nTraining Epoch: 12 [34304/50000]\tLoss: 3.7831\tLR: 0.100000\nTraining Epoch: 12 [34432/50000]\tLoss: 3.7045\tLR: 0.100000\nTraining Epoch: 12 [34560/50000]\tLoss: 3.3428\tLR: 0.100000\nTraining Epoch: 12 [34688/50000]\tLoss: 3.3441\tLR: 0.100000\nTraining Epoch: 12 [34816/50000]\tLoss: 3.3213\tLR: 0.100000\nTraining Epoch: 12 [34944/50000]\tLoss: 3.6435\tLR: 0.100000\nTraining Epoch: 12 [35072/50000]\tLoss: 3.5470\tLR: 0.100000\nTraining Epoch: 12 [35200/50000]\tLoss: 3.6544\tLR: 0.100000\nTraining Epoch: 12 [35328/50000]\tLoss: 3.7657\tLR: 0.100000\nTraining Epoch: 12 [35456/50000]\tLoss: 3.3545\tLR: 0.100000\nTraining Epoch: 12 [35584/50000]\tLoss: 3.5760\tLR: 0.100000\nTraining Epoch: 12 [35712/50000]\tLoss: 3.5203\tLR: 0.100000\nTraining Epoch: 12 [35840/50000]\tLoss: 3.7164\tLR: 0.100000\nTraining Epoch: 12 [35968/50000]\tLoss: 3.4188\tLR: 0.100000\nTraining Epoch: 12 [36096/50000]\tLoss: 3.6478\tLR: 0.100000\nTraining Epoch: 12 [36224/50000]\tLoss: 3.6249\tLR: 0.100000\nTraining Epoch: 12 [36352/50000]\tLoss: 3.8502\tLR: 0.100000\nTraining Epoch: 12 [36480/50000]\tLoss: 3.6008\tLR: 0.100000\nTraining Epoch: 12 [36608/50000]\tLoss: 3.6854\tLR: 0.100000\nTraining Epoch: 12 [36736/50000]\tLoss: 3.5049\tLR: 0.100000\nTraining Epoch: 12 [36864/50000]\tLoss: 3.7157\tLR: 0.100000\nTraining Epoch: 12 [36992/50000]\tLoss: 3.4407\tLR: 0.100000\nTraining Epoch: 12 [37120/50000]\tLoss: 3.5983\tLR: 0.100000\nTraining Epoch: 12 [37248/50000]\tLoss: 3.6316\tLR: 0.100000\nTraining Epoch: 12 [37376/50000]\tLoss: 3.5227\tLR: 0.100000\nTraining Epoch: 12 [37504/50000]\tLoss: 3.5843\tLR: 0.100000\nTraining Epoch: 12 [37632/50000]\tLoss: 3.3022\tLR: 0.100000\nTraining Epoch: 12 [37760/50000]\tLoss: 3.5925\tLR: 0.100000\nTraining Epoch: 12 [37888/50000]\tLoss: 3.3607\tLR: 0.100000\nTraining Epoch: 12 [38016/50000]\tLoss: 3.5066\tLR: 0.100000\nTraining Epoch: 12 [38144/50000]\tLoss: 3.6178\tLR: 0.100000\nTraining Epoch: 12 [38272/50000]\tLoss: 3.5095\tLR: 0.100000\nTraining Epoch: 12 [38400/50000]\tLoss: 3.8466\tLR: 0.100000\nTraining Epoch: 12 [38528/50000]\tLoss: 3.6473\tLR: 0.100000\nTraining Epoch: 12 [38656/50000]\tLoss: 3.4242\tLR: 0.100000\nTraining Epoch: 12 [38784/50000]\tLoss: 3.6188\tLR: 0.100000\nTraining Epoch: 12 [38912/50000]\tLoss: 3.6338\tLR: 0.100000\nTraining Epoch: 12 [39040/50000]\tLoss: 3.6903\tLR: 0.100000\nTraining Epoch: 12 [39168/50000]\tLoss: 3.5083\tLR: 0.100000\nTraining Epoch: 12 [39296/50000]\tLoss: 3.4608\tLR: 0.100000\nTraining Epoch: 12 [39424/50000]\tLoss: 3.6447\tLR: 0.100000\nTraining Epoch: 12 [39552/50000]\tLoss: 3.5822\tLR: 0.100000\nTraining Epoch: 12 [39680/50000]\tLoss: 3.6299\tLR: 0.100000\nTraining Epoch: 12 [39808/50000]\tLoss: 3.5411\tLR: 0.100000\nTraining Epoch: 12 [39936/50000]\tLoss: 3.6031\tLR: 0.100000\nTraining Epoch: 12 [40064/50000]\tLoss: 3.5951\tLR: 0.100000\nTraining Epoch: 12 [40192/50000]\tLoss: 3.7611\tLR: 0.100000\nTraining Epoch: 12 [40320/50000]\tLoss: 3.6027\tLR: 0.100000\nTraining Epoch: 12 [40448/50000]\tLoss: 3.6673\tLR: 0.100000\nTraining Epoch: 12 [40576/50000]\tLoss: 3.7220\tLR: 0.100000\nTraining Epoch: 12 [40704/50000]\tLoss: 3.6069\tLR: 0.100000\nTraining Epoch: 12 [40832/50000]\tLoss: 3.6625\tLR: 0.100000\nTraining Epoch: 12 [40960/50000]\tLoss: 3.3918\tLR: 0.100000\nTraining Epoch: 12 [41088/50000]\tLoss: 3.7052\tLR: 0.100000\nTraining Epoch: 12 [41216/50000]\tLoss: 3.5706\tLR: 0.100000\nTraining Epoch: 12 [41344/50000]\tLoss: 3.5190\tLR: 0.100000\nTraining Epoch: 12 [41472/50000]\tLoss: 3.6333\tLR: 0.100000\nTraining Epoch: 12 [41600/50000]\tLoss: 3.7147\tLR: 0.100000\nTraining Epoch: 12 [41728/50000]\tLoss: 3.6894\tLR: 0.100000\nTraining Epoch: 12 [41856/50000]\tLoss: 3.6061\tLR: 0.100000\nTraining Epoch: 12 [41984/50000]\tLoss: 3.6364\tLR: 0.100000\nTraining Epoch: 12 [42112/50000]\tLoss: 3.3628\tLR: 0.100000\nTraining Epoch: 12 [42240/50000]\tLoss: 3.3273\tLR: 0.100000\nTraining Epoch: 12 [42368/50000]\tLoss: 3.5208\tLR: 0.100000\nTraining Epoch: 12 [42496/50000]\tLoss: 3.3877\tLR: 0.100000\nTraining Epoch: 12 [42624/50000]\tLoss: 3.3579\tLR: 0.100000\nTraining Epoch: 12 [42752/50000]\tLoss: 3.4817\tLR: 0.100000\nTraining Epoch: 12 [42880/50000]\tLoss: 3.4653\tLR: 0.100000\nTraining Epoch: 12 [43008/50000]\tLoss: 3.5264\tLR: 0.100000\nTraining Epoch: 12 [43136/50000]\tLoss: 3.7500\tLR: 0.100000\nTraining Epoch: 12 [43264/50000]\tLoss: 3.6004\tLR: 0.100000\nTraining Epoch: 12 [43392/50000]\tLoss: 3.6351\tLR: 0.100000\nTraining Epoch: 12 [43520/50000]\tLoss: 3.8217\tLR: 0.100000\nTraining Epoch: 12 [43648/50000]\tLoss: 3.4245\tLR: 0.100000\n","name":"stdout"},{"output_type":"stream","text":"Training Epoch: 12 [43776/50000]\tLoss: 3.5965\tLR: 0.100000\nTraining Epoch: 12 [43904/50000]\tLoss: 3.5365\tLR: 0.100000\nTraining Epoch: 12 [44032/50000]\tLoss: 3.4752\tLR: 0.100000\nTraining Epoch: 12 [44160/50000]\tLoss: 3.5127\tLR: 0.100000\nTraining Epoch: 12 [44288/50000]\tLoss: 3.3791\tLR: 0.100000\nTraining Epoch: 12 [44416/50000]\tLoss: 3.6054\tLR: 0.100000\nTraining Epoch: 12 [44544/50000]\tLoss: 3.4882\tLR: 0.100000\nTraining Epoch: 12 [44672/50000]\tLoss: 3.3588\tLR: 0.100000\nTraining Epoch: 12 [44800/50000]\tLoss: 3.6617\tLR: 0.100000\nTraining Epoch: 12 [44928/50000]\tLoss: 3.4745\tLR: 0.100000\nTraining Epoch: 12 [45056/50000]\tLoss: 3.5018\tLR: 0.100000\nTraining Epoch: 12 [45184/50000]\tLoss: 3.4329\tLR: 0.100000\nTraining Epoch: 12 [45312/50000]\tLoss: 3.6024\tLR: 0.100000\nTraining Epoch: 12 [45440/50000]\tLoss: 3.6248\tLR: 0.100000\nTraining Epoch: 12 [45568/50000]\tLoss: 3.5768\tLR: 0.100000\nTraining Epoch: 12 [45696/50000]\tLoss: 3.6293\tLR: 0.100000\nTraining Epoch: 12 [45824/50000]\tLoss: 3.4609\tLR: 0.100000\nTraining Epoch: 12 [45952/50000]\tLoss: 3.7993\tLR: 0.100000\nTraining Epoch: 12 [46080/50000]\tLoss: 3.3634\tLR: 0.100000\nTraining Epoch: 12 [46208/50000]\tLoss: 3.4414\tLR: 0.100000\nTraining Epoch: 12 [46336/50000]\tLoss: 3.5372\tLR: 0.100000\nTraining Epoch: 12 [46464/50000]\tLoss: 3.3233\tLR: 0.100000\nTraining Epoch: 12 [46592/50000]\tLoss: 3.5842\tLR: 0.100000\nTraining Epoch: 12 [46720/50000]\tLoss: 3.3060\tLR: 0.100000\nTraining Epoch: 12 [46848/50000]\tLoss: 3.5769\tLR: 0.100000\nTraining Epoch: 12 [46976/50000]\tLoss: 3.7758\tLR: 0.100000\nTraining Epoch: 12 [47104/50000]\tLoss: 3.4768\tLR: 0.100000\nTraining Epoch: 12 [47232/50000]\tLoss: 3.3805\tLR: 0.100000\nTraining Epoch: 12 [47360/50000]\tLoss: 3.3373\tLR: 0.100000\nTraining Epoch: 12 [47488/50000]\tLoss: 3.6451\tLR: 0.100000\nTraining Epoch: 12 [47616/50000]\tLoss: 3.2495\tLR: 0.100000\nTraining Epoch: 12 [47744/50000]\tLoss: 3.5839\tLR: 0.100000\nTraining Epoch: 12 [47872/50000]\tLoss: 3.5040\tLR: 0.100000\nTraining Epoch: 12 [48000/50000]\tLoss: 3.5018\tLR: 0.100000\nTraining Epoch: 12 [48128/50000]\tLoss: 3.4516\tLR: 0.100000\nTraining Epoch: 12 [48256/50000]\tLoss: 3.7409\tLR: 0.100000\nTraining Epoch: 12 [48384/50000]\tLoss: 3.4955\tLR: 0.100000\nTraining Epoch: 12 [48512/50000]\tLoss: 3.3393\tLR: 0.100000\nTraining Epoch: 12 [48640/50000]\tLoss: 3.4521\tLR: 0.100000\nTraining Epoch: 12 [48768/50000]\tLoss: 3.3445\tLR: 0.100000\nTraining Epoch: 12 [48896/50000]\tLoss: 3.4725\tLR: 0.100000\nTraining Epoch: 12 [49024/50000]\tLoss: 3.8718\tLR: 0.100000\nTraining Epoch: 12 [49152/50000]\tLoss: 3.3557\tLR: 0.100000\nTraining Epoch: 12 [49280/50000]\tLoss: 3.3639\tLR: 0.100000\nTraining Epoch: 12 [49408/50000]\tLoss: 3.6123\tLR: 0.100000\nTraining Epoch: 12 [49536/50000]\tLoss: 3.3519\tLR: 0.100000\nTraining Epoch: 12 [49664/50000]\tLoss: 3.2982\tLR: 0.100000\nTraining Epoch: 12 [49792/50000]\tLoss: 3.5467\tLR: 0.100000\nTraining Epoch: 12 [49920/50000]\tLoss: 3.4564\tLR: 0.100000\nTraining Epoch: 12 [50000/50000]\tLoss: 3.3559\tLR: 0.100000\n=========================================================================================\nTraining Epoch: 13 [128/50000]\tLoss: 3.6393\tLR: 0.100000\nTraining Epoch: 13 [256/50000]\tLoss: 3.7605\tLR: 0.100000\nTraining Epoch: 13 [384/50000]\tLoss: 3.5306\tLR: 0.100000\nTraining Epoch: 13 [512/50000]\tLoss: 3.5347\tLR: 0.100000\nTraining Epoch: 13 [640/50000]\tLoss: 3.5403\tLR: 0.100000\nTraining Epoch: 13 [768/50000]\tLoss: 3.5059\tLR: 0.100000\nTraining Epoch: 13 [896/50000]\tLoss: 3.4644\tLR: 0.100000\nTraining Epoch: 13 [1024/50000]\tLoss: 3.6013\tLR: 0.100000\nTraining Epoch: 13 [1152/50000]\tLoss: 3.6425\tLR: 0.100000\nTraining Epoch: 13 [1280/50000]\tLoss: 3.3036\tLR: 0.100000\nTraining Epoch: 13 [1408/50000]\tLoss: 3.6812\tLR: 0.100000\nTraining Epoch: 13 [1536/50000]\tLoss: 3.5960\tLR: 0.100000\nTraining Epoch: 13 [1664/50000]\tLoss: 3.5025\tLR: 0.100000\nTraining Epoch: 13 [1792/50000]\tLoss: 3.6138\tLR: 0.100000\nTraining Epoch: 13 [1920/50000]\tLoss: 3.5460\tLR: 0.100000\nTraining Epoch: 13 [2048/50000]\tLoss: 3.4627\tLR: 0.100000\nTraining Epoch: 13 [2176/50000]\tLoss: 3.5275\tLR: 0.100000\nTraining Epoch: 13 [2304/50000]\tLoss: 3.5286\tLR: 0.100000\nTraining Epoch: 13 [2432/50000]\tLoss: 3.4532\tLR: 0.100000\nTraining Epoch: 13 [2560/50000]\tLoss: 3.7697\tLR: 0.100000\nTraining Epoch: 13 [2688/50000]\tLoss: 3.4656\tLR: 0.100000\nTraining Epoch: 13 [2816/50000]\tLoss: 3.3717\tLR: 0.100000\nTraining Epoch: 13 [2944/50000]\tLoss: 3.4482\tLR: 0.100000\nTraining Epoch: 13 [3072/50000]\tLoss: 3.3906\tLR: 0.100000\nTraining Epoch: 13 [3200/50000]\tLoss: 3.5052\tLR: 0.100000\nTraining Epoch: 13 [3328/50000]\tLoss: 3.5185\tLR: 0.100000\nTraining Epoch: 13 [3456/50000]\tLoss: 3.5309\tLR: 0.100000\nTraining Epoch: 13 [3584/50000]\tLoss: 3.3377\tLR: 0.100000\nTraining Epoch: 13 [3712/50000]\tLoss: 3.4632\tLR: 0.100000\nTraining Epoch: 13 [3840/50000]\tLoss: 3.6647\tLR: 0.100000\nTraining Epoch: 13 [3968/50000]\tLoss: 3.5428\tLR: 0.100000\nTraining Epoch: 13 [4096/50000]\tLoss: 3.6919\tLR: 0.100000\nTraining Epoch: 13 [4224/50000]\tLoss: 3.5303\tLR: 0.100000\nTraining Epoch: 13 [4352/50000]\tLoss: 3.4403\tLR: 0.100000\nTraining Epoch: 13 [4480/50000]\tLoss: 3.2993\tLR: 0.100000\nTraining Epoch: 13 [4608/50000]\tLoss: 3.3729\tLR: 0.100000\nTraining Epoch: 13 [4736/50000]\tLoss: 3.4853\tLR: 0.100000\nTraining Epoch: 13 [4864/50000]\tLoss: 3.5715\tLR: 0.100000\nTraining Epoch: 13 [4992/50000]\tLoss: 3.5681\tLR: 0.100000\nTraining Epoch: 13 [5120/50000]\tLoss: 3.3949\tLR: 0.100000\nTraining Epoch: 13 [5248/50000]\tLoss: 3.5082\tLR: 0.100000\nTraining Epoch: 13 [5376/50000]\tLoss: 3.5636\tLR: 0.100000\nTraining Epoch: 13 [5504/50000]\tLoss: 3.4694\tLR: 0.100000\nTraining Epoch: 13 [5632/50000]\tLoss: 3.3981\tLR: 0.100000\nTraining Epoch: 13 [5760/50000]\tLoss: 3.5436\tLR: 0.100000\nTraining Epoch: 13 [5888/50000]\tLoss: 3.5001\tLR: 0.100000\nTraining Epoch: 13 [6016/50000]\tLoss: 3.4728\tLR: 0.100000\nTraining Epoch: 13 [6144/50000]\tLoss: 3.3836\tLR: 0.100000\nTraining Epoch: 13 [6272/50000]\tLoss: 3.2856\tLR: 0.100000\nTraining Epoch: 13 [6400/50000]\tLoss: 3.6652\tLR: 0.100000\nTraining Epoch: 13 [6528/50000]\tLoss: 3.4720\tLR: 0.100000\nTraining Epoch: 13 [6656/50000]\tLoss: 3.8315\tLR: 0.100000\nTraining Epoch: 13 [6784/50000]\tLoss: 3.7113\tLR: 0.100000\nTraining Epoch: 13 [6912/50000]\tLoss: 3.5634\tLR: 0.100000\nTraining Epoch: 13 [7040/50000]\tLoss: 3.4347\tLR: 0.100000\nTraining Epoch: 13 [7168/50000]\tLoss: 3.5327\tLR: 0.100000\nTraining Epoch: 13 [7296/50000]\tLoss: 3.3983\tLR: 0.100000\nTraining Epoch: 13 [7424/50000]\tLoss: 3.4922\tLR: 0.100000\nTraining Epoch: 13 [7552/50000]\tLoss: 3.4155\tLR: 0.100000\nTraining Epoch: 13 [7680/50000]\tLoss: 3.5052\tLR: 0.100000\nTraining Epoch: 13 [7808/50000]\tLoss: 3.7479\tLR: 0.100000\nTraining Epoch: 13 [7936/50000]\tLoss: 3.6337\tLR: 0.100000\nTraining Epoch: 13 [8064/50000]\tLoss: 3.6350\tLR: 0.100000\nTraining Epoch: 13 [8192/50000]\tLoss: 3.4927\tLR: 0.100000\nTraining Epoch: 13 [8320/50000]\tLoss: 3.3999\tLR: 0.100000\nTraining Epoch: 13 [8448/50000]\tLoss: 3.6119\tLR: 0.100000\nTraining Epoch: 13 [8576/50000]\tLoss: 3.3980\tLR: 0.100000\nTraining Epoch: 13 [8704/50000]\tLoss: 3.5411\tLR: 0.100000\nTraining Epoch: 13 [8832/50000]\tLoss: 3.1615\tLR: 0.100000\nTraining Epoch: 13 [8960/50000]\tLoss: 3.6289\tLR: 0.100000\nTraining Epoch: 13 [9088/50000]\tLoss: 3.7564\tLR: 0.100000\nTraining Epoch: 13 [9216/50000]\tLoss: 3.4887\tLR: 0.100000\nTraining Epoch: 13 [9344/50000]\tLoss: 3.5394\tLR: 0.100000\nTraining Epoch: 13 [9472/50000]\tLoss: 3.8313\tLR: 0.100000\nTraining Epoch: 13 [9600/50000]\tLoss: 3.5505\tLR: 0.100000\nTraining Epoch: 13 [9728/50000]\tLoss: 3.7100\tLR: 0.100000\nTraining Epoch: 13 [9856/50000]\tLoss: 3.5981\tLR: 0.100000\nTraining Epoch: 13 [9984/50000]\tLoss: 3.6316\tLR: 0.100000\nTraining Epoch: 13 [10112/50000]\tLoss: 3.5685\tLR: 0.100000\nTraining Epoch: 13 [10240/50000]\tLoss: 3.8192\tLR: 0.100000\nTraining Epoch: 13 [10368/50000]\tLoss: 3.4218\tLR: 0.100000\nTraining Epoch: 13 [10496/50000]\tLoss: 3.4700\tLR: 0.100000\nTraining Epoch: 13 [10624/50000]\tLoss: 3.4785\tLR: 0.100000\nTraining Epoch: 13 [10752/50000]\tLoss: 3.4736\tLR: 0.100000\nTraining Epoch: 13 [10880/50000]\tLoss: 3.6527\tLR: 0.100000\nTraining Epoch: 13 [11008/50000]\tLoss: 3.3725\tLR: 0.100000\nTraining Epoch: 13 [11136/50000]\tLoss: 3.5736\tLR: 0.100000\nTraining Epoch: 13 [11264/50000]\tLoss: 3.6282\tLR: 0.100000\nTraining Epoch: 13 [11392/50000]\tLoss: 3.3433\tLR: 0.100000\nTraining Epoch: 13 [11520/50000]\tLoss: 3.5743\tLR: 0.100000\nTraining Epoch: 13 [11648/50000]\tLoss: 3.6479\tLR: 0.100000\n","name":"stdout"},{"output_type":"stream","text":"Training Epoch: 13 [11776/50000]\tLoss: 3.3824\tLR: 0.100000\nTraining Epoch: 13 [11904/50000]\tLoss: 3.1884\tLR: 0.100000\nTraining Epoch: 13 [12032/50000]\tLoss: 3.5400\tLR: 0.100000\nTraining Epoch: 13 [12160/50000]\tLoss: 3.7381\tLR: 0.100000\nTraining Epoch: 13 [12288/50000]\tLoss: 3.4872\tLR: 0.100000\nTraining Epoch: 13 [12416/50000]\tLoss: 3.7589\tLR: 0.100000\nTraining Epoch: 13 [12544/50000]\tLoss: 3.4942\tLR: 0.100000\nTraining Epoch: 13 [12672/50000]\tLoss: 3.4954\tLR: 0.100000\nTraining Epoch: 13 [12800/50000]\tLoss: 3.6859\tLR: 0.100000\nTraining Epoch: 13 [12928/50000]\tLoss: 3.5415\tLR: 0.100000\nTraining Epoch: 13 [13056/50000]\tLoss: 3.4772\tLR: 0.100000\nTraining Epoch: 13 [13184/50000]\tLoss: 3.3909\tLR: 0.100000\nTraining Epoch: 13 [13312/50000]\tLoss: 3.5324\tLR: 0.100000\nTraining Epoch: 13 [13440/50000]\tLoss: 3.6173\tLR: 0.100000\nTraining Epoch: 13 [13568/50000]\tLoss: 3.7959\tLR: 0.100000\nTraining Epoch: 13 [13696/50000]\tLoss: 3.4388\tLR: 0.100000\nTraining Epoch: 13 [13824/50000]\tLoss: 3.5382\tLR: 0.100000\nTraining Epoch: 13 [13952/50000]\tLoss: 3.2775\tLR: 0.100000\nTraining Epoch: 13 [14080/50000]\tLoss: 3.5608\tLR: 0.100000\nTraining Epoch: 13 [14208/50000]\tLoss: 3.4646\tLR: 0.100000\nTraining Epoch: 13 [14336/50000]\tLoss: 3.5092\tLR: 0.100000\nTraining Epoch: 13 [14464/50000]\tLoss: 3.6048\tLR: 0.100000\nTraining Epoch: 13 [14592/50000]\tLoss: 3.4224\tLR: 0.100000\nTraining Epoch: 13 [14720/50000]\tLoss: 3.6834\tLR: 0.100000\nTraining Epoch: 13 [14848/50000]\tLoss: 3.6245\tLR: 0.100000\nTraining Epoch: 13 [14976/50000]\tLoss: 3.4642\tLR: 0.100000\nTraining Epoch: 13 [15104/50000]\tLoss: 3.2210\tLR: 0.100000\nTraining Epoch: 13 [15232/50000]\tLoss: 3.2885\tLR: 0.100000\nTraining Epoch: 13 [15360/50000]\tLoss: 3.5242\tLR: 0.100000\nTraining Epoch: 13 [15488/50000]\tLoss: 3.4311\tLR: 0.100000\nTraining Epoch: 13 [15616/50000]\tLoss: 3.7807\tLR: 0.100000\nTraining Epoch: 13 [15744/50000]\tLoss: 3.4801\tLR: 0.100000\nTraining Epoch: 13 [15872/50000]\tLoss: 3.6237\tLR: 0.100000\nTraining Epoch: 13 [16000/50000]\tLoss: 3.6113\tLR: 0.100000\nTraining Epoch: 13 [16128/50000]\tLoss: 3.6730\tLR: 0.100000\nTraining Epoch: 13 [16256/50000]\tLoss: 3.4308\tLR: 0.100000\nTraining Epoch: 13 [16384/50000]\tLoss: 3.7368\tLR: 0.100000\nTraining Epoch: 13 [16512/50000]\tLoss: 3.4051\tLR: 0.100000\nTraining Epoch: 13 [16640/50000]\tLoss: 3.4271\tLR: 0.100000\nTraining Epoch: 13 [16768/50000]\tLoss: 3.4696\tLR: 0.100000\nTraining Epoch: 13 [16896/50000]\tLoss: 3.3823\tLR: 0.100000\nTraining Epoch: 13 [17024/50000]\tLoss: 3.4917\tLR: 0.100000\nTraining Epoch: 13 [17152/50000]\tLoss: 3.4253\tLR: 0.100000\nTraining Epoch: 13 [17280/50000]\tLoss: 3.4542\tLR: 0.100000\nTraining Epoch: 13 [17408/50000]\tLoss: 3.5301\tLR: 0.100000\nTraining Epoch: 13 [17536/50000]\tLoss: 3.7713\tLR: 0.100000\nTraining Epoch: 13 [17664/50000]\tLoss: 3.5832\tLR: 0.100000\nTraining Epoch: 13 [17792/50000]\tLoss: 3.5250\tLR: 0.100000\nTraining Epoch: 13 [17920/50000]\tLoss: 3.6460\tLR: 0.100000\nTraining Epoch: 13 [18048/50000]\tLoss: 3.3179\tLR: 0.100000\nTraining Epoch: 13 [18176/50000]\tLoss: 3.4556\tLR: 0.100000\nTraining Epoch: 13 [18304/50000]\tLoss: 3.3808\tLR: 0.100000\nTraining Epoch: 13 [18432/50000]\tLoss: 3.6164\tLR: 0.100000\nTraining Epoch: 13 [18560/50000]\tLoss: 3.5200\tLR: 0.100000\nTraining Epoch: 13 [18688/50000]\tLoss: 3.4785\tLR: 0.100000\nTraining Epoch: 13 [18816/50000]\tLoss: 3.5850\tLR: 0.100000\nTraining Epoch: 13 [18944/50000]\tLoss: 3.7484\tLR: 0.100000\nTraining Epoch: 13 [19072/50000]\tLoss: 3.5476\tLR: 0.100000\nTraining Epoch: 13 [19200/50000]\tLoss: 3.6954\tLR: 0.100000\nTraining Epoch: 13 [19328/50000]\tLoss: 3.5775\tLR: 0.100000\nTraining Epoch: 13 [19456/50000]\tLoss: 3.3807\tLR: 0.100000\nTraining Epoch: 13 [19584/50000]\tLoss: 3.4881\tLR: 0.100000\nTraining Epoch: 13 [19712/50000]\tLoss: 3.5990\tLR: 0.100000\nTraining Epoch: 13 [19840/50000]\tLoss: 3.5652\tLR: 0.100000\nTraining Epoch: 13 [19968/50000]\tLoss: 3.4155\tLR: 0.100000\nTraining Epoch: 13 [20096/50000]\tLoss: 3.6141\tLR: 0.100000\nTraining Epoch: 13 [20224/50000]\tLoss: 3.2622\tLR: 0.100000\nTraining Epoch: 13 [20352/50000]\tLoss: 3.4003\tLR: 0.100000\nTraining Epoch: 13 [20480/50000]\tLoss: 3.4383\tLR: 0.100000\nTraining Epoch: 13 [20608/50000]\tLoss: 3.5615\tLR: 0.100000\nTraining Epoch: 13 [20736/50000]\tLoss: 3.3574\tLR: 0.100000\nTraining Epoch: 13 [20864/50000]\tLoss: 3.4732\tLR: 0.100000\nTraining Epoch: 13 [20992/50000]\tLoss: 3.4451\tLR: 0.100000\nTraining Epoch: 13 [21120/50000]\tLoss: 3.5377\tLR: 0.100000\nTraining Epoch: 13 [21248/50000]\tLoss: 3.5309\tLR: 0.100000\nTraining Epoch: 13 [21376/50000]\tLoss: 3.6051\tLR: 0.100000\nTraining Epoch: 13 [21504/50000]\tLoss: 3.5743\tLR: 0.100000\nTraining Epoch: 13 [21632/50000]\tLoss: 3.4904\tLR: 0.100000\nTraining Epoch: 13 [21760/50000]\tLoss: 3.4424\tLR: 0.100000\nTraining Epoch: 13 [21888/50000]\tLoss: 3.6056\tLR: 0.100000\nTraining Epoch: 13 [22016/50000]\tLoss: 3.3987\tLR: 0.100000\nTraining Epoch: 13 [22144/50000]\tLoss: 3.3742\tLR: 0.100000\nTraining Epoch: 13 [22272/50000]\tLoss: 3.6231\tLR: 0.100000\nTraining Epoch: 13 [22400/50000]\tLoss: 3.6547\tLR: 0.100000\nTraining Epoch: 13 [22528/50000]\tLoss: 3.7660\tLR: 0.100000\nTraining Epoch: 13 [22656/50000]\tLoss: 3.5500\tLR: 0.100000\nTraining Epoch: 13 [22784/50000]\tLoss: 3.4261\tLR: 0.100000\nTraining Epoch: 13 [22912/50000]\tLoss: 3.7755\tLR: 0.100000\nTraining Epoch: 13 [23040/50000]\tLoss: 3.3571\tLR: 0.100000\nTraining Epoch: 13 [23168/50000]\tLoss: 3.7394\tLR: 0.100000\nTraining Epoch: 13 [23296/50000]\tLoss: 3.4007\tLR: 0.100000\nTraining Epoch: 13 [23424/50000]\tLoss: 3.4293\tLR: 0.100000\nTraining Epoch: 13 [23552/50000]\tLoss: 3.4690\tLR: 0.100000\nTraining Epoch: 13 [23680/50000]\tLoss: 3.4413\tLR: 0.100000\nTraining Epoch: 13 [23808/50000]\tLoss: 3.6013\tLR: 0.100000\nTraining Epoch: 13 [23936/50000]\tLoss: 3.8317\tLR: 0.100000\nTraining Epoch: 13 [24064/50000]\tLoss: 3.5668\tLR: 0.100000\nTraining Epoch: 13 [24192/50000]\tLoss: 3.5068\tLR: 0.100000\nTraining Epoch: 13 [24320/50000]\tLoss: 3.4908\tLR: 0.100000\nTraining Epoch: 13 [24448/50000]\tLoss: 3.3365\tLR: 0.100000\nTraining Epoch: 13 [24576/50000]\tLoss: 3.7191\tLR: 0.100000\nTraining Epoch: 13 [24704/50000]\tLoss: 3.6776\tLR: 0.100000\nTraining Epoch: 13 [24832/50000]\tLoss: 3.7537\tLR: 0.100000\nTraining Epoch: 13 [24960/50000]\tLoss: 3.5894\tLR: 0.100000\nTraining Epoch: 13 [25088/50000]\tLoss: 3.7329\tLR: 0.100000\nTraining Epoch: 13 [25216/50000]\tLoss: 3.4869\tLR: 0.100000\nTraining Epoch: 13 [25344/50000]\tLoss: 3.4244\tLR: 0.100000\nTraining Epoch: 13 [25472/50000]\tLoss: 3.4061\tLR: 0.100000\nTraining Epoch: 13 [25600/50000]\tLoss: 3.4672\tLR: 0.100000\nTraining Epoch: 13 [25728/50000]\tLoss: 3.3332\tLR: 0.100000\nTraining Epoch: 13 [25856/50000]\tLoss: 3.5878\tLR: 0.100000\nTraining Epoch: 13 [25984/50000]\tLoss: 3.4980\tLR: 0.100000\nTraining Epoch: 13 [26112/50000]\tLoss: 3.3170\tLR: 0.100000\nTraining Epoch: 13 [26240/50000]\tLoss: 3.3568\tLR: 0.100000\nTraining Epoch: 13 [26368/50000]\tLoss: 3.5270\tLR: 0.100000\nTraining Epoch: 13 [26496/50000]\tLoss: 3.4136\tLR: 0.100000\nTraining Epoch: 13 [26624/50000]\tLoss: 3.5519\tLR: 0.100000\nTraining Epoch: 13 [26752/50000]\tLoss: 3.5073\tLR: 0.100000\nTraining Epoch: 13 [26880/50000]\tLoss: 3.4572\tLR: 0.100000\nTraining Epoch: 13 [27008/50000]\tLoss: 3.3797\tLR: 0.100000\nTraining Epoch: 13 [27136/50000]\tLoss: 3.3778\tLR: 0.100000\nTraining Epoch: 13 [27264/50000]\tLoss: 3.2941\tLR: 0.100000\nTraining Epoch: 13 [27392/50000]\tLoss: 3.4832\tLR: 0.100000\nTraining Epoch: 13 [27520/50000]\tLoss: 3.6308\tLR: 0.100000\nTraining Epoch: 13 [27648/50000]\tLoss: 3.5623\tLR: 0.100000\nTraining Epoch: 13 [27776/50000]\tLoss: 3.8044\tLR: 0.100000\nTraining Epoch: 13 [27904/50000]\tLoss: 3.5532\tLR: 0.100000\nTraining Epoch: 13 [28032/50000]\tLoss: 3.3718\tLR: 0.100000\nTraining Epoch: 13 [28160/50000]\tLoss: 3.4793\tLR: 0.100000\nTraining Epoch: 13 [28288/50000]\tLoss: 3.4366\tLR: 0.100000\nTraining Epoch: 13 [28416/50000]\tLoss: 3.4040\tLR: 0.100000\nTraining Epoch: 13 [28544/50000]\tLoss: 3.6752\tLR: 0.100000\nTraining Epoch: 13 [28672/50000]\tLoss: 3.3790\tLR: 0.100000\nTraining Epoch: 13 [28800/50000]\tLoss: 3.4366\tLR: 0.100000\nTraining Epoch: 13 [28928/50000]\tLoss: 3.6637\tLR: 0.100000\nTraining Epoch: 13 [29056/50000]\tLoss: 3.6094\tLR: 0.100000\nTraining Epoch: 13 [29184/50000]\tLoss: 3.4273\tLR: 0.100000\nTraining Epoch: 13 [29312/50000]\tLoss: 3.1864\tLR: 0.100000\nTraining Epoch: 13 [29440/50000]\tLoss: 3.3249\tLR: 0.100000\nTraining Epoch: 13 [29568/50000]\tLoss: 3.8763\tLR: 0.100000\nTraining Epoch: 13 [29696/50000]\tLoss: 3.5853\tLR: 0.100000\n","name":"stdout"},{"output_type":"stream","text":"Training Epoch: 13 [29824/50000]\tLoss: 3.5845\tLR: 0.100000\nTraining Epoch: 13 [29952/50000]\tLoss: 3.5926\tLR: 0.100000\nTraining Epoch: 13 [30080/50000]\tLoss: 3.3141\tLR: 0.100000\nTraining Epoch: 13 [30208/50000]\tLoss: 3.4355\tLR: 0.100000\nTraining Epoch: 13 [30336/50000]\tLoss: 3.2314\tLR: 0.100000\nTraining Epoch: 13 [30464/50000]\tLoss: 3.2084\tLR: 0.100000\nTraining Epoch: 13 [30592/50000]\tLoss: 3.6026\tLR: 0.100000\nTraining Epoch: 13 [30720/50000]\tLoss: 3.4057\tLR: 0.100000\nTraining Epoch: 13 [30848/50000]\tLoss: 3.2256\tLR: 0.100000\nTraining Epoch: 13 [30976/50000]\tLoss: 3.6487\tLR: 0.100000\nTraining Epoch: 13 [31104/50000]\tLoss: 3.5003\tLR: 0.100000\nTraining Epoch: 13 [31232/50000]\tLoss: 3.2426\tLR: 0.100000\nTraining Epoch: 13 [31360/50000]\tLoss: 3.5086\tLR: 0.100000\nTraining Epoch: 13 [31488/50000]\tLoss: 3.4410\tLR: 0.100000\nTraining Epoch: 13 [31616/50000]\tLoss: 3.4050\tLR: 0.100000\nTraining Epoch: 13 [31744/50000]\tLoss: 3.5056\tLR: 0.100000\nTraining Epoch: 13 [31872/50000]\tLoss: 3.5155\tLR: 0.100000\nTraining Epoch: 13 [32000/50000]\tLoss: 3.7153\tLR: 0.100000\nTraining Epoch: 13 [32128/50000]\tLoss: 3.4400\tLR: 0.100000\nTraining Epoch: 13 [32256/50000]\tLoss: 3.4302\tLR: 0.100000\nTraining Epoch: 13 [32384/50000]\tLoss: 3.6325\tLR: 0.100000\nTraining Epoch: 13 [32512/50000]\tLoss: 3.2418\tLR: 0.100000\nTraining Epoch: 13 [32640/50000]\tLoss: 3.6615\tLR: 0.100000\nTraining Epoch: 13 [32768/50000]\tLoss: 3.5529\tLR: 0.100000\nTraining Epoch: 13 [32896/50000]\tLoss: 3.5208\tLR: 0.100000\nTraining Epoch: 13 [33024/50000]\tLoss: 3.3522\tLR: 0.100000\nTraining Epoch: 13 [33152/50000]\tLoss: 3.5244\tLR: 0.100000\nTraining Epoch: 13 [33280/50000]\tLoss: 3.4636\tLR: 0.100000\nTraining Epoch: 13 [33408/50000]\tLoss: 3.3926\tLR: 0.100000\nTraining Epoch: 13 [33536/50000]\tLoss: 3.7502\tLR: 0.100000\nTraining Epoch: 13 [33664/50000]\tLoss: 3.5967\tLR: 0.100000\nTraining Epoch: 13 [33792/50000]\tLoss: 3.6376\tLR: 0.100000\nTraining Epoch: 13 [33920/50000]\tLoss: 3.5790\tLR: 0.100000\nTraining Epoch: 13 [34048/50000]\tLoss: 3.3946\tLR: 0.100000\nTraining Epoch: 13 [34176/50000]\tLoss: 3.4773\tLR: 0.100000\nTraining Epoch: 13 [34304/50000]\tLoss: 3.6251\tLR: 0.100000\nTraining Epoch: 13 [34432/50000]\tLoss: 3.7935\tLR: 0.100000\nTraining Epoch: 13 [34560/50000]\tLoss: 3.3113\tLR: 0.100000\nTraining Epoch: 13 [34688/50000]\tLoss: 3.6545\tLR: 0.100000\nTraining Epoch: 13 [34816/50000]\tLoss: 3.7205\tLR: 0.100000\nTraining Epoch: 13 [34944/50000]\tLoss: 3.3330\tLR: 0.100000\nTraining Epoch: 13 [35072/50000]\tLoss: 3.7549\tLR: 0.100000\nTraining Epoch: 13 [35200/50000]\tLoss: 3.3953\tLR: 0.100000\nTraining Epoch: 13 [35328/50000]\tLoss: 3.5272\tLR: 0.100000\nTraining Epoch: 13 [35456/50000]\tLoss: 3.8204\tLR: 0.100000\nTraining Epoch: 13 [35584/50000]\tLoss: 3.5648\tLR: 0.100000\nTraining Epoch: 13 [35712/50000]\tLoss: 3.3568\tLR: 0.100000\nTraining Epoch: 13 [35840/50000]\tLoss: 3.5549\tLR: 0.100000\nTraining Epoch: 13 [35968/50000]\tLoss: 3.4240\tLR: 0.100000\nTraining Epoch: 13 [36096/50000]\tLoss: 3.9153\tLR: 0.100000\nTraining Epoch: 13 [36224/50000]\tLoss: 3.5815\tLR: 0.100000\nTraining Epoch: 13 [36352/50000]\tLoss: 3.7342\tLR: 0.100000\nTraining Epoch: 13 [36480/50000]\tLoss: 3.4272\tLR: 0.100000\nTraining Epoch: 13 [36608/50000]\tLoss: 3.5525\tLR: 0.100000\nTraining Epoch: 13 [36736/50000]\tLoss: 3.6051\tLR: 0.100000\nTraining Epoch: 13 [36864/50000]\tLoss: 3.4530\tLR: 0.100000\nTraining Epoch: 13 [36992/50000]\tLoss: 3.5207\tLR: 0.100000\nTraining Epoch: 13 [37120/50000]\tLoss: 3.5904\tLR: 0.100000\nTraining Epoch: 13 [37248/50000]\tLoss: 3.4981\tLR: 0.100000\nTraining Epoch: 13 [37376/50000]\tLoss: 3.5086\tLR: 0.100000\nTraining Epoch: 13 [37504/50000]\tLoss: 3.5543\tLR: 0.100000\nTraining Epoch: 13 [37632/50000]\tLoss: 3.6840\tLR: 0.100000\nTraining Epoch: 13 [37760/50000]\tLoss: 3.5147\tLR: 0.100000\nTraining Epoch: 13 [37888/50000]\tLoss: 3.7106\tLR: 0.100000\nTraining Epoch: 13 [38016/50000]\tLoss: 3.3710\tLR: 0.100000\nTraining Epoch: 13 [38144/50000]\tLoss: 3.6325\tLR: 0.100000\nTraining Epoch: 13 [38272/50000]\tLoss: 3.4116\tLR: 0.100000\nTraining Epoch: 13 [38400/50000]\tLoss: 3.4881\tLR: 0.100000\nTraining Epoch: 13 [38528/50000]\tLoss: 3.4757\tLR: 0.100000\nTraining Epoch: 13 [38656/50000]\tLoss: 3.3637\tLR: 0.100000\nTraining Epoch: 13 [38784/50000]\tLoss: 3.3875\tLR: 0.100000\nTraining Epoch: 13 [38912/50000]\tLoss: 3.4756\tLR: 0.100000\nTraining Epoch: 13 [39040/50000]\tLoss: 3.4650\tLR: 0.100000\nTraining Epoch: 13 [39168/50000]\tLoss: 3.4737\tLR: 0.100000\nTraining Epoch: 13 [39296/50000]\tLoss: 3.4903\tLR: 0.100000\nTraining Epoch: 13 [39424/50000]\tLoss: 3.5844\tLR: 0.100000\nTraining Epoch: 13 [39552/50000]\tLoss: 3.5735\tLR: 0.100000\nTraining Epoch: 13 [39680/50000]\tLoss: 3.5654\tLR: 0.100000\nTraining Epoch: 13 [39808/50000]\tLoss: 3.3706\tLR: 0.100000\nTraining Epoch: 13 [39936/50000]\tLoss: 3.4999\tLR: 0.100000\nTraining Epoch: 13 [40064/50000]\tLoss: 3.3623\tLR: 0.100000\nTraining Epoch: 13 [40192/50000]\tLoss: 3.6653\tLR: 0.100000\nTraining Epoch: 13 [40320/50000]\tLoss: 3.4693\tLR: 0.100000\nTraining Epoch: 13 [40448/50000]\tLoss: 3.6284\tLR: 0.100000\nTraining Epoch: 13 [40576/50000]\tLoss: 3.4256\tLR: 0.100000\nTraining Epoch: 13 [40704/50000]\tLoss: 3.6763\tLR: 0.100000\nTraining Epoch: 13 [40832/50000]\tLoss: 3.3071\tLR: 0.100000\nTraining Epoch: 13 [40960/50000]\tLoss: 3.4521\tLR: 0.100000\nTraining Epoch: 13 [41088/50000]\tLoss: 3.4438\tLR: 0.100000\nTraining Epoch: 13 [41216/50000]\tLoss: 3.5042\tLR: 0.100000\nTraining Epoch: 13 [41344/50000]\tLoss: 3.4182\tLR: 0.100000\nTraining Epoch: 13 [41472/50000]\tLoss: 3.7204\tLR: 0.100000\nTraining Epoch: 13 [41600/50000]\tLoss: 3.3955\tLR: 0.100000\nTraining Epoch: 13 [41728/50000]\tLoss: 3.3955\tLR: 0.100000\nTraining Epoch: 13 [41856/50000]\tLoss: 3.4495\tLR: 0.100000\nTraining Epoch: 13 [41984/50000]\tLoss: 3.5755\tLR: 0.100000\nTraining Epoch: 13 [42112/50000]\tLoss: 3.3425\tLR: 0.100000\nTraining Epoch: 13 [42240/50000]\tLoss: 3.3709\tLR: 0.100000\nTraining Epoch: 13 [42368/50000]\tLoss: 3.6420\tLR: 0.100000\nTraining Epoch: 13 [42496/50000]\tLoss: 3.4787\tLR: 0.100000\nTraining Epoch: 13 [42624/50000]\tLoss: 3.5693\tLR: 0.100000\nTraining Epoch: 13 [42752/50000]\tLoss: 3.5478\tLR: 0.100000\nTraining Epoch: 13 [42880/50000]\tLoss: 3.6569\tLR: 0.100000\nTraining Epoch: 13 [43008/50000]\tLoss: 3.5885\tLR: 0.100000\nTraining Epoch: 13 [43136/50000]\tLoss: 3.4897\tLR: 0.100000\nTraining Epoch: 13 [43264/50000]\tLoss: 3.2957\tLR: 0.100000\nTraining Epoch: 13 [43392/50000]\tLoss: 3.3456\tLR: 0.100000\nTraining Epoch: 13 [43520/50000]\tLoss: 3.1775\tLR: 0.100000\nTraining Epoch: 13 [43648/50000]\tLoss: 3.4249\tLR: 0.100000\nTraining Epoch: 13 [43776/50000]\tLoss: 3.4847\tLR: 0.100000\nTraining Epoch: 13 [43904/50000]\tLoss: 3.5130\tLR: 0.100000\nTraining Epoch: 13 [44032/50000]\tLoss: 3.5004\tLR: 0.100000\nTraining Epoch: 13 [44160/50000]\tLoss: 3.5373\tLR: 0.100000\nTraining Epoch: 13 [44288/50000]\tLoss: 3.4369\tLR: 0.100000\nTraining Epoch: 13 [44416/50000]\tLoss: 3.6298\tLR: 0.100000\nTraining Epoch: 13 [44544/50000]\tLoss: 3.6474\tLR: 0.100000\nTraining Epoch: 13 [44672/50000]\tLoss: 3.6521\tLR: 0.100000\nTraining Epoch: 13 [44800/50000]\tLoss: 3.3341\tLR: 0.100000\nTraining Epoch: 13 [44928/50000]\tLoss: 3.5885\tLR: 0.100000\nTraining Epoch: 13 [45056/50000]\tLoss: 3.5493\tLR: 0.100000\nTraining Epoch: 13 [45184/50000]\tLoss: 3.4845\tLR: 0.100000\nTraining Epoch: 13 [45312/50000]\tLoss: 3.2664\tLR: 0.100000\nTraining Epoch: 13 [45440/50000]\tLoss: 3.3871\tLR: 0.100000\nTraining Epoch: 13 [45568/50000]\tLoss: 3.3852\tLR: 0.100000\nTraining Epoch: 13 [45696/50000]\tLoss: 3.2556\tLR: 0.100000\nTraining Epoch: 13 [45824/50000]\tLoss: 3.3821\tLR: 0.100000\nTraining Epoch: 13 [45952/50000]\tLoss: 3.4355\tLR: 0.100000\nTraining Epoch: 13 [46080/50000]\tLoss: 3.5203\tLR: 0.100000\nTraining Epoch: 13 [46208/50000]\tLoss: 3.3697\tLR: 0.100000\nTraining Epoch: 13 [46336/50000]\tLoss: 3.2219\tLR: 0.100000\nTraining Epoch: 13 [46464/50000]\tLoss: 3.4316\tLR: 0.100000\nTraining Epoch: 13 [46592/50000]\tLoss: 3.6038\tLR: 0.100000\nTraining Epoch: 13 [46720/50000]\tLoss: 3.3614\tLR: 0.100000\nTraining Epoch: 13 [46848/50000]\tLoss: 3.4008\tLR: 0.100000\nTraining Epoch: 13 [46976/50000]\tLoss: 3.2660\tLR: 0.100000\nTraining Epoch: 13 [47104/50000]\tLoss: 3.5957\tLR: 0.100000\nTraining Epoch: 13 [47232/50000]\tLoss: 3.4380\tLR: 0.100000\nTraining Epoch: 13 [47360/50000]\tLoss: 3.3938\tLR: 0.100000\nTraining Epoch: 13 [47488/50000]\tLoss: 3.4533\tLR: 0.100000\nTraining Epoch: 13 [47616/50000]\tLoss: 3.5828\tLR: 0.100000\nTraining Epoch: 13 [47744/50000]\tLoss: 3.4846\tLR: 0.100000\n","name":"stdout"},{"output_type":"stream","text":"Training Epoch: 13 [47872/50000]\tLoss: 3.5209\tLR: 0.100000\nTraining Epoch: 13 [48000/50000]\tLoss: 3.6114\tLR: 0.100000\nTraining Epoch: 13 [48128/50000]\tLoss: 3.3307\tLR: 0.100000\nTraining Epoch: 13 [48256/50000]\tLoss: 3.3782\tLR: 0.100000\nTraining Epoch: 13 [48384/50000]\tLoss: 3.4647\tLR: 0.100000\nTraining Epoch: 13 [48512/50000]\tLoss: 3.4387\tLR: 0.100000\nTraining Epoch: 13 [48640/50000]\tLoss: 3.4961\tLR: 0.100000\nTraining Epoch: 13 [48768/50000]\tLoss: 3.4569\tLR: 0.100000\nTraining Epoch: 13 [48896/50000]\tLoss: 3.4215\tLR: 0.100000\nTraining Epoch: 13 [49024/50000]\tLoss: 3.6030\tLR: 0.100000\nTraining Epoch: 13 [49152/50000]\tLoss: 3.2908\tLR: 0.100000\nTraining Epoch: 13 [49280/50000]\tLoss: 3.3504\tLR: 0.100000\nTraining Epoch: 13 [49408/50000]\tLoss: 3.6025\tLR: 0.100000\nTraining Epoch: 13 [49536/50000]\tLoss: 3.6114\tLR: 0.100000\nTraining Epoch: 13 [49664/50000]\tLoss: 3.6573\tLR: 0.100000\nTraining Epoch: 13 [49792/50000]\tLoss: 3.6284\tLR: 0.100000\nTraining Epoch: 13 [49920/50000]\tLoss: 3.6072\tLR: 0.100000\nTraining Epoch: 13 [50000/50000]\tLoss: 3.3911\tLR: 0.100000\n=========================================================================================\nTraining Epoch: 14 [128/50000]\tLoss: 3.4970\tLR: 0.100000\nTraining Epoch: 14 [256/50000]\tLoss: 3.4715\tLR: 0.100000\nTraining Epoch: 14 [384/50000]\tLoss: 3.5269\tLR: 0.100000\nTraining Epoch: 14 [512/50000]\tLoss: 3.7286\tLR: 0.100000\nTraining Epoch: 14 [640/50000]\tLoss: 3.4933\tLR: 0.100000\nTraining Epoch: 14 [768/50000]\tLoss: 3.5106\tLR: 0.100000\nTraining Epoch: 14 [896/50000]\tLoss: 3.5786\tLR: 0.100000\nTraining Epoch: 14 [1024/50000]\tLoss: 3.7145\tLR: 0.100000\nTraining Epoch: 14 [1152/50000]\tLoss: 3.4283\tLR: 0.100000\nTraining Epoch: 14 [1280/50000]\tLoss: 3.5158\tLR: 0.100000\nTraining Epoch: 14 [1408/50000]\tLoss: 3.3754\tLR: 0.100000\nTraining Epoch: 14 [1536/50000]\tLoss: 3.2944\tLR: 0.100000\nTraining Epoch: 14 [1664/50000]\tLoss: 3.5800\tLR: 0.100000\nTraining Epoch: 14 [1792/50000]\tLoss: 3.4159\tLR: 0.100000\nTraining Epoch: 14 [1920/50000]\tLoss: 3.4467\tLR: 0.100000\nTraining Epoch: 14 [2048/50000]\tLoss: 3.4210\tLR: 0.100000\nTraining Epoch: 14 [2176/50000]\tLoss: 3.4132\tLR: 0.100000\nTraining Epoch: 14 [2304/50000]\tLoss: 3.4819\tLR: 0.100000\nTraining Epoch: 14 [2432/50000]\tLoss: 3.4642\tLR: 0.100000\nTraining Epoch: 14 [2560/50000]\tLoss: 3.5750\tLR: 0.100000\nTraining Epoch: 14 [2688/50000]\tLoss: 3.3289\tLR: 0.100000\nTraining Epoch: 14 [2816/50000]\tLoss: 3.6589\tLR: 0.100000\nTraining Epoch: 14 [2944/50000]\tLoss: 3.1961\tLR: 0.100000\nTraining Epoch: 14 [3072/50000]\tLoss: 3.6942\tLR: 0.100000\nTraining Epoch: 14 [3200/50000]\tLoss: 3.4172\tLR: 0.100000\nTraining Epoch: 14 [3328/50000]\tLoss: 3.2494\tLR: 0.100000\nTraining Epoch: 14 [3456/50000]\tLoss: 3.5897\tLR: 0.100000\nTraining Epoch: 14 [3584/50000]\tLoss: 3.5747\tLR: 0.100000\nTraining Epoch: 14 [3712/50000]\tLoss: 3.3898\tLR: 0.100000\nTraining Epoch: 14 [3840/50000]\tLoss: 3.4305\tLR: 0.100000\nTraining Epoch: 14 [3968/50000]\tLoss: 3.5056\tLR: 0.100000\nTraining Epoch: 14 [4096/50000]\tLoss: 3.4013\tLR: 0.100000\nTraining Epoch: 14 [4224/50000]\tLoss: 3.5315\tLR: 0.100000\nTraining Epoch: 14 [4352/50000]\tLoss: 3.3685\tLR: 0.100000\nTraining Epoch: 14 [4480/50000]\tLoss: 3.3735\tLR: 0.100000\nTraining Epoch: 14 [4608/50000]\tLoss: 3.4929\tLR: 0.100000\nTraining Epoch: 14 [4736/50000]\tLoss: 3.2387\tLR: 0.100000\nTraining Epoch: 14 [4864/50000]\tLoss: 3.4394\tLR: 0.100000\nTraining Epoch: 14 [4992/50000]\tLoss: 3.2789\tLR: 0.100000\nTraining Epoch: 14 [5120/50000]\tLoss: 3.3277\tLR: 0.100000\nTraining Epoch: 14 [5248/50000]\tLoss: 3.4935\tLR: 0.100000\nTraining Epoch: 14 [5376/50000]\tLoss: 3.4300\tLR: 0.100000\nTraining Epoch: 14 [5504/50000]\tLoss: 3.5733\tLR: 0.100000\nTraining Epoch: 14 [5632/50000]\tLoss: 3.3242\tLR: 0.100000\nTraining Epoch: 14 [5760/50000]\tLoss: 3.4824\tLR: 0.100000\nTraining Epoch: 14 [5888/50000]\tLoss: 3.3725\tLR: 0.100000\nTraining Epoch: 14 [6016/50000]\tLoss: 3.5670\tLR: 0.100000\nTraining Epoch: 14 [6144/50000]\tLoss: 3.4826\tLR: 0.100000\nTraining Epoch: 14 [6272/50000]\tLoss: 3.5777\tLR: 0.100000\nTraining Epoch: 14 [6400/50000]\tLoss: 3.6523\tLR: 0.100000\nTraining Epoch: 14 [6528/50000]\tLoss: 3.5517\tLR: 0.100000\nTraining Epoch: 14 [6656/50000]\tLoss: 3.3922\tLR: 0.100000\nTraining Epoch: 14 [6784/50000]\tLoss: 3.4894\tLR: 0.100000\nTraining Epoch: 14 [6912/50000]\tLoss: 3.3495\tLR: 0.100000\nTraining Epoch: 14 [7040/50000]\tLoss: 3.2808\tLR: 0.100000\nTraining Epoch: 14 [7168/50000]\tLoss: 3.4796\tLR: 0.100000\nTraining Epoch: 14 [7296/50000]\tLoss: 3.7185\tLR: 0.100000\nTraining Epoch: 14 [7424/50000]\tLoss: 3.5474\tLR: 0.100000\nTraining Epoch: 14 [7552/50000]\tLoss: 3.5402\tLR: 0.100000\nTraining Epoch: 14 [7680/50000]\tLoss: 3.6379\tLR: 0.100000\nTraining Epoch: 14 [7808/50000]\tLoss: 3.4008\tLR: 0.100000\nTraining Epoch: 14 [7936/50000]\tLoss: 3.4494\tLR: 0.100000\nTraining Epoch: 14 [8064/50000]\tLoss: 3.4745\tLR: 0.100000\nTraining Epoch: 14 [8192/50000]\tLoss: 3.4119\tLR: 0.100000\nTraining Epoch: 14 [8320/50000]\tLoss: 3.6092\tLR: 0.100000\nTraining Epoch: 14 [8448/50000]\tLoss: 3.5331\tLR: 0.100000\nTraining Epoch: 14 [8576/50000]\tLoss: 3.4722\tLR: 0.100000\nTraining Epoch: 14 [8704/50000]\tLoss: 3.6321\tLR: 0.100000\nTraining Epoch: 14 [8832/50000]\tLoss: 3.7093\tLR: 0.100000\nTraining Epoch: 14 [8960/50000]\tLoss: 3.1060\tLR: 0.100000\nTraining Epoch: 14 [9088/50000]\tLoss: 3.4401\tLR: 0.100000\nTraining Epoch: 14 [9216/50000]\tLoss: 3.3133\tLR: 0.100000\nTraining Epoch: 14 [9344/50000]\tLoss: 3.4372\tLR: 0.100000\nTraining Epoch: 14 [9472/50000]\tLoss: 3.5356\tLR: 0.100000\nTraining Epoch: 14 [9600/50000]\tLoss: 3.6183\tLR: 0.100000\nTraining Epoch: 14 [9728/50000]\tLoss: 3.4105\tLR: 0.100000\nTraining Epoch: 14 [9856/50000]\tLoss: 3.4679\tLR: 0.100000\nTraining Epoch: 14 [9984/50000]\tLoss: 3.4030\tLR: 0.100000\nTraining Epoch: 14 [10112/50000]\tLoss: 3.4163\tLR: 0.100000\nTraining Epoch: 14 [10240/50000]\tLoss: 3.3446\tLR: 0.100000\nTraining Epoch: 14 [10368/50000]\tLoss: 3.6033\tLR: 0.100000\nTraining Epoch: 14 [10496/50000]\tLoss: 3.6248\tLR: 0.100000\nTraining Epoch: 14 [10624/50000]\tLoss: 3.6514\tLR: 0.100000\nTraining Epoch: 14 [10752/50000]\tLoss: 3.3597\tLR: 0.100000\nTraining Epoch: 14 [10880/50000]\tLoss: 3.7956\tLR: 0.100000\nTraining Epoch: 14 [11008/50000]\tLoss: 3.3512\tLR: 0.100000\nTraining Epoch: 14 [11136/50000]\tLoss: 3.4946\tLR: 0.100000\nTraining Epoch: 14 [11264/50000]\tLoss: 3.2810\tLR: 0.100000\nTraining Epoch: 14 [11392/50000]\tLoss: 3.3825\tLR: 0.100000\nTraining Epoch: 14 [11520/50000]\tLoss: 3.7310\tLR: 0.100000\nTraining Epoch: 14 [11648/50000]\tLoss: 3.4045\tLR: 0.100000\nTraining Epoch: 14 [11776/50000]\tLoss: 3.7093\tLR: 0.100000\nTraining Epoch: 14 [11904/50000]\tLoss: 3.6350\tLR: 0.100000\nTraining Epoch: 14 [12032/50000]\tLoss: 3.5596\tLR: 0.100000\nTraining Epoch: 14 [12160/50000]\tLoss: 3.4582\tLR: 0.100000\nTraining Epoch: 14 [12288/50000]\tLoss: 3.2294\tLR: 0.100000\nTraining Epoch: 14 [12416/50000]\tLoss: 3.7504\tLR: 0.100000\nTraining Epoch: 14 [12544/50000]\tLoss: 3.4610\tLR: 0.100000\nTraining Epoch: 14 [12672/50000]\tLoss: 3.5246\tLR: 0.100000\nTraining Epoch: 14 [12800/50000]\tLoss: 3.6319\tLR: 0.100000\nTraining Epoch: 14 [12928/50000]\tLoss: 3.4147\tLR: 0.100000\nTraining Epoch: 14 [13056/50000]\tLoss: 3.3970\tLR: 0.100000\nTraining Epoch: 14 [13184/50000]\tLoss: 3.4159\tLR: 0.100000\nTraining Epoch: 14 [13312/50000]\tLoss: 3.3467\tLR: 0.100000\nTraining Epoch: 14 [13440/50000]\tLoss: 3.6015\tLR: 0.100000\nTraining Epoch: 14 [13568/50000]\tLoss: 3.4705\tLR: 0.100000\nTraining Epoch: 14 [13696/50000]\tLoss: 3.4673\tLR: 0.100000\nTraining Epoch: 14 [13824/50000]\tLoss: 3.5819\tLR: 0.100000\nTraining Epoch: 14 [13952/50000]\tLoss: 3.2292\tLR: 0.100000\nTraining Epoch: 14 [14080/50000]\tLoss: 3.6977\tLR: 0.100000\nTraining Epoch: 14 [14208/50000]\tLoss: 3.4309\tLR: 0.100000\nTraining Epoch: 14 [14336/50000]\tLoss: 3.4100\tLR: 0.100000\nTraining Epoch: 14 [14464/50000]\tLoss: 3.7263\tLR: 0.100000\nTraining Epoch: 14 [14592/50000]\tLoss: 3.6787\tLR: 0.100000\nTraining Epoch: 14 [14720/50000]\tLoss: 3.5024\tLR: 0.100000\nTraining Epoch: 14 [14848/50000]\tLoss: 3.7072\tLR: 0.100000\nTraining Epoch: 14 [14976/50000]\tLoss: 3.6313\tLR: 0.100000\nTraining Epoch: 14 [15104/50000]\tLoss: 3.4113\tLR: 0.100000\nTraining Epoch: 14 [15232/50000]\tLoss: 3.5925\tLR: 0.100000\nTraining Epoch: 14 [15360/50000]\tLoss: 3.2391\tLR: 0.100000\nTraining Epoch: 14 [15488/50000]\tLoss: 3.3170\tLR: 0.100000\nTraining Epoch: 14 [15616/50000]\tLoss: 3.7722\tLR: 0.100000\nTraining Epoch: 14 [15744/50000]\tLoss: 3.3887\tLR: 0.100000\n","name":"stdout"},{"output_type":"stream","text":"Training Epoch: 14 [15872/50000]\tLoss: 3.5961\tLR: 0.100000\nTraining Epoch: 14 [16000/50000]\tLoss: 3.6425\tLR: 0.100000\nTraining Epoch: 14 [16128/50000]\tLoss: 3.3873\tLR: 0.100000\nTraining Epoch: 14 [16256/50000]\tLoss: 3.4575\tLR: 0.100000\nTraining Epoch: 14 [16384/50000]\tLoss: 3.4420\tLR: 0.100000\nTraining Epoch: 14 [16512/50000]\tLoss: 3.7174\tLR: 0.100000\nTraining Epoch: 14 [16640/50000]\tLoss: 3.6880\tLR: 0.100000\nTraining Epoch: 14 [16768/50000]\tLoss: 3.6021\tLR: 0.100000\nTraining Epoch: 14 [16896/50000]\tLoss: 3.5752\tLR: 0.100000\nTraining Epoch: 14 [17024/50000]\tLoss: 3.4645\tLR: 0.100000\nTraining Epoch: 14 [17152/50000]\tLoss: 3.6236\tLR: 0.100000\nTraining Epoch: 14 [17280/50000]\tLoss: 3.4584\tLR: 0.100000\nTraining Epoch: 14 [17408/50000]\tLoss: 3.4095\tLR: 0.100000\nTraining Epoch: 14 [17536/50000]\tLoss: 3.6177\tLR: 0.100000\nTraining Epoch: 14 [17664/50000]\tLoss: 3.3543\tLR: 0.100000\nTraining Epoch: 14 [17792/50000]\tLoss: 3.4773\tLR: 0.100000\nTraining Epoch: 14 [17920/50000]\tLoss: 3.5113\tLR: 0.100000\nTraining Epoch: 14 [18048/50000]\tLoss: 3.6358\tLR: 0.100000\nTraining Epoch: 14 [18176/50000]\tLoss: 3.3346\tLR: 0.100000\nTraining Epoch: 14 [18304/50000]\tLoss: 3.4366\tLR: 0.100000\nTraining Epoch: 14 [18432/50000]\tLoss: 3.4412\tLR: 0.100000\nTraining Epoch: 14 [18560/50000]\tLoss: 3.3336\tLR: 0.100000\nTraining Epoch: 14 [18688/50000]\tLoss: 3.6194\tLR: 0.100000\nTraining Epoch: 14 [18816/50000]\tLoss: 3.5646\tLR: 0.100000\nTraining Epoch: 14 [18944/50000]\tLoss: 3.7498\tLR: 0.100000\nTraining Epoch: 14 [19072/50000]\tLoss: 3.2813\tLR: 0.100000\nTraining Epoch: 14 [19200/50000]\tLoss: 3.6617\tLR: 0.100000\nTraining Epoch: 14 [19328/50000]\tLoss: 3.5965\tLR: 0.100000\nTraining Epoch: 14 [19456/50000]\tLoss: 3.2723\tLR: 0.100000\nTraining Epoch: 14 [19584/50000]\tLoss: 3.4544\tLR: 0.100000\nTraining Epoch: 14 [19712/50000]\tLoss: 3.4216\tLR: 0.100000\nTraining Epoch: 14 [19840/50000]\tLoss: 3.4336\tLR: 0.100000\nTraining Epoch: 14 [19968/50000]\tLoss: 3.3889\tLR: 0.100000\nTraining Epoch: 14 [20096/50000]\tLoss: 3.5464\tLR: 0.100000\nTraining Epoch: 14 [20224/50000]\tLoss: 3.4784\tLR: 0.100000\nTraining Epoch: 14 [20352/50000]\tLoss: 3.5957\tLR: 0.100000\nTraining Epoch: 14 [20480/50000]\tLoss: 3.6654\tLR: 0.100000\nTraining Epoch: 14 [20608/50000]\tLoss: 3.4908\tLR: 0.100000\nTraining Epoch: 14 [20736/50000]\tLoss: 3.6285\tLR: 0.100000\nTraining Epoch: 14 [20864/50000]\tLoss: 3.3748\tLR: 0.100000\nTraining Epoch: 14 [20992/50000]\tLoss: 3.2260\tLR: 0.100000\nTraining Epoch: 14 [21120/50000]\tLoss: 3.6592\tLR: 0.100000\nTraining Epoch: 14 [21248/50000]\tLoss: 3.6042\tLR: 0.100000\nTraining Epoch: 14 [21376/50000]\tLoss: 3.6124\tLR: 0.100000\nTraining Epoch: 14 [21504/50000]\tLoss: 3.4230\tLR: 0.100000\nTraining Epoch: 14 [21632/50000]\tLoss: 3.5172\tLR: 0.100000\nTraining Epoch: 14 [21760/50000]\tLoss: 3.5508\tLR: 0.100000\nTraining Epoch: 14 [21888/50000]\tLoss: 3.6630\tLR: 0.100000\nTraining Epoch: 14 [22016/50000]\tLoss: 3.6713\tLR: 0.100000\nTraining Epoch: 14 [22144/50000]\tLoss: 3.6261\tLR: 0.100000\nTraining Epoch: 14 [22272/50000]\tLoss: 3.2844\tLR: 0.100000\nTraining Epoch: 14 [22400/50000]\tLoss: 3.5269\tLR: 0.100000\nTraining Epoch: 14 [22528/50000]\tLoss: 3.4874\tLR: 0.100000\nTraining Epoch: 14 [22656/50000]\tLoss: 3.5176\tLR: 0.100000\nTraining Epoch: 14 [22784/50000]\tLoss: 3.3566\tLR: 0.100000\nTraining Epoch: 14 [22912/50000]\tLoss: 3.4198\tLR: 0.100000\nTraining Epoch: 14 [23040/50000]\tLoss: 3.6071\tLR: 0.100000\nTraining Epoch: 14 [23168/50000]\tLoss: 3.4859\tLR: 0.100000\nTraining Epoch: 14 [23296/50000]\tLoss: 3.3969\tLR: 0.100000\nTraining Epoch: 14 [23424/50000]\tLoss: 3.4919\tLR: 0.100000\nTraining Epoch: 14 [23552/50000]\tLoss: 3.6270\tLR: 0.100000\nTraining Epoch: 14 [23680/50000]\tLoss: 3.5894\tLR: 0.100000\nTraining Epoch: 14 [23808/50000]\tLoss: 3.5014\tLR: 0.100000\nTraining Epoch: 14 [23936/50000]\tLoss: 3.4564\tLR: 0.100000\nTraining Epoch: 14 [24064/50000]\tLoss: 3.4554\tLR: 0.100000\nTraining Epoch: 14 [24192/50000]\tLoss: 3.3929\tLR: 0.100000\nTraining Epoch: 14 [24320/50000]\tLoss: 3.5996\tLR: 0.100000\nTraining Epoch: 14 [24448/50000]\tLoss: 3.5465\tLR: 0.100000\nTraining Epoch: 14 [24576/50000]\tLoss: 3.3823\tLR: 0.100000\nTraining Epoch: 14 [24704/50000]\tLoss: 3.4478\tLR: 0.100000\nTraining Epoch: 14 [24832/50000]\tLoss: 3.5328\tLR: 0.100000\nTraining Epoch: 14 [24960/50000]\tLoss: 3.6162\tLR: 0.100000\nTraining Epoch: 14 [25088/50000]\tLoss: 3.3419\tLR: 0.100000\nTraining Epoch: 14 [25216/50000]\tLoss: 3.3295\tLR: 0.100000\nTraining Epoch: 14 [25344/50000]\tLoss: 3.5127\tLR: 0.100000\nTraining Epoch: 14 [25472/50000]\tLoss: 3.4628\tLR: 0.100000\nTraining Epoch: 14 [25600/50000]\tLoss: 3.6623\tLR: 0.100000\nTraining Epoch: 14 [25728/50000]\tLoss: 3.4822\tLR: 0.100000\nTraining Epoch: 14 [25856/50000]\tLoss: 3.4907\tLR: 0.100000\nTraining Epoch: 14 [25984/50000]\tLoss: 3.3065\tLR: 0.100000\nTraining Epoch: 14 [26112/50000]\tLoss: 3.4955\tLR: 0.100000\nTraining Epoch: 14 [26240/50000]\tLoss: 3.3130\tLR: 0.100000\nTraining Epoch: 14 [26368/50000]\tLoss: 3.4774\tLR: 0.100000\nTraining Epoch: 14 [26496/50000]\tLoss: 3.3533\tLR: 0.100000\nTraining Epoch: 14 [26624/50000]\tLoss: 3.5633\tLR: 0.100000\nTraining Epoch: 14 [26752/50000]\tLoss: 3.5681\tLR: 0.100000\nTraining Epoch: 14 [26880/50000]\tLoss: 3.4871\tLR: 0.100000\nTraining Epoch: 14 [27008/50000]\tLoss: 3.5164\tLR: 0.100000\nTraining Epoch: 14 [27136/50000]\tLoss: 3.4394\tLR: 0.100000\nTraining Epoch: 14 [27264/50000]\tLoss: 3.4635\tLR: 0.100000\nTraining Epoch: 14 [27392/50000]\tLoss: 3.4455\tLR: 0.100000\nTraining Epoch: 14 [27520/50000]\tLoss: 3.2558\tLR: 0.100000\nTraining Epoch: 14 [27648/50000]\tLoss: 3.7064\tLR: 0.100000\nTraining Epoch: 14 [27776/50000]\tLoss: 3.5130\tLR: 0.100000\nTraining Epoch: 14 [27904/50000]\tLoss: 3.7260\tLR: 0.100000\nTraining Epoch: 14 [28032/50000]\tLoss: 3.2793\tLR: 0.100000\nTraining Epoch: 14 [28160/50000]\tLoss: 3.7272\tLR: 0.100000\nTraining Epoch: 14 [28288/50000]\tLoss: 3.7378\tLR: 0.100000\nTraining Epoch: 14 [28416/50000]\tLoss: 3.8762\tLR: 0.100000\nTraining Epoch: 14 [28544/50000]\tLoss: 3.5634\tLR: 0.100000\nTraining Epoch: 14 [28672/50000]\tLoss: 3.4208\tLR: 0.100000\nTraining Epoch: 14 [28800/50000]\tLoss: 3.4685\tLR: 0.100000\nTraining Epoch: 14 [28928/50000]\tLoss: 3.4476\tLR: 0.100000\nTraining Epoch: 14 [29056/50000]\tLoss: 3.4565\tLR: 0.100000\nTraining Epoch: 14 [29184/50000]\tLoss: 3.5230\tLR: 0.100000\nTraining Epoch: 14 [29312/50000]\tLoss: 3.3140\tLR: 0.100000\nTraining Epoch: 14 [29440/50000]\tLoss: 3.2352\tLR: 0.100000\nTraining Epoch: 14 [29568/50000]\tLoss: 3.5664\tLR: 0.100000\nTraining Epoch: 14 [29696/50000]\tLoss: 3.6891\tLR: 0.100000\nTraining Epoch: 14 [29824/50000]\tLoss: 3.6207\tLR: 0.100000\nTraining Epoch: 14 [29952/50000]\tLoss: 3.6098\tLR: 0.100000\nTraining Epoch: 14 [30080/50000]\tLoss: 3.6332\tLR: 0.100000\nTraining Epoch: 14 [30208/50000]\tLoss: 3.6504\tLR: 0.100000\nTraining Epoch: 14 [30336/50000]\tLoss: 3.3507\tLR: 0.100000\nTraining Epoch: 14 [30464/50000]\tLoss: 3.4134\tLR: 0.100000\nTraining Epoch: 14 [30592/50000]\tLoss: 3.4860\tLR: 0.100000\nTraining Epoch: 14 [30720/50000]\tLoss: 3.5280\tLR: 0.100000\nTraining Epoch: 14 [30848/50000]\tLoss: 3.5601\tLR: 0.100000\nTraining Epoch: 14 [30976/50000]\tLoss: 3.5571\tLR: 0.100000\nTraining Epoch: 14 [31104/50000]\tLoss: 3.2151\tLR: 0.100000\nTraining Epoch: 14 [31232/50000]\tLoss: 3.3979\tLR: 0.100000\nTraining Epoch: 14 [31360/50000]\tLoss: 3.5559\tLR: 0.100000\nTraining Epoch: 14 [31488/50000]\tLoss: 3.2370\tLR: 0.100000\nTraining Epoch: 14 [31616/50000]\tLoss: 3.4070\tLR: 0.100000\nTraining Epoch: 14 [31744/50000]\tLoss: 3.4118\tLR: 0.100000\nTraining Epoch: 14 [31872/50000]\tLoss: 3.4746\tLR: 0.100000\nTraining Epoch: 14 [32000/50000]\tLoss: 3.5534\tLR: 0.100000\nTraining Epoch: 14 [32128/50000]\tLoss: 3.6240\tLR: 0.100000\nTraining Epoch: 14 [32256/50000]\tLoss: 3.3716\tLR: 0.100000\nTraining Epoch: 14 [32384/50000]\tLoss: 3.5503\tLR: 0.100000\nTraining Epoch: 14 [32512/50000]\tLoss: 3.3429\tLR: 0.100000\nTraining Epoch: 14 [32640/50000]\tLoss: 3.4896\tLR: 0.100000\nTraining Epoch: 14 [32768/50000]\tLoss: 3.6128\tLR: 0.100000\nTraining Epoch: 14 [32896/50000]\tLoss: 3.4785\tLR: 0.100000\nTraining Epoch: 14 [33024/50000]\tLoss: 3.6670\tLR: 0.100000\nTraining Epoch: 14 [33152/50000]\tLoss: 3.3955\tLR: 0.100000\nTraining Epoch: 14 [33280/50000]\tLoss: 3.6453\tLR: 0.100000\nTraining Epoch: 14 [33408/50000]\tLoss: 3.7209\tLR: 0.100000\nTraining Epoch: 14 [33536/50000]\tLoss: 3.4044\tLR: 0.100000\nTraining Epoch: 14 [33664/50000]\tLoss: 3.4181\tLR: 0.100000\nTraining Epoch: 14 [33792/50000]\tLoss: 3.4337\tLR: 0.100000\n","name":"stdout"},{"output_type":"stream","text":"Training Epoch: 14 [33920/50000]\tLoss: 3.6707\tLR: 0.100000\nTraining Epoch: 14 [34048/50000]\tLoss: 3.2900\tLR: 0.100000\nTraining Epoch: 14 [34176/50000]\tLoss: 3.4312\tLR: 0.100000\nTraining Epoch: 14 [34304/50000]\tLoss: 3.4664\tLR: 0.100000\nTraining Epoch: 14 [34432/50000]\tLoss: 3.5843\tLR: 0.100000\nTraining Epoch: 14 [34560/50000]\tLoss: 3.4711\tLR: 0.100000\nTraining Epoch: 14 [34688/50000]\tLoss: 3.7333\tLR: 0.100000\nTraining Epoch: 14 [34816/50000]\tLoss: 3.5017\tLR: 0.100000\nTraining Epoch: 14 [34944/50000]\tLoss: 3.5450\tLR: 0.100000\nTraining Epoch: 14 [35072/50000]\tLoss: 3.4722\tLR: 0.100000\nTraining Epoch: 14 [35200/50000]\tLoss: 3.5276\tLR: 0.100000\nTraining Epoch: 14 [35328/50000]\tLoss: 3.3836\tLR: 0.100000\nTraining Epoch: 14 [35456/50000]\tLoss: 3.3847\tLR: 0.100000\nTraining Epoch: 14 [35584/50000]\tLoss: 3.1934\tLR: 0.100000\nTraining Epoch: 14 [35712/50000]\tLoss: 3.4996\tLR: 0.100000\nTraining Epoch: 14 [35840/50000]\tLoss: 3.4674\tLR: 0.100000\nTraining Epoch: 14 [35968/50000]\tLoss: 3.3553\tLR: 0.100000\nTraining Epoch: 14 [36096/50000]\tLoss: 3.4449\tLR: 0.100000\nTraining Epoch: 14 [36224/50000]\tLoss: 3.5123\tLR: 0.100000\nTraining Epoch: 14 [36352/50000]\tLoss: 3.3193\tLR: 0.100000\nTraining Epoch: 14 [36480/50000]\tLoss: 3.4566\tLR: 0.100000\nTraining Epoch: 14 [36608/50000]\tLoss: 3.5177\tLR: 0.100000\nTraining Epoch: 14 [36736/50000]\tLoss: 3.6438\tLR: 0.100000\nTraining Epoch: 14 [36864/50000]\tLoss: 3.5637\tLR: 0.100000\nTraining Epoch: 14 [36992/50000]\tLoss: 3.4210\tLR: 0.100000\nTraining Epoch: 14 [37120/50000]\tLoss: 3.3426\tLR: 0.100000\nTraining Epoch: 14 [37248/50000]\tLoss: 3.6475\tLR: 0.100000\nTraining Epoch: 14 [37376/50000]\tLoss: 3.5925\tLR: 0.100000\nTraining Epoch: 14 [37504/50000]\tLoss: 3.6052\tLR: 0.100000\nTraining Epoch: 14 [37632/50000]\tLoss: 3.4153\tLR: 0.100000\nTraining Epoch: 14 [37760/50000]\tLoss: 3.5472\tLR: 0.100000\nTraining Epoch: 14 [37888/50000]\tLoss: 3.6056\tLR: 0.100000\nTraining Epoch: 14 [38016/50000]\tLoss: 3.4397\tLR: 0.100000\nTraining Epoch: 14 [38144/50000]\tLoss: 3.4902\tLR: 0.100000\nTraining Epoch: 14 [38272/50000]\tLoss: 3.5433\tLR: 0.100000\nTraining Epoch: 14 [38400/50000]\tLoss: 3.3928\tLR: 0.100000\nTraining Epoch: 14 [38528/50000]\tLoss: 3.5163\tLR: 0.100000\nTraining Epoch: 14 [38656/50000]\tLoss: 3.4869\tLR: 0.100000\nTraining Epoch: 14 [38784/50000]\tLoss: 3.5987\tLR: 0.100000\nTraining Epoch: 14 [38912/50000]\tLoss: 3.2863\tLR: 0.100000\nTraining Epoch: 14 [39040/50000]\tLoss: 3.3367\tLR: 0.100000\nTraining Epoch: 14 [39168/50000]\tLoss: 3.2847\tLR: 0.100000\nTraining Epoch: 14 [39296/50000]\tLoss: 3.3210\tLR: 0.100000\nTraining Epoch: 14 [39424/50000]\tLoss: 3.4518\tLR: 0.100000\nTraining Epoch: 14 [39552/50000]\tLoss: 3.3921\tLR: 0.100000\nTraining Epoch: 14 [39680/50000]\tLoss: 3.4461\tLR: 0.100000\nTraining Epoch: 14 [39808/50000]\tLoss: 3.5515\tLR: 0.100000\nTraining Epoch: 14 [39936/50000]\tLoss: 3.4997\tLR: 0.100000\nTraining Epoch: 14 [40064/50000]\tLoss: 3.3382\tLR: 0.100000\nTraining Epoch: 14 [40192/50000]\tLoss: 3.5772\tLR: 0.100000\nTraining Epoch: 14 [40320/50000]\tLoss: 3.2764\tLR: 0.100000\nTraining Epoch: 14 [40448/50000]\tLoss: 3.3141\tLR: 0.100000\nTraining Epoch: 14 [40576/50000]\tLoss: 3.5283\tLR: 0.100000\nTraining Epoch: 14 [40704/50000]\tLoss: 3.5121\tLR: 0.100000\nTraining Epoch: 14 [40832/50000]\tLoss: 3.4757\tLR: 0.100000\nTraining Epoch: 14 [40960/50000]\tLoss: 3.3610\tLR: 0.100000\nTraining Epoch: 14 [41088/50000]\tLoss: 3.5420\tLR: 0.100000\nTraining Epoch: 14 [41216/50000]\tLoss: 3.3508\tLR: 0.100000\nTraining Epoch: 14 [41344/50000]\tLoss: 3.4792\tLR: 0.100000\nTraining Epoch: 14 [41472/50000]\tLoss: 3.4699\tLR: 0.100000\nTraining Epoch: 14 [41600/50000]\tLoss: 3.3907\tLR: 0.100000\nTraining Epoch: 14 [41728/50000]\tLoss: 3.2507\tLR: 0.100000\nTraining Epoch: 14 [41856/50000]\tLoss: 3.5475\tLR: 0.100000\nTraining Epoch: 14 [41984/50000]\tLoss: 3.3134\tLR: 0.100000\nTraining Epoch: 14 [42112/50000]\tLoss: 3.5079\tLR: 0.100000\nTraining Epoch: 14 [42240/50000]\tLoss: 3.5309\tLR: 0.100000\nTraining Epoch: 14 [42368/50000]\tLoss: 3.3402\tLR: 0.100000\nTraining Epoch: 14 [42496/50000]\tLoss: 3.2990\tLR: 0.100000\nTraining Epoch: 14 [42624/50000]\tLoss: 3.4649\tLR: 0.100000\nTraining Epoch: 14 [42752/50000]\tLoss: 3.5666\tLR: 0.100000\nTraining Epoch: 14 [42880/50000]\tLoss: 3.3624\tLR: 0.100000\nTraining Epoch: 14 [43008/50000]\tLoss: 3.3655\tLR: 0.100000\nTraining Epoch: 14 [43136/50000]\tLoss: 3.3183\tLR: 0.100000\nTraining Epoch: 14 [43264/50000]\tLoss: 3.3910\tLR: 0.100000\nTraining Epoch: 14 [43392/50000]\tLoss: 3.5760\tLR: 0.100000\nTraining Epoch: 14 [43520/50000]\tLoss: 3.3990\tLR: 0.100000\nTraining Epoch: 14 [43648/50000]\tLoss: 3.3179\tLR: 0.100000\nTraining Epoch: 14 [43776/50000]\tLoss: 3.4642\tLR: 0.100000\nTraining Epoch: 14 [43904/50000]\tLoss: 3.3712\tLR: 0.100000\nTraining Epoch: 14 [44032/50000]\tLoss: 3.3486\tLR: 0.100000\nTraining Epoch: 14 [44160/50000]\tLoss: 3.5837\tLR: 0.100000\nTraining Epoch: 14 [44288/50000]\tLoss: 3.2763\tLR: 0.100000\nTraining Epoch: 14 [44416/50000]\tLoss: 3.3440\tLR: 0.100000\nTraining Epoch: 14 [44544/50000]\tLoss: 3.4939\tLR: 0.100000\nTraining Epoch: 14 [44672/50000]\tLoss: 3.4668\tLR: 0.100000\nTraining Epoch: 14 [44800/50000]\tLoss: 3.3243\tLR: 0.100000\nTraining Epoch: 14 [44928/50000]\tLoss: 3.4694\tLR: 0.100000\nTraining Epoch: 14 [45056/50000]\tLoss: 3.7303\tLR: 0.100000\nTraining Epoch: 14 [45184/50000]\tLoss: 3.3997\tLR: 0.100000\nTraining Epoch: 14 [45312/50000]\tLoss: 3.4552\tLR: 0.100000\nTraining Epoch: 14 [45440/50000]\tLoss: 3.4434\tLR: 0.100000\nTraining Epoch: 14 [45568/50000]\tLoss: 3.3391\tLR: 0.100000\nTraining Epoch: 14 [45696/50000]\tLoss: 3.4930\tLR: 0.100000\nTraining Epoch: 14 [45824/50000]\tLoss: 3.5939\tLR: 0.100000\nTraining Epoch: 14 [45952/50000]\tLoss: 3.7128\tLR: 0.100000\nTraining Epoch: 14 [46080/50000]\tLoss: 3.2879\tLR: 0.100000\nTraining Epoch: 14 [46208/50000]\tLoss: 3.3649\tLR: 0.100000\nTraining Epoch: 14 [46336/50000]\tLoss: 3.5905\tLR: 0.100000\nTraining Epoch: 14 [46464/50000]\tLoss: 3.4276\tLR: 0.100000\nTraining Epoch: 14 [46592/50000]\tLoss: 3.5795\tLR: 0.100000\nTraining Epoch: 14 [46720/50000]\tLoss: 3.4158\tLR: 0.100000\nTraining Epoch: 14 [46848/50000]\tLoss: 3.5498\tLR: 0.100000\nTraining Epoch: 14 [46976/50000]\tLoss: 3.5485\tLR: 0.100000\nTraining Epoch: 14 [47104/50000]\tLoss: 3.5037\tLR: 0.100000\nTraining Epoch: 14 [47232/50000]\tLoss: 3.4360\tLR: 0.100000\nTraining Epoch: 14 [47360/50000]\tLoss: 3.3085\tLR: 0.100000\nTraining Epoch: 14 [47488/50000]\tLoss: 3.5357\tLR: 0.100000\nTraining Epoch: 14 [47616/50000]\tLoss: 3.6204\tLR: 0.100000\nTraining Epoch: 14 [47744/50000]\tLoss: 3.4936\tLR: 0.100000\nTraining Epoch: 14 [47872/50000]\tLoss: 3.5754\tLR: 0.100000\nTraining Epoch: 14 [48000/50000]\tLoss: 3.3570\tLR: 0.100000\nTraining Epoch: 14 [48128/50000]\tLoss: 3.4816\tLR: 0.100000\nTraining Epoch: 14 [48256/50000]\tLoss: 3.4099\tLR: 0.100000\nTraining Epoch: 14 [48384/50000]\tLoss: 3.5002\tLR: 0.100000\nTraining Epoch: 14 [48512/50000]\tLoss: 3.4369\tLR: 0.100000\nTraining Epoch: 14 [48640/50000]\tLoss: 3.4649\tLR: 0.100000\nTraining Epoch: 14 [48768/50000]\tLoss: 3.3509\tLR: 0.100000\nTraining Epoch: 14 [48896/50000]\tLoss: 3.5926\tLR: 0.100000\nTraining Epoch: 14 [49024/50000]\tLoss: 3.2879\tLR: 0.100000\nTraining Epoch: 14 [49152/50000]\tLoss: 3.5405\tLR: 0.100000\nTraining Epoch: 14 [49280/50000]\tLoss: 3.5040\tLR: 0.100000\nTraining Epoch: 14 [49408/50000]\tLoss: 3.4978\tLR: 0.100000\nTraining Epoch: 14 [49536/50000]\tLoss: 3.4388\tLR: 0.100000\nTraining Epoch: 14 [49664/50000]\tLoss: 3.5420\tLR: 0.100000\nTraining Epoch: 14 [49792/50000]\tLoss: 3.4541\tLR: 0.100000\nTraining Epoch: 14 [49920/50000]\tLoss: 3.4510\tLR: 0.100000\nTraining Epoch: 14 [50000/50000]\tLoss: 3.7430\tLR: 0.100000\n=========================================================================================\nTraining Epoch: 15 [128/50000]\tLoss: 3.2852\tLR: 0.100000\nTraining Epoch: 15 [256/50000]\tLoss: 3.2115\tLR: 0.100000\nTraining Epoch: 15 [384/50000]\tLoss: 3.5221\tLR: 0.100000\nTraining Epoch: 15 [512/50000]\tLoss: 3.2540\tLR: 0.100000\nTraining Epoch: 15 [640/50000]\tLoss: 3.3144\tLR: 0.100000\nTraining Epoch: 15 [768/50000]\tLoss: 3.4213\tLR: 0.100000\nTraining Epoch: 15 [896/50000]\tLoss: 3.1418\tLR: 0.100000\nTraining Epoch: 15 [1024/50000]\tLoss: 3.7684\tLR: 0.100000\nTraining Epoch: 15 [1152/50000]\tLoss: 3.3146\tLR: 0.100000\nTraining Epoch: 15 [1280/50000]\tLoss: 3.7263\tLR: 0.100000\nTraining Epoch: 15 [1408/50000]\tLoss: 3.4775\tLR: 0.100000\nTraining Epoch: 15 [1536/50000]\tLoss: 3.4063\tLR: 0.100000\n","name":"stdout"},{"output_type":"stream","text":"Training Epoch: 15 [1664/50000]\tLoss: 3.4623\tLR: 0.100000\nTraining Epoch: 15 [1792/50000]\tLoss: 3.6385\tLR: 0.100000\nTraining Epoch: 15 [1920/50000]\tLoss: 3.5614\tLR: 0.100000\nTraining Epoch: 15 [2048/50000]\tLoss: 3.3157\tLR: 0.100000\nTraining Epoch: 15 [2176/50000]\tLoss: 3.3473\tLR: 0.100000\nTraining Epoch: 15 [2304/50000]\tLoss: 3.5927\tLR: 0.100000\nTraining Epoch: 15 [2432/50000]\tLoss: 3.2505\tLR: 0.100000\nTraining Epoch: 15 [2560/50000]\tLoss: 3.7867\tLR: 0.100000\nTraining Epoch: 15 [2688/50000]\tLoss: 3.4634\tLR: 0.100000\nTraining Epoch: 15 [2816/50000]\tLoss: 3.5310\tLR: 0.100000\nTraining Epoch: 15 [2944/50000]\tLoss: 3.6192\tLR: 0.100000\nTraining Epoch: 15 [3072/50000]\tLoss: 3.3174\tLR: 0.100000\nTraining Epoch: 15 [3200/50000]\tLoss: 3.4534\tLR: 0.100000\nTraining Epoch: 15 [3328/50000]\tLoss: 3.2214\tLR: 0.100000\nTraining Epoch: 15 [3456/50000]\tLoss: 3.4288\tLR: 0.100000\nTraining Epoch: 15 [3584/50000]\tLoss: 3.3262\tLR: 0.100000\nTraining Epoch: 15 [3712/50000]\tLoss: 3.6206\tLR: 0.100000\nTraining Epoch: 15 [3840/50000]\tLoss: 3.3776\tLR: 0.100000\nTraining Epoch: 15 [3968/50000]\tLoss: 3.4656\tLR: 0.100000\nTraining Epoch: 15 [4096/50000]\tLoss: 3.7414\tLR: 0.100000\nTraining Epoch: 15 [4224/50000]\tLoss: 3.6468\tLR: 0.100000\nTraining Epoch: 15 [4352/50000]\tLoss: 3.4446\tLR: 0.100000\nTraining Epoch: 15 [4480/50000]\tLoss: 3.5069\tLR: 0.100000\nTraining Epoch: 15 [4608/50000]\tLoss: 3.6021\tLR: 0.100000\nTraining Epoch: 15 [4736/50000]\tLoss: 3.6356\tLR: 0.100000\nTraining Epoch: 15 [4864/50000]\tLoss: 3.3849\tLR: 0.100000\nTraining Epoch: 15 [4992/50000]\tLoss: 3.4691\tLR: 0.100000\nTraining Epoch: 15 [5120/50000]\tLoss: 3.5469\tLR: 0.100000\nTraining Epoch: 15 [5248/50000]\tLoss: 3.4777\tLR: 0.100000\nTraining Epoch: 15 [5376/50000]\tLoss: 3.5420\tLR: 0.100000\nTraining Epoch: 15 [5504/50000]\tLoss: 3.1906\tLR: 0.100000\nTraining Epoch: 15 [5632/50000]\tLoss: 3.3971\tLR: 0.100000\nTraining Epoch: 15 [5760/50000]\tLoss: 3.4399\tLR: 0.100000\nTraining Epoch: 15 [5888/50000]\tLoss: 3.5035\tLR: 0.100000\nTraining Epoch: 15 [6016/50000]\tLoss: 3.3517\tLR: 0.100000\nTraining Epoch: 15 [6144/50000]\tLoss: 3.4709\tLR: 0.100000\nTraining Epoch: 15 [6272/50000]\tLoss: 3.3927\tLR: 0.100000\nTraining Epoch: 15 [6400/50000]\tLoss: 3.3578\tLR: 0.100000\nTraining Epoch: 15 [6528/50000]\tLoss: 3.3894\tLR: 0.100000\nTraining Epoch: 15 [6656/50000]\tLoss: 3.4666\tLR: 0.100000\nTraining Epoch: 15 [6784/50000]\tLoss: 3.3695\tLR: 0.100000\nTraining Epoch: 15 [6912/50000]\tLoss: 3.4151\tLR: 0.100000\nTraining Epoch: 15 [7040/50000]\tLoss: 3.5256\tLR: 0.100000\nTraining Epoch: 15 [7168/50000]\tLoss: 3.5561\tLR: 0.100000\nTraining Epoch: 15 [7296/50000]\tLoss: 3.6353\tLR: 0.100000\nTraining Epoch: 15 [7424/50000]\tLoss: 3.6396\tLR: 0.100000\nTraining Epoch: 15 [7552/50000]\tLoss: 3.5957\tLR: 0.100000\nTraining Epoch: 15 [7680/50000]\tLoss: 3.8138\tLR: 0.100000\nTraining Epoch: 15 [7808/50000]\tLoss: 3.3018\tLR: 0.100000\nTraining Epoch: 15 [7936/50000]\tLoss: 3.4526\tLR: 0.100000\nTraining Epoch: 15 [8064/50000]\tLoss: 3.2922\tLR: 0.100000\nTraining Epoch: 15 [8192/50000]\tLoss: 3.6548\tLR: 0.100000\nTraining Epoch: 15 [8320/50000]\tLoss: 3.2081\tLR: 0.100000\nTraining Epoch: 15 [8448/50000]\tLoss: 3.3509\tLR: 0.100000\nTraining Epoch: 15 [8576/50000]\tLoss: 3.4900\tLR: 0.100000\nTraining Epoch: 15 [8704/50000]\tLoss: 3.5985\tLR: 0.100000\nTraining Epoch: 15 [8832/50000]\tLoss: 3.3167\tLR: 0.100000\nTraining Epoch: 15 [8960/50000]\tLoss: 3.4993\tLR: 0.100000\nTraining Epoch: 15 [9088/50000]\tLoss: 3.3738\tLR: 0.100000\nTraining Epoch: 15 [9216/50000]\tLoss: 3.4679\tLR: 0.100000\nTraining Epoch: 15 [9344/50000]\tLoss: 3.4060\tLR: 0.100000\nTraining Epoch: 15 [9472/50000]\tLoss: 3.5173\tLR: 0.100000\nTraining Epoch: 15 [9600/50000]\tLoss: 3.2756\tLR: 0.100000\nTraining Epoch: 15 [9728/50000]\tLoss: 3.5273\tLR: 0.100000\nTraining Epoch: 15 [9856/50000]\tLoss: 3.2501\tLR: 0.100000\nTraining Epoch: 15 [9984/50000]\tLoss: 3.5134\tLR: 0.100000\nTraining Epoch: 15 [10112/50000]\tLoss: 3.3617\tLR: 0.100000\nTraining Epoch: 15 [10240/50000]\tLoss: 3.4007\tLR: 0.100000\nTraining Epoch: 15 [10368/50000]\tLoss: 3.2566\tLR: 0.100000\nTraining Epoch: 15 [10496/50000]\tLoss: 3.5279\tLR: 0.100000\nTraining Epoch: 15 [10624/50000]\tLoss: 3.4913\tLR: 0.100000\nTraining Epoch: 15 [10752/50000]\tLoss: 3.4436\tLR: 0.100000\nTraining Epoch: 15 [10880/50000]\tLoss: 3.3848\tLR: 0.100000\nTraining Epoch: 15 [11008/50000]\tLoss: 3.2453\tLR: 0.100000\nTraining Epoch: 15 [11136/50000]\tLoss: 3.4879\tLR: 0.100000\nTraining Epoch: 15 [11264/50000]\tLoss: 3.3216\tLR: 0.100000\nTraining Epoch: 15 [11392/50000]\tLoss: 3.3447\tLR: 0.100000\nTraining Epoch: 15 [11520/50000]\tLoss: 3.3308\tLR: 0.100000\nTraining Epoch: 15 [11648/50000]\tLoss: 3.2790\tLR: 0.100000\nTraining Epoch: 15 [11776/50000]\tLoss: 3.3469\tLR: 0.100000\nTraining Epoch: 15 [11904/50000]\tLoss: 3.3801\tLR: 0.100000\nTraining Epoch: 15 [12032/50000]\tLoss: 3.3463\tLR: 0.100000\nTraining Epoch: 15 [12160/50000]\tLoss: 3.2628\tLR: 0.100000\nTraining Epoch: 15 [12288/50000]\tLoss: 3.4445\tLR: 0.100000\nTraining Epoch: 15 [12416/50000]\tLoss: 3.5077\tLR: 0.100000\nTraining Epoch: 15 [12544/50000]\tLoss: 3.3425\tLR: 0.100000\nTraining Epoch: 15 [12672/50000]\tLoss: 3.3189\tLR: 0.100000\nTraining Epoch: 15 [12800/50000]\tLoss: 3.1153\tLR: 0.100000\nTraining Epoch: 15 [12928/50000]\tLoss: 3.2793\tLR: 0.100000\nTraining Epoch: 15 [13056/50000]\tLoss: 2.9953\tLR: 0.100000\nTraining Epoch: 15 [13184/50000]\tLoss: 3.6236\tLR: 0.100000\nTraining Epoch: 15 [13312/50000]\tLoss: 3.4051\tLR: 0.100000\nTraining Epoch: 15 [13440/50000]\tLoss: 3.4027\tLR: 0.100000\nTraining Epoch: 15 [13568/50000]\tLoss: 3.3968\tLR: 0.100000\nTraining Epoch: 15 [13696/50000]\tLoss: 3.3764\tLR: 0.100000\nTraining Epoch: 15 [13824/50000]\tLoss: 3.4178\tLR: 0.100000\nTraining Epoch: 15 [13952/50000]\tLoss: 3.3207\tLR: 0.100000\nTraining Epoch: 15 [14080/50000]\tLoss: 3.6710\tLR: 0.100000\nTraining Epoch: 15 [14208/50000]\tLoss: 3.2564\tLR: 0.100000\nTraining Epoch: 15 [14336/50000]\tLoss: 3.3797\tLR: 0.100000\nTraining Epoch: 15 [14464/50000]\tLoss: 3.4592\tLR: 0.100000\nTraining Epoch: 15 [14592/50000]\tLoss: 3.3443\tLR: 0.100000\nTraining Epoch: 15 [14720/50000]\tLoss: 3.4497\tLR: 0.100000\nTraining Epoch: 15 [14848/50000]\tLoss: 3.5863\tLR: 0.100000\nTraining Epoch: 15 [14976/50000]\tLoss: 3.1274\tLR: 0.100000\nTraining Epoch: 15 [15104/50000]\tLoss: 3.4202\tLR: 0.100000\nTraining Epoch: 15 [15232/50000]\tLoss: 3.5051\tLR: 0.100000\nTraining Epoch: 15 [15360/50000]\tLoss: 3.5093\tLR: 0.100000\nTraining Epoch: 15 [15488/50000]\tLoss: 3.4413\tLR: 0.100000\nTraining Epoch: 15 [15616/50000]\tLoss: 3.6374\tLR: 0.100000\nTraining Epoch: 15 [15744/50000]\tLoss: 3.5152\tLR: 0.100000\nTraining Epoch: 15 [15872/50000]\tLoss: 3.1944\tLR: 0.100000\nTraining Epoch: 15 [16000/50000]\tLoss: 3.5015\tLR: 0.100000\nTraining Epoch: 15 [16128/50000]\tLoss: 3.2362\tLR: 0.100000\nTraining Epoch: 15 [16256/50000]\tLoss: 3.3353\tLR: 0.100000\nTraining Epoch: 15 [16384/50000]\tLoss: 3.4142\tLR: 0.100000\nTraining Epoch: 15 [16512/50000]\tLoss: 3.5201\tLR: 0.100000\nTraining Epoch: 15 [16640/50000]\tLoss: 3.3856\tLR: 0.100000\nTraining Epoch: 15 [16768/50000]\tLoss: 3.5423\tLR: 0.100000\nTraining Epoch: 15 [16896/50000]\tLoss: 3.5450\tLR: 0.100000\nTraining Epoch: 15 [17024/50000]\tLoss: 3.1168\tLR: 0.100000\nTraining Epoch: 15 [17152/50000]\tLoss: 3.4317\tLR: 0.100000\nTraining Epoch: 15 [17280/50000]\tLoss: 3.3495\tLR: 0.100000\nTraining Epoch: 15 [17408/50000]\tLoss: 3.4043\tLR: 0.100000\nTraining Epoch: 15 [17536/50000]\tLoss: 3.3782\tLR: 0.100000\nTraining Epoch: 15 [17664/50000]\tLoss: 3.4320\tLR: 0.100000\nTraining Epoch: 15 [17792/50000]\tLoss: 3.7781\tLR: 0.100000\nTraining Epoch: 15 [17920/50000]\tLoss: 3.5443\tLR: 0.100000\nTraining Epoch: 15 [18048/50000]\tLoss: 3.2021\tLR: 0.100000\nTraining Epoch: 15 [18176/50000]\tLoss: 3.3524\tLR: 0.100000\nTraining Epoch: 15 [18304/50000]\tLoss: 3.4386\tLR: 0.100000\nTraining Epoch: 15 [18432/50000]\tLoss: 3.3958\tLR: 0.100000\nTraining Epoch: 15 [18560/50000]\tLoss: 3.4207\tLR: 0.100000\nTraining Epoch: 15 [18688/50000]\tLoss: 3.5536\tLR: 0.100000\nTraining Epoch: 15 [18816/50000]\tLoss: 3.4848\tLR: 0.100000\nTraining Epoch: 15 [18944/50000]\tLoss: 3.5608\tLR: 0.100000\nTraining Epoch: 15 [19072/50000]\tLoss: 3.6899\tLR: 0.100000\nTraining Epoch: 15 [19200/50000]\tLoss: 3.4565\tLR: 0.100000\nTraining Epoch: 15 [19328/50000]\tLoss: 3.3548\tLR: 0.100000\nTraining Epoch: 15 [19456/50000]\tLoss: 3.3703\tLR: 0.100000\nTraining Epoch: 15 [19584/50000]\tLoss: 3.3467\tLR: 0.100000\n","name":"stdout"},{"output_type":"stream","text":"Training Epoch: 15 [19712/50000]\tLoss: 3.2693\tLR: 0.100000\nTraining Epoch: 15 [19840/50000]\tLoss: 3.4837\tLR: 0.100000\nTraining Epoch: 15 [19968/50000]\tLoss: 3.4730\tLR: 0.100000\nTraining Epoch: 15 [20096/50000]\tLoss: 3.6108\tLR: 0.100000\nTraining Epoch: 15 [20224/50000]\tLoss: 3.3932\tLR: 0.100000\nTraining Epoch: 15 [20352/50000]\tLoss: 3.5080\tLR: 0.100000\nTraining Epoch: 15 [20480/50000]\tLoss: 3.4961\tLR: 0.100000\nTraining Epoch: 15 [20608/50000]\tLoss: 3.5388\tLR: 0.100000\nTraining Epoch: 15 [20736/50000]\tLoss: 3.5080\tLR: 0.100000\nTraining Epoch: 15 [20864/50000]\tLoss: 3.3471\tLR: 0.100000\nTraining Epoch: 15 [20992/50000]\tLoss: 3.4762\tLR: 0.100000\nTraining Epoch: 15 [21120/50000]\tLoss: 3.6620\tLR: 0.100000\nTraining Epoch: 15 [21248/50000]\tLoss: 3.4507\tLR: 0.100000\nTraining Epoch: 15 [21376/50000]\tLoss: 3.5331\tLR: 0.100000\nTraining Epoch: 15 [21504/50000]\tLoss: 3.3804\tLR: 0.100000\nTraining Epoch: 15 [21632/50000]\tLoss: 3.5233\tLR: 0.100000\nTraining Epoch: 15 [21760/50000]\tLoss: 3.5669\tLR: 0.100000\nTraining Epoch: 15 [21888/50000]\tLoss: 3.5213\tLR: 0.100000\nTraining Epoch: 15 [22016/50000]\tLoss: 3.4614\tLR: 0.100000\nTraining Epoch: 15 [22144/50000]\tLoss: 3.2927\tLR: 0.100000\nTraining Epoch: 15 [22272/50000]\tLoss: 3.5696\tLR: 0.100000\nTraining Epoch: 15 [22400/50000]\tLoss: 3.2113\tLR: 0.100000\nTraining Epoch: 15 [22528/50000]\tLoss: 3.3625\tLR: 0.100000\nTraining Epoch: 15 [22656/50000]\tLoss: 3.4880\tLR: 0.100000\nTraining Epoch: 15 [22784/50000]\tLoss: 3.6789\tLR: 0.100000\nTraining Epoch: 15 [22912/50000]\tLoss: 3.2668\tLR: 0.100000\nTraining Epoch: 15 [23040/50000]\tLoss: 3.4418\tLR: 0.100000\nTraining Epoch: 15 [23168/50000]\tLoss: 3.3872\tLR: 0.100000\nTraining Epoch: 15 [23296/50000]\tLoss: 3.3642\tLR: 0.100000\nTraining Epoch: 15 [23424/50000]\tLoss: 3.2504\tLR: 0.100000\nTraining Epoch: 15 [23552/50000]\tLoss: 3.0970\tLR: 0.100000\nTraining Epoch: 15 [23680/50000]\tLoss: 3.4128\tLR: 0.100000\nTraining Epoch: 15 [23808/50000]\tLoss: 3.5909\tLR: 0.100000\nTraining Epoch: 15 [23936/50000]\tLoss: 3.4788\tLR: 0.100000\nTraining Epoch: 15 [24064/50000]\tLoss: 3.2013\tLR: 0.100000\nTraining Epoch: 15 [24192/50000]\tLoss: 3.5466\tLR: 0.100000\nTraining Epoch: 15 [24320/50000]\tLoss: 3.5235\tLR: 0.100000\nTraining Epoch: 15 [24448/50000]\tLoss: 3.3153\tLR: 0.100000\nTraining Epoch: 15 [24576/50000]\tLoss: 3.4208\tLR: 0.100000\nTraining Epoch: 15 [24704/50000]\tLoss: 3.3921\tLR: 0.100000\nTraining Epoch: 15 [24832/50000]\tLoss: 3.3896\tLR: 0.100000\nTraining Epoch: 15 [24960/50000]\tLoss: 3.2715\tLR: 0.100000\nTraining Epoch: 15 [25088/50000]\tLoss: 3.5163\tLR: 0.100000\nTraining Epoch: 15 [25216/50000]\tLoss: 3.2821\tLR: 0.100000\nTraining Epoch: 15 [25344/50000]\tLoss: 3.2904\tLR: 0.100000\nTraining Epoch: 15 [25472/50000]\tLoss: 3.5215\tLR: 0.100000\nTraining Epoch: 15 [25600/50000]\tLoss: 3.4659\tLR: 0.100000\nTraining Epoch: 15 [25728/50000]\tLoss: 3.4725\tLR: 0.100000\nTraining Epoch: 15 [25856/50000]\tLoss: 3.2116\tLR: 0.100000\nTraining Epoch: 15 [25984/50000]\tLoss: 3.3906\tLR: 0.100000\nTraining Epoch: 15 [26112/50000]\tLoss: 3.2709\tLR: 0.100000\nTraining Epoch: 15 [26240/50000]\tLoss: 3.4884\tLR: 0.100000\nTraining Epoch: 15 [26368/50000]\tLoss: 3.4967\tLR: 0.100000\nTraining Epoch: 15 [26496/50000]\tLoss: 3.6039\tLR: 0.100000\nTraining Epoch: 15 [26624/50000]\tLoss: 3.5664\tLR: 0.100000\nTraining Epoch: 15 [26752/50000]\tLoss: 3.5213\tLR: 0.100000\nTraining Epoch: 15 [26880/50000]\tLoss: 3.5109\tLR: 0.100000\nTraining Epoch: 15 [27008/50000]\tLoss: 3.4729\tLR: 0.100000\nTraining Epoch: 15 [27136/50000]\tLoss: 3.4193\tLR: 0.100000\nTraining Epoch: 15 [27264/50000]\tLoss: 3.2451\tLR: 0.100000\nTraining Epoch: 15 [27392/50000]\tLoss: 3.3855\tLR: 0.100000\nTraining Epoch: 15 [27520/50000]\tLoss: 3.3412\tLR: 0.100000\nTraining Epoch: 15 [27648/50000]\tLoss: 3.1549\tLR: 0.100000\nTraining Epoch: 15 [27776/50000]\tLoss: 3.4040\tLR: 0.100000\nTraining Epoch: 15 [27904/50000]\tLoss: 3.5088\tLR: 0.100000\nTraining Epoch: 15 [28032/50000]\tLoss: 3.4717\tLR: 0.100000\nTraining Epoch: 15 [28160/50000]\tLoss: 3.3953\tLR: 0.100000\nTraining Epoch: 15 [28288/50000]\tLoss: 3.3966\tLR: 0.100000\nTraining Epoch: 15 [28416/50000]\tLoss: 3.3355\tLR: 0.100000\nTraining Epoch: 15 [28544/50000]\tLoss: 3.3953\tLR: 0.100000\nTraining Epoch: 15 [28672/50000]\tLoss: 3.4890\tLR: 0.100000\nTraining Epoch: 15 [28800/50000]\tLoss: 3.6438\tLR: 0.100000\nTraining Epoch: 15 [28928/50000]\tLoss: 3.3706\tLR: 0.100000\nTraining Epoch: 15 [29056/50000]\tLoss: 3.1288\tLR: 0.100000\nTraining Epoch: 15 [29184/50000]\tLoss: 3.5450\tLR: 0.100000\nTraining Epoch: 15 [29312/50000]\tLoss: 3.3869\tLR: 0.100000\nTraining Epoch: 15 [29440/50000]\tLoss: 3.7018\tLR: 0.100000\nTraining Epoch: 15 [29568/50000]\tLoss: 3.2212\tLR: 0.100000\nTraining Epoch: 15 [29696/50000]\tLoss: 3.4739\tLR: 0.100000\nTraining Epoch: 15 [29824/50000]\tLoss: 3.1943\tLR: 0.100000\nTraining Epoch: 15 [29952/50000]\tLoss: 3.0976\tLR: 0.100000\nTraining Epoch: 15 [30080/50000]\tLoss: 3.3844\tLR: 0.100000\nTraining Epoch: 15 [30208/50000]\tLoss: 3.4668\tLR: 0.100000\nTraining Epoch: 15 [30336/50000]\tLoss: 3.3292\tLR: 0.100000\nTraining Epoch: 15 [30464/50000]\tLoss: 3.2590\tLR: 0.100000\nTraining Epoch: 15 [30592/50000]\tLoss: 3.4184\tLR: 0.100000\nTraining Epoch: 15 [30720/50000]\tLoss: 3.5622\tLR: 0.100000\nTraining Epoch: 15 [30848/50000]\tLoss: 3.5336\tLR: 0.100000\nTraining Epoch: 15 [30976/50000]\tLoss: 3.4861\tLR: 0.100000\nTraining Epoch: 15 [31104/50000]\tLoss: 3.6702\tLR: 0.100000\nTraining Epoch: 15 [31232/50000]\tLoss: 3.4542\tLR: 0.100000\nTraining Epoch: 15 [31360/50000]\tLoss: 3.3894\tLR: 0.100000\nTraining Epoch: 15 [31488/50000]\tLoss: 3.3424\tLR: 0.100000\nTraining Epoch: 15 [31616/50000]\tLoss: 3.6518\tLR: 0.100000\nTraining Epoch: 15 [31744/50000]\tLoss: 3.5039\tLR: 0.100000\nTraining Epoch: 15 [31872/50000]\tLoss: 3.6452\tLR: 0.100000\nTraining Epoch: 15 [32000/50000]\tLoss: 3.3995\tLR: 0.100000\nTraining Epoch: 15 [32128/50000]\tLoss: 3.5100\tLR: 0.100000\nTraining Epoch: 15 [32256/50000]\tLoss: 3.5211\tLR: 0.100000\nTraining Epoch: 15 [32384/50000]\tLoss: 3.5741\tLR: 0.100000\nTraining Epoch: 15 [32512/50000]\tLoss: 3.4398\tLR: 0.100000\nTraining Epoch: 15 [32640/50000]\tLoss: 3.7087\tLR: 0.100000\nTraining Epoch: 15 [32768/50000]\tLoss: 3.4267\tLR: 0.100000\nTraining Epoch: 15 [32896/50000]\tLoss: 3.4143\tLR: 0.100000\nTraining Epoch: 15 [33024/50000]\tLoss: 3.4632\tLR: 0.100000\nTraining Epoch: 15 [33152/50000]\tLoss: 3.5249\tLR: 0.100000\nTraining Epoch: 15 [33280/50000]\tLoss: 3.3953\tLR: 0.100000\nTraining Epoch: 15 [33408/50000]\tLoss: 3.4124\tLR: 0.100000\nTraining Epoch: 15 [33536/50000]\tLoss: 3.2943\tLR: 0.100000\nTraining Epoch: 15 [33664/50000]\tLoss: 3.5958\tLR: 0.100000\nTraining Epoch: 15 [33792/50000]\tLoss: 3.1613\tLR: 0.100000\nTraining Epoch: 15 [33920/50000]\tLoss: 3.3470\tLR: 0.100000\nTraining Epoch: 15 [34048/50000]\tLoss: 3.5562\tLR: 0.100000\nTraining Epoch: 15 [34176/50000]\tLoss: 3.5347\tLR: 0.100000\nTraining Epoch: 15 [34304/50000]\tLoss: 3.0777\tLR: 0.100000\nTraining Epoch: 15 [34432/50000]\tLoss: 3.3727\tLR: 0.100000\nTraining Epoch: 15 [34560/50000]\tLoss: 3.3147\tLR: 0.100000\nTraining Epoch: 15 [34688/50000]\tLoss: 3.5506\tLR: 0.100000\nTraining Epoch: 15 [34816/50000]\tLoss: 3.2940\tLR: 0.100000\nTraining Epoch: 15 [34944/50000]\tLoss: 3.6014\tLR: 0.100000\nTraining Epoch: 15 [35072/50000]\tLoss: 3.2433\tLR: 0.100000\nTraining Epoch: 15 [35200/50000]\tLoss: 3.6257\tLR: 0.100000\nTraining Epoch: 15 [35328/50000]\tLoss: 3.7153\tLR: 0.100000\nTraining Epoch: 15 [35456/50000]\tLoss: 3.3896\tLR: 0.100000\nTraining Epoch: 15 [35584/50000]\tLoss: 3.4449\tLR: 0.100000\nTraining Epoch: 15 [35712/50000]\tLoss: 3.3159\tLR: 0.100000\nTraining Epoch: 15 [35840/50000]\tLoss: 3.3892\tLR: 0.100000\nTraining Epoch: 15 [35968/50000]\tLoss: 3.4967\tLR: 0.100000\nTraining Epoch: 15 [36096/50000]\tLoss: 3.4831\tLR: 0.100000\nTraining Epoch: 15 [36224/50000]\tLoss: 3.5648\tLR: 0.100000\nTraining Epoch: 15 [36352/50000]\tLoss: 3.4843\tLR: 0.100000\nTraining Epoch: 15 [36480/50000]\tLoss: 3.5383\tLR: 0.100000\nTraining Epoch: 15 [36608/50000]\tLoss: 3.5457\tLR: 0.100000\nTraining Epoch: 15 [36736/50000]\tLoss: 3.6151\tLR: 0.100000\nTraining Epoch: 15 [36864/50000]\tLoss: 3.4733\tLR: 0.100000\nTraining Epoch: 15 [36992/50000]\tLoss: 3.3725\tLR: 0.100000\nTraining Epoch: 15 [37120/50000]\tLoss: 3.3357\tLR: 0.100000\nTraining Epoch: 15 [37248/50000]\tLoss: 3.4877\tLR: 0.100000\nTraining Epoch: 15 [37376/50000]\tLoss: 3.2305\tLR: 0.100000\nTraining Epoch: 15 [37504/50000]\tLoss: 3.3092\tLR: 0.100000\nTraining Epoch: 15 [37632/50000]\tLoss: 3.3841\tLR: 0.100000\n","name":"stdout"},{"output_type":"stream","text":"Training Epoch: 15 [37760/50000]\tLoss: 3.3611\tLR: 0.100000\nTraining Epoch: 15 [37888/50000]\tLoss: 3.4236\tLR: 0.100000\nTraining Epoch: 15 [38016/50000]\tLoss: 3.4685\tLR: 0.100000\nTraining Epoch: 15 [38144/50000]\tLoss: 3.3788\tLR: 0.100000\nTraining Epoch: 15 [38272/50000]\tLoss: 3.3357\tLR: 0.100000\nTraining Epoch: 15 [38400/50000]\tLoss: 3.6473\tLR: 0.100000\nTraining Epoch: 15 [38528/50000]\tLoss: 3.2958\tLR: 0.100000\nTraining Epoch: 15 [38656/50000]\tLoss: 3.5422\tLR: 0.100000\nTraining Epoch: 15 [38784/50000]\tLoss: 3.4426\tLR: 0.100000\nTraining Epoch: 15 [38912/50000]\tLoss: 3.4523\tLR: 0.100000\nTraining Epoch: 15 [39040/50000]\tLoss: 3.5784\tLR: 0.100000\nTraining Epoch: 15 [39168/50000]\tLoss: 3.6002\tLR: 0.100000\nTraining Epoch: 15 [39296/50000]\tLoss: 3.3635\tLR: 0.100000\nTraining Epoch: 15 [39424/50000]\tLoss: 3.2817\tLR: 0.100000\nTraining Epoch: 15 [39552/50000]\tLoss: 3.5705\tLR: 0.100000\nTraining Epoch: 15 [39680/50000]\tLoss: 3.5002\tLR: 0.100000\nTraining Epoch: 15 [39808/50000]\tLoss: 3.3494\tLR: 0.100000\nTraining Epoch: 15 [39936/50000]\tLoss: 3.3837\tLR: 0.100000\nTraining Epoch: 15 [40064/50000]\tLoss: 3.3747\tLR: 0.100000\nTraining Epoch: 15 [40192/50000]\tLoss: 3.4675\tLR: 0.100000\nTraining Epoch: 15 [40320/50000]\tLoss: 3.3648\tLR: 0.100000\nTraining Epoch: 15 [40448/50000]\tLoss: 3.6027\tLR: 0.100000\nTraining Epoch: 15 [40576/50000]\tLoss: 3.2267\tLR: 0.100000\nTraining Epoch: 15 [40704/50000]\tLoss: 3.5819\tLR: 0.100000\nTraining Epoch: 15 [40832/50000]\tLoss: 3.2899\tLR: 0.100000\nTraining Epoch: 15 [40960/50000]\tLoss: 3.5498\tLR: 0.100000\nTraining Epoch: 15 [41088/50000]\tLoss: 3.2643\tLR: 0.100000\nTraining Epoch: 15 [41216/50000]\tLoss: 3.3283\tLR: 0.100000\nTraining Epoch: 15 [41344/50000]\tLoss: 3.3105\tLR: 0.100000\nTraining Epoch: 15 [41472/50000]\tLoss: 3.2744\tLR: 0.100000\nTraining Epoch: 15 [41600/50000]\tLoss: 3.2477\tLR: 0.100000\nTraining Epoch: 15 [41728/50000]\tLoss: 3.3777\tLR: 0.100000\nTraining Epoch: 15 [41856/50000]\tLoss: 3.5913\tLR: 0.100000\nTraining Epoch: 15 [41984/50000]\tLoss: 3.1651\tLR: 0.100000\nTraining Epoch: 15 [42112/50000]\tLoss: 3.3879\tLR: 0.100000\nTraining Epoch: 15 [42240/50000]\tLoss: 3.4051\tLR: 0.100000\nTraining Epoch: 15 [42368/50000]\tLoss: 3.4937\tLR: 0.100000\nTraining Epoch: 15 [42496/50000]\tLoss: 3.3014\tLR: 0.100000\nTraining Epoch: 15 [42624/50000]\tLoss: 3.3224\tLR: 0.100000\nTraining Epoch: 15 [42752/50000]\tLoss: 3.2642\tLR: 0.100000\nTraining Epoch: 15 [42880/50000]\tLoss: 3.2909\tLR: 0.100000\nTraining Epoch: 15 [43008/50000]\tLoss: 3.1405\tLR: 0.100000\nTraining Epoch: 15 [43136/50000]\tLoss: 3.3160\tLR: 0.100000\nTraining Epoch: 15 [43264/50000]\tLoss: 3.2064\tLR: 0.100000\nTraining Epoch: 15 [43392/50000]\tLoss: 3.3900\tLR: 0.100000\nTraining Epoch: 15 [43520/50000]\tLoss: 3.6023\tLR: 0.100000\nTraining Epoch: 15 [43648/50000]\tLoss: 3.6503\tLR: 0.100000\nTraining Epoch: 15 [43776/50000]\tLoss: 3.4099\tLR: 0.100000\nTraining Epoch: 15 [43904/50000]\tLoss: 3.6746\tLR: 0.100000\nTraining Epoch: 15 [44032/50000]\tLoss: 3.4552\tLR: 0.100000\nTraining Epoch: 15 [44160/50000]\tLoss: 3.2488\tLR: 0.100000\nTraining Epoch: 15 [44288/50000]\tLoss: 3.4974\tLR: 0.100000\nTraining Epoch: 15 [44416/50000]\tLoss: 3.4671\tLR: 0.100000\nTraining Epoch: 15 [44544/50000]\tLoss: 3.2976\tLR: 0.100000\nTraining Epoch: 15 [44672/50000]\tLoss: 3.6067\tLR: 0.100000\nTraining Epoch: 15 [44800/50000]\tLoss: 3.5763\tLR: 0.100000\nTraining Epoch: 15 [44928/50000]\tLoss: 3.4863\tLR: 0.100000\nTraining Epoch: 15 [45056/50000]\tLoss: 3.4464\tLR: 0.100000\nTraining Epoch: 15 [45184/50000]\tLoss: 3.4573\tLR: 0.100000\nTraining Epoch: 15 [45312/50000]\tLoss: 3.5835\tLR: 0.100000\nTraining Epoch: 15 [45440/50000]\tLoss: 3.5783\tLR: 0.100000\nTraining Epoch: 15 [45568/50000]\tLoss: 3.5275\tLR: 0.100000\nTraining Epoch: 15 [45696/50000]\tLoss: 3.2999\tLR: 0.100000\nTraining Epoch: 15 [45824/50000]\tLoss: 3.4525\tLR: 0.100000\nTraining Epoch: 15 [45952/50000]\tLoss: 3.4546\tLR: 0.100000\nTraining Epoch: 15 [46080/50000]\tLoss: 3.5469\tLR: 0.100000\nTraining Epoch: 15 [46208/50000]\tLoss: 3.7076\tLR: 0.100000\nTraining Epoch: 15 [46336/50000]\tLoss: 3.2616\tLR: 0.100000\nTraining Epoch: 15 [46464/50000]\tLoss: 3.3334\tLR: 0.100000\nTraining Epoch: 15 [46592/50000]\tLoss: 3.4826\tLR: 0.100000\nTraining Epoch: 15 [46720/50000]\tLoss: 3.4410\tLR: 0.100000\nTraining Epoch: 15 [46848/50000]\tLoss: 3.4598\tLR: 0.100000\nTraining Epoch: 15 [46976/50000]\tLoss: 3.2737\tLR: 0.100000\nTraining Epoch: 15 [47104/50000]\tLoss: 3.2151\tLR: 0.100000\nTraining Epoch: 15 [47232/50000]\tLoss: 3.4301\tLR: 0.100000\nTraining Epoch: 15 [47360/50000]\tLoss: 3.3491\tLR: 0.100000\nTraining Epoch: 15 [47488/50000]\tLoss: 3.5057\tLR: 0.100000\nTraining Epoch: 15 [47616/50000]\tLoss: 3.6366\tLR: 0.100000\nTraining Epoch: 15 [47744/50000]\tLoss: 3.4012\tLR: 0.100000\nTraining Epoch: 15 [47872/50000]\tLoss: 3.3432\tLR: 0.100000\nTraining Epoch: 15 [48000/50000]\tLoss: 3.5160\tLR: 0.100000\nTraining Epoch: 15 [48128/50000]\tLoss: 3.2167\tLR: 0.100000\nTraining Epoch: 15 [48256/50000]\tLoss: 3.5004\tLR: 0.100000\nTraining Epoch: 15 [48384/50000]\tLoss: 3.4012\tLR: 0.100000\nTraining Epoch: 15 [48512/50000]\tLoss: 3.2701\tLR: 0.100000\nTraining Epoch: 15 [48640/50000]\tLoss: 3.3403\tLR: 0.100000\nTraining Epoch: 15 [48768/50000]\tLoss: 3.4499\tLR: 0.100000\nTraining Epoch: 15 [48896/50000]\tLoss: 3.4227\tLR: 0.100000\nTraining Epoch: 15 [49024/50000]\tLoss: 3.4814\tLR: 0.100000\nTraining Epoch: 15 [49152/50000]\tLoss: 3.2770\tLR: 0.100000\nTraining Epoch: 15 [49280/50000]\tLoss: 3.7016\tLR: 0.100000\nTraining Epoch: 15 [49408/50000]\tLoss: 3.3099\tLR: 0.100000\nTraining Epoch: 15 [49536/50000]\tLoss: 3.4707\tLR: 0.100000\nTraining Epoch: 15 [49664/50000]\tLoss: 3.4742\tLR: 0.100000\nTraining Epoch: 15 [49792/50000]\tLoss: 3.3498\tLR: 0.100000\nTraining Epoch: 15 [49920/50000]\tLoss: 3.2440\tLR: 0.100000\nTraining Epoch: 15 [50000/50000]\tLoss: 3.6226\tLR: 0.100000\n=========================================================================================\nTraining Epoch: 16 [128/50000]\tLoss: 3.5559\tLR: 0.100000\nTraining Epoch: 16 [256/50000]\tLoss: 3.5744\tLR: 0.100000\nTraining Epoch: 16 [384/50000]\tLoss: 3.3735\tLR: 0.100000\nTraining Epoch: 16 [512/50000]\tLoss: 3.7201\tLR: 0.100000\nTraining Epoch: 16 [640/50000]\tLoss: 3.5248\tLR: 0.100000\nTraining Epoch: 16 [768/50000]\tLoss: 3.5059\tLR: 0.100000\nTraining Epoch: 16 [896/50000]\tLoss: 3.4292\tLR: 0.100000\nTraining Epoch: 16 [1024/50000]\tLoss: 3.4776\tLR: 0.100000\nTraining Epoch: 16 [1152/50000]\tLoss: 3.4355\tLR: 0.100000\nTraining Epoch: 16 [1280/50000]\tLoss: 3.3092\tLR: 0.100000\nTraining Epoch: 16 [1408/50000]\tLoss: 3.2414\tLR: 0.100000\nTraining Epoch: 16 [1536/50000]\tLoss: 3.4937\tLR: 0.100000\nTraining Epoch: 16 [1664/50000]\tLoss: 3.6134\tLR: 0.100000\nTraining Epoch: 16 [1792/50000]\tLoss: 3.4524\tLR: 0.100000\nTraining Epoch: 16 [1920/50000]\tLoss: 3.6122\tLR: 0.100000\nTraining Epoch: 16 [2048/50000]\tLoss: 3.3836\tLR: 0.100000\nTraining Epoch: 16 [2176/50000]\tLoss: 3.4185\tLR: 0.100000\nTraining Epoch: 16 [2304/50000]\tLoss: 3.2601\tLR: 0.100000\nTraining Epoch: 16 [2432/50000]\tLoss: 3.3189\tLR: 0.100000\nTraining Epoch: 16 [2560/50000]\tLoss: 3.4032\tLR: 0.100000\nTraining Epoch: 16 [2688/50000]\tLoss: 3.4626\tLR: 0.100000\nTraining Epoch: 16 [2816/50000]\tLoss: 3.5788\tLR: 0.100000\nTraining Epoch: 16 [2944/50000]\tLoss: 3.4304\tLR: 0.100000\nTraining Epoch: 16 [3072/50000]\tLoss: 3.3809\tLR: 0.100000\nTraining Epoch: 16 [3200/50000]\tLoss: 3.4225\tLR: 0.100000\nTraining Epoch: 16 [3328/50000]\tLoss: 3.3733\tLR: 0.100000\nTraining Epoch: 16 [3456/50000]\tLoss: 3.5049\tLR: 0.100000\nTraining Epoch: 16 [3584/50000]\tLoss: 3.3351\tLR: 0.100000\nTraining Epoch: 16 [3712/50000]\tLoss: 3.1726\tLR: 0.100000\nTraining Epoch: 16 [3840/50000]\tLoss: 3.4802\tLR: 0.100000\nTraining Epoch: 16 [3968/50000]\tLoss: 3.1626\tLR: 0.100000\nTraining Epoch: 16 [4096/50000]\tLoss: 3.5968\tLR: 0.100000\nTraining Epoch: 16 [4224/50000]\tLoss: 3.3779\tLR: 0.100000\nTraining Epoch: 16 [4352/50000]\tLoss: 3.7589\tLR: 0.100000\nTraining Epoch: 16 [4480/50000]\tLoss: 3.3332\tLR: 0.100000\nTraining Epoch: 16 [4608/50000]\tLoss: 3.3144\tLR: 0.100000\nTraining Epoch: 16 [4736/50000]\tLoss: 3.5067\tLR: 0.100000\nTraining Epoch: 16 [4864/50000]\tLoss: 3.5652\tLR: 0.100000\nTraining Epoch: 16 [4992/50000]\tLoss: 3.4661\tLR: 0.100000\nTraining Epoch: 16 [5120/50000]\tLoss: 3.3786\tLR: 0.100000\nTraining Epoch: 16 [5248/50000]\tLoss: 3.5495\tLR: 0.100000\nTraining Epoch: 16 [5376/50000]\tLoss: 3.4449\tLR: 0.100000\nTraining Epoch: 16 [5504/50000]\tLoss: 3.4329\tLR: 0.100000\nTraining Epoch: 16 [5632/50000]\tLoss: 3.3878\tLR: 0.100000\n","name":"stdout"},{"output_type":"stream","text":"Training Epoch: 16 [5760/50000]\tLoss: 3.4014\tLR: 0.100000\nTraining Epoch: 16 [5888/50000]\tLoss: 3.4803\tLR: 0.100000\nTraining Epoch: 16 [6016/50000]\tLoss: 3.3929\tLR: 0.100000\nTraining Epoch: 16 [6144/50000]\tLoss: 3.2012\tLR: 0.100000\nTraining Epoch: 16 [6272/50000]\tLoss: 3.3914\tLR: 0.100000\nTraining Epoch: 16 [6400/50000]\tLoss: 3.4612\tLR: 0.100000\nTraining Epoch: 16 [6528/50000]\tLoss: 3.4139\tLR: 0.100000\nTraining Epoch: 16 [6656/50000]\tLoss: 3.1110\tLR: 0.100000\nTraining Epoch: 16 [6784/50000]\tLoss: 3.1657\tLR: 0.100000\nTraining Epoch: 16 [6912/50000]\tLoss: 3.6889\tLR: 0.100000\nTraining Epoch: 16 [7040/50000]\tLoss: 3.3619\tLR: 0.100000\nTraining Epoch: 16 [7168/50000]\tLoss: 3.1376\tLR: 0.100000\nTraining Epoch: 16 [7296/50000]\tLoss: 3.3275\tLR: 0.100000\nTraining Epoch: 16 [7424/50000]\tLoss: 3.4067\tLR: 0.100000\nTraining Epoch: 16 [7552/50000]\tLoss: 3.3550\tLR: 0.100000\nTraining Epoch: 16 [7680/50000]\tLoss: 3.5532\tLR: 0.100000\nTraining Epoch: 16 [7808/50000]\tLoss: 3.4919\tLR: 0.100000\nTraining Epoch: 16 [7936/50000]\tLoss: 3.3722\tLR: 0.100000\nTraining Epoch: 16 [8064/50000]\tLoss: 3.3727\tLR: 0.100000\nTraining Epoch: 16 [8192/50000]\tLoss: 3.6253\tLR: 0.100000\nTraining Epoch: 16 [8320/50000]\tLoss: 3.2423\tLR: 0.100000\nTraining Epoch: 16 [8448/50000]\tLoss: 3.2770\tLR: 0.100000\nTraining Epoch: 16 [8576/50000]\tLoss: 3.3078\tLR: 0.100000\nTraining Epoch: 16 [8704/50000]\tLoss: 3.2721\tLR: 0.100000\nTraining Epoch: 16 [8832/50000]\tLoss: 3.3563\tLR: 0.100000\nTraining Epoch: 16 [8960/50000]\tLoss: 3.3514\tLR: 0.100000\nTraining Epoch: 16 [9088/50000]\tLoss: 3.1873\tLR: 0.100000\nTraining Epoch: 16 [9216/50000]\tLoss: 3.4952\tLR: 0.100000\nTraining Epoch: 16 [9344/50000]\tLoss: 3.0444\tLR: 0.100000\nTraining Epoch: 16 [9472/50000]\tLoss: 3.3376\tLR: 0.100000\nTraining Epoch: 16 [9600/50000]\tLoss: 3.5216\tLR: 0.100000\nTraining Epoch: 16 [9728/50000]\tLoss: 3.1524\tLR: 0.100000\nTraining Epoch: 16 [9856/50000]\tLoss: 3.3476\tLR: 0.100000\nTraining Epoch: 16 [9984/50000]\tLoss: 3.4373\tLR: 0.100000\nTraining Epoch: 16 [10112/50000]\tLoss: 3.4157\tLR: 0.100000\nTraining Epoch: 16 [10240/50000]\tLoss: 3.4965\tLR: 0.100000\nTraining Epoch: 16 [10368/50000]\tLoss: 3.4520\tLR: 0.100000\nTraining Epoch: 16 [10496/50000]\tLoss: 3.3819\tLR: 0.100000\nTraining Epoch: 16 [10624/50000]\tLoss: 3.2667\tLR: 0.100000\nTraining Epoch: 16 [10752/50000]\tLoss: 3.3372\tLR: 0.100000\nTraining Epoch: 16 [10880/50000]\tLoss: 3.2536\tLR: 0.100000\nTraining Epoch: 16 [11008/50000]\tLoss: 3.5201\tLR: 0.100000\nTraining Epoch: 16 [11136/50000]\tLoss: 3.4117\tLR: 0.100000\nTraining Epoch: 16 [11264/50000]\tLoss: 3.3489\tLR: 0.100000\nTraining Epoch: 16 [11392/50000]\tLoss: 3.4008\tLR: 0.100000\nTraining Epoch: 16 [11520/50000]\tLoss: 3.4807\tLR: 0.100000\nTraining Epoch: 16 [11648/50000]\tLoss: 3.3510\tLR: 0.100000\nTraining Epoch: 16 [11776/50000]\tLoss: 3.3246\tLR: 0.100000\nTraining Epoch: 16 [11904/50000]\tLoss: 3.2038\tLR: 0.100000\nTraining Epoch: 16 [12032/50000]\tLoss: 3.3575\tLR: 0.100000\nTraining Epoch: 16 [12160/50000]\tLoss: 3.4214\tLR: 0.100000\nTraining Epoch: 16 [12288/50000]\tLoss: 3.4571\tLR: 0.100000\nTraining Epoch: 16 [12416/50000]\tLoss: 3.3681\tLR: 0.100000\nTraining Epoch: 16 [12544/50000]\tLoss: 3.5381\tLR: 0.100000\nTraining Epoch: 16 [12672/50000]\tLoss: 3.6397\tLR: 0.100000\nTraining Epoch: 16 [12800/50000]\tLoss: 3.3218\tLR: 0.100000\nTraining Epoch: 16 [12928/50000]\tLoss: 3.4229\tLR: 0.100000\nTraining Epoch: 16 [13056/50000]\tLoss: 3.4500\tLR: 0.100000\nTraining Epoch: 16 [13184/50000]\tLoss: 3.3464\tLR: 0.100000\nTraining Epoch: 16 [13312/50000]\tLoss: 3.5785\tLR: 0.100000\nTraining Epoch: 16 [13440/50000]\tLoss: 3.4334\tLR: 0.100000\nTraining Epoch: 16 [13568/50000]\tLoss: 3.4959\tLR: 0.100000\nTraining Epoch: 16 [13696/50000]\tLoss: 3.2172\tLR: 0.100000\nTraining Epoch: 16 [13824/50000]\tLoss: 3.2894\tLR: 0.100000\nTraining Epoch: 16 [13952/50000]\tLoss: 3.5015\tLR: 0.100000\nTraining Epoch: 16 [14080/50000]\tLoss: 3.1823\tLR: 0.100000\nTraining Epoch: 16 [14208/50000]\tLoss: 3.3111\tLR: 0.100000\nTraining Epoch: 16 [14336/50000]\tLoss: 3.2005\tLR: 0.100000\nTraining Epoch: 16 [14464/50000]\tLoss: 3.3550\tLR: 0.100000\nTraining Epoch: 16 [14592/50000]\tLoss: 3.4528\tLR: 0.100000\nTraining Epoch: 16 [14720/50000]\tLoss: 3.4468\tLR: 0.100000\nTraining Epoch: 16 [14848/50000]\tLoss: 3.3138\tLR: 0.100000\nTraining Epoch: 16 [14976/50000]\tLoss: 3.4334\tLR: 0.100000\nTraining Epoch: 16 [15104/50000]\tLoss: 3.4372\tLR: 0.100000\nTraining Epoch: 16 [15232/50000]\tLoss: 3.2794\tLR: 0.100000\nTraining Epoch: 16 [15360/50000]\tLoss: 3.2940\tLR: 0.100000\nTraining Epoch: 16 [15488/50000]\tLoss: 3.3557\tLR: 0.100000\nTraining Epoch: 16 [15616/50000]\tLoss: 3.4467\tLR: 0.100000\nTraining Epoch: 16 [15744/50000]\tLoss: 3.4335\tLR: 0.100000\nTraining Epoch: 16 [15872/50000]\tLoss: 3.6510\tLR: 0.100000\nTraining Epoch: 16 [16000/50000]\tLoss: 3.3659\tLR: 0.100000\nTraining Epoch: 16 [16128/50000]\tLoss: 3.6255\tLR: 0.100000\nTraining Epoch: 16 [16256/50000]\tLoss: 3.5455\tLR: 0.100000\nTraining Epoch: 16 [16384/50000]\tLoss: 3.2625\tLR: 0.100000\nTraining Epoch: 16 [16512/50000]\tLoss: 3.3870\tLR: 0.100000\nTraining Epoch: 16 [16640/50000]\tLoss: 3.3667\tLR: 0.100000\nTraining Epoch: 16 [16768/50000]\tLoss: 3.4617\tLR: 0.100000\nTraining Epoch: 16 [16896/50000]\tLoss: 3.5517\tLR: 0.100000\nTraining Epoch: 16 [17024/50000]\tLoss: 3.6179\tLR: 0.100000\nTraining Epoch: 16 [17152/50000]\tLoss: 3.4204\tLR: 0.100000\nTraining Epoch: 16 [17280/50000]\tLoss: 3.5673\tLR: 0.100000\nTraining Epoch: 16 [17408/50000]\tLoss: 3.2549\tLR: 0.100000\nTraining Epoch: 16 [17536/50000]\tLoss: 3.2705\tLR: 0.100000\nTraining Epoch: 16 [17664/50000]\tLoss: 3.4714\tLR: 0.100000\nTraining Epoch: 16 [17792/50000]\tLoss: 3.3583\tLR: 0.100000\nTraining Epoch: 16 [17920/50000]\tLoss: 3.4263\tLR: 0.100000\nTraining Epoch: 16 [18048/50000]\tLoss: 3.5297\tLR: 0.100000\nTraining Epoch: 16 [18176/50000]\tLoss: 3.5104\tLR: 0.100000\nTraining Epoch: 16 [18304/50000]\tLoss: 3.4168\tLR: 0.100000\nTraining Epoch: 16 [18432/50000]\tLoss: 3.2259\tLR: 0.100000\nTraining Epoch: 16 [18560/50000]\tLoss: 3.3907\tLR: 0.100000\nTraining Epoch: 16 [18688/50000]\tLoss: 3.3418\tLR: 0.100000\nTraining Epoch: 16 [18816/50000]\tLoss: 3.1707\tLR: 0.100000\nTraining Epoch: 16 [18944/50000]\tLoss: 3.2889\tLR: 0.100000\nTraining Epoch: 16 [19072/50000]\tLoss: 3.5696\tLR: 0.100000\nTraining Epoch: 16 [19200/50000]\tLoss: 3.2946\tLR: 0.100000\nTraining Epoch: 16 [19328/50000]\tLoss: 3.4511\tLR: 0.100000\nTraining Epoch: 16 [19456/50000]\tLoss: 3.3131\tLR: 0.100000\nTraining Epoch: 16 [19584/50000]\tLoss: 3.4514\tLR: 0.100000\nTraining Epoch: 16 [19712/50000]\tLoss: 3.1302\tLR: 0.100000\nTraining Epoch: 16 [19840/50000]\tLoss: 3.2291\tLR: 0.100000\nTraining Epoch: 16 [19968/50000]\tLoss: 3.3180\tLR: 0.100000\nTraining Epoch: 16 [20096/50000]\tLoss: 3.2121\tLR: 0.100000\nTraining Epoch: 16 [20224/50000]\tLoss: 3.3626\tLR: 0.100000\nTraining Epoch: 16 [20352/50000]\tLoss: 3.2201\tLR: 0.100000\nTraining Epoch: 16 [20480/50000]\tLoss: 3.3564\tLR: 0.100000\nTraining Epoch: 16 [20608/50000]\tLoss: 3.3382\tLR: 0.100000\nTraining Epoch: 16 [20736/50000]\tLoss: 3.2756\tLR: 0.100000\nTraining Epoch: 16 [20864/50000]\tLoss: 3.2100\tLR: 0.100000\nTraining Epoch: 16 [20992/50000]\tLoss: 3.5412\tLR: 0.100000\nTraining Epoch: 16 [21120/50000]\tLoss: 3.5348\tLR: 0.100000\nTraining Epoch: 16 [21248/50000]\tLoss: 3.1433\tLR: 0.100000\nTraining Epoch: 16 [21376/50000]\tLoss: 3.2767\tLR: 0.100000\nTraining Epoch: 16 [21504/50000]\tLoss: 3.3883\tLR: 0.100000\nTraining Epoch: 16 [21632/50000]\tLoss: 3.4459\tLR: 0.100000\nTraining Epoch: 16 [21760/50000]\tLoss: 3.1870\tLR: 0.100000\nTraining Epoch: 16 [21888/50000]\tLoss: 3.5741\tLR: 0.100000\nTraining Epoch: 16 [22016/50000]\tLoss: 3.2524\tLR: 0.100000\nTraining Epoch: 16 [22144/50000]\tLoss: 3.3726\tLR: 0.100000\nTraining Epoch: 16 [22272/50000]\tLoss: 3.4310\tLR: 0.100000\nTraining Epoch: 16 [22400/50000]\tLoss: 3.5648\tLR: 0.100000\nTraining Epoch: 16 [22528/50000]\tLoss: 3.4535\tLR: 0.100000\nTraining Epoch: 16 [22656/50000]\tLoss: 3.2922\tLR: 0.100000\nTraining Epoch: 16 [22784/50000]\tLoss: 3.2544\tLR: 0.100000\nTraining Epoch: 16 [22912/50000]\tLoss: 3.4741\tLR: 0.100000\nTraining Epoch: 16 [23040/50000]\tLoss: 3.3139\tLR: 0.100000\nTraining Epoch: 16 [23168/50000]\tLoss: 3.4235\tLR: 0.100000\nTraining Epoch: 16 [23296/50000]\tLoss: 3.4571\tLR: 0.100000\nTraining Epoch: 16 [23424/50000]\tLoss: 3.1212\tLR: 0.100000\nTraining Epoch: 16 [23552/50000]\tLoss: 3.4186\tLR: 0.100000\nTraining Epoch: 16 [23680/50000]\tLoss: 3.3203\tLR: 0.100000\n","name":"stdout"},{"output_type":"stream","text":"Training Epoch: 16 [23808/50000]\tLoss: 3.2889\tLR: 0.100000\nTraining Epoch: 16 [23936/50000]\tLoss: 3.3307\tLR: 0.100000\nTraining Epoch: 16 [24064/50000]\tLoss: 3.3624\tLR: 0.100000\nTraining Epoch: 16 [24192/50000]\tLoss: 3.2918\tLR: 0.100000\nTraining Epoch: 16 [24320/50000]\tLoss: 3.5194\tLR: 0.100000\nTraining Epoch: 16 [24448/50000]\tLoss: 3.4905\tLR: 0.100000\nTraining Epoch: 16 [24576/50000]\tLoss: 3.1210\tLR: 0.100000\nTraining Epoch: 16 [24704/50000]\tLoss: 3.3362\tLR: 0.100000\nTraining Epoch: 16 [24832/50000]\tLoss: 3.3292\tLR: 0.100000\nTraining Epoch: 16 [24960/50000]\tLoss: 3.4472\tLR: 0.100000\nTraining Epoch: 16 [25088/50000]\tLoss: 3.4862\tLR: 0.100000\nTraining Epoch: 16 [25216/50000]\tLoss: 3.2442\tLR: 0.100000\nTraining Epoch: 16 [25344/50000]\tLoss: 3.2068\tLR: 0.100000\nTraining Epoch: 16 [25472/50000]\tLoss: 3.3967\tLR: 0.100000\nTraining Epoch: 16 [25600/50000]\tLoss: 3.2136\tLR: 0.100000\nTraining Epoch: 16 [25728/50000]\tLoss: 3.3272\tLR: 0.100000\nTraining Epoch: 16 [25856/50000]\tLoss: 3.2208\tLR: 0.100000\nTraining Epoch: 16 [25984/50000]\tLoss: 3.2757\tLR: 0.100000\nTraining Epoch: 16 [26112/50000]\tLoss: 3.5582\tLR: 0.100000\nTraining Epoch: 16 [26240/50000]\tLoss: 3.5227\tLR: 0.100000\nTraining Epoch: 16 [26368/50000]\tLoss: 3.4988\tLR: 0.100000\nTraining Epoch: 16 [26496/50000]\tLoss: 3.6639\tLR: 0.100000\nTraining Epoch: 16 [26624/50000]\tLoss: 3.4248\tLR: 0.100000\nTraining Epoch: 16 [26752/50000]\tLoss: 3.4141\tLR: 0.100000\nTraining Epoch: 16 [26880/50000]\tLoss: 3.3203\tLR: 0.100000\nTraining Epoch: 16 [27008/50000]\tLoss: 3.3899\tLR: 0.100000\nTraining Epoch: 16 [27136/50000]\tLoss: 3.4100\tLR: 0.100000\nTraining Epoch: 16 [27264/50000]\tLoss: 3.2217\tLR: 0.100000\nTraining Epoch: 16 [27392/50000]\tLoss: 3.3533\tLR: 0.100000\nTraining Epoch: 16 [27520/50000]\tLoss: 3.3002\tLR: 0.100000\nTraining Epoch: 16 [27648/50000]\tLoss: 3.6572\tLR: 0.100000\nTraining Epoch: 16 [27776/50000]\tLoss: 3.4977\tLR: 0.100000\nTraining Epoch: 16 [27904/50000]\tLoss: 3.2404\tLR: 0.100000\nTraining Epoch: 16 [28032/50000]\tLoss: 3.5044\tLR: 0.100000\nTraining Epoch: 16 [28160/50000]\tLoss: 3.1951\tLR: 0.100000\nTraining Epoch: 16 [28288/50000]\tLoss: 3.3633\tLR: 0.100000\nTraining Epoch: 16 [28416/50000]\tLoss: 3.2773\tLR: 0.100000\nTraining Epoch: 16 [28544/50000]\tLoss: 3.4510\tLR: 0.100000\nTraining Epoch: 16 [28672/50000]\tLoss: 3.5088\tLR: 0.100000\nTraining Epoch: 16 [28800/50000]\tLoss: 3.3592\tLR: 0.100000\nTraining Epoch: 16 [28928/50000]\tLoss: 3.4639\tLR: 0.100000\nTraining Epoch: 16 [29056/50000]\tLoss: 3.3509\tLR: 0.100000\nTraining Epoch: 16 [29184/50000]\tLoss: 3.4825\tLR: 0.100000\nTraining Epoch: 16 [29312/50000]\tLoss: 3.1519\tLR: 0.100000\nTraining Epoch: 16 [29440/50000]\tLoss: 3.0085\tLR: 0.100000\nTraining Epoch: 16 [29568/50000]\tLoss: 3.0781\tLR: 0.100000\nTraining Epoch: 16 [29696/50000]\tLoss: 3.3399\tLR: 0.100000\nTraining Epoch: 16 [29824/50000]\tLoss: 3.5825\tLR: 0.100000\nTraining Epoch: 16 [29952/50000]\tLoss: 3.1223\tLR: 0.100000\nTraining Epoch: 16 [30080/50000]\tLoss: 3.3718\tLR: 0.100000\nTraining Epoch: 16 [30208/50000]\tLoss: 3.2099\tLR: 0.100000\nTraining Epoch: 16 [30336/50000]\tLoss: 3.4465\tLR: 0.100000\nTraining Epoch: 16 [30464/50000]\tLoss: 3.1561\tLR: 0.100000\nTraining Epoch: 16 [30592/50000]\tLoss: 3.2522\tLR: 0.100000\nTraining Epoch: 16 [30720/50000]\tLoss: 3.4958\tLR: 0.100000\nTraining Epoch: 16 [30848/50000]\tLoss: 3.0658\tLR: 0.100000\nTraining Epoch: 16 [30976/50000]\tLoss: 3.2130\tLR: 0.100000\nTraining Epoch: 16 [31104/50000]\tLoss: 3.2174\tLR: 0.100000\nTraining Epoch: 16 [31232/50000]\tLoss: 3.4154\tLR: 0.100000\nTraining Epoch: 16 [31360/50000]\tLoss: 3.3677\tLR: 0.100000\nTraining Epoch: 16 [31488/50000]\tLoss: 3.6945\tLR: 0.100000\nTraining Epoch: 16 [31616/50000]\tLoss: 3.5344\tLR: 0.100000\nTraining Epoch: 16 [31744/50000]\tLoss: 3.4861\tLR: 0.100000\nTraining Epoch: 16 [31872/50000]\tLoss: 3.3265\tLR: 0.100000\nTraining Epoch: 16 [32000/50000]\tLoss: 3.0925\tLR: 0.100000\nTraining Epoch: 16 [32128/50000]\tLoss: 3.1895\tLR: 0.100000\nTraining Epoch: 16 [32256/50000]\tLoss: 3.4203\tLR: 0.100000\nTraining Epoch: 16 [32384/50000]\tLoss: 3.5539\tLR: 0.100000\nTraining Epoch: 16 [32512/50000]\tLoss: 3.4640\tLR: 0.100000\nTraining Epoch: 16 [32640/50000]\tLoss: 3.3305\tLR: 0.100000\nTraining Epoch: 16 [32768/50000]\tLoss: 3.3435\tLR: 0.100000\nTraining Epoch: 16 [32896/50000]\tLoss: 3.4623\tLR: 0.100000\nTraining Epoch: 16 [33024/50000]\tLoss: 3.5821\tLR: 0.100000\nTraining Epoch: 16 [33152/50000]\tLoss: 3.3275\tLR: 0.100000\nTraining Epoch: 16 [33280/50000]\tLoss: 3.4350\tLR: 0.100000\nTraining Epoch: 16 [33408/50000]\tLoss: 3.3548\tLR: 0.100000\nTraining Epoch: 16 [33536/50000]\tLoss: 3.5417\tLR: 0.100000\nTraining Epoch: 16 [33664/50000]\tLoss: 3.7021\tLR: 0.100000\nTraining Epoch: 16 [33792/50000]\tLoss: 3.3797\tLR: 0.100000\nTraining Epoch: 16 [33920/50000]\tLoss: 3.7079\tLR: 0.100000\nTraining Epoch: 16 [34048/50000]\tLoss: 3.3112\tLR: 0.100000\nTraining Epoch: 16 [34176/50000]\tLoss: 3.3624\tLR: 0.100000\nTraining Epoch: 16 [34304/50000]\tLoss: 3.2525\tLR: 0.100000\nTraining Epoch: 16 [34432/50000]\tLoss: 3.3960\tLR: 0.100000\nTraining Epoch: 16 [34560/50000]\tLoss: 3.1949\tLR: 0.100000\nTraining Epoch: 16 [34688/50000]\tLoss: 3.2938\tLR: 0.100000\nTraining Epoch: 16 [34816/50000]\tLoss: 3.2871\tLR: 0.100000\nTraining Epoch: 16 [34944/50000]\tLoss: 3.3987\tLR: 0.100000\nTraining Epoch: 16 [35072/50000]\tLoss: 3.6204\tLR: 0.100000\nTraining Epoch: 16 [35200/50000]\tLoss: 3.5313\tLR: 0.100000\nTraining Epoch: 16 [35328/50000]\tLoss: 3.6111\tLR: 0.100000\nTraining Epoch: 16 [35456/50000]\tLoss: 3.7568\tLR: 0.100000\nTraining Epoch: 16 [35584/50000]\tLoss: 3.3960\tLR: 0.100000\nTraining Epoch: 16 [35712/50000]\tLoss: 3.6036\tLR: 0.100000\nTraining Epoch: 16 [35840/50000]\tLoss: 3.2833\tLR: 0.100000\nTraining Epoch: 16 [35968/50000]\tLoss: 3.3340\tLR: 0.100000\nTraining Epoch: 16 [36096/50000]\tLoss: 3.3327\tLR: 0.100000\nTraining Epoch: 16 [36224/50000]\tLoss: 3.3034\tLR: 0.100000\nTraining Epoch: 16 [36352/50000]\tLoss: 3.2949\tLR: 0.100000\nTraining Epoch: 16 [36480/50000]\tLoss: 3.4074\tLR: 0.100000\nTraining Epoch: 16 [36608/50000]\tLoss: 3.7385\tLR: 0.100000\nTraining Epoch: 16 [36736/50000]\tLoss: 3.5195\tLR: 0.100000\nTraining Epoch: 16 [36864/50000]\tLoss: 3.4072\tLR: 0.100000\nTraining Epoch: 16 [36992/50000]\tLoss: 3.4392\tLR: 0.100000\nTraining Epoch: 16 [37120/50000]\tLoss: 3.2013\tLR: 0.100000\nTraining Epoch: 16 [37248/50000]\tLoss: 3.4227\tLR: 0.100000\nTraining Epoch: 16 [37376/50000]\tLoss: 3.4052\tLR: 0.100000\nTraining Epoch: 16 [37504/50000]\tLoss: 3.3584\tLR: 0.100000\nTraining Epoch: 16 [37632/50000]\tLoss: 3.3060\tLR: 0.100000\nTraining Epoch: 16 [37760/50000]\tLoss: 3.4445\tLR: 0.100000\nTraining Epoch: 16 [37888/50000]\tLoss: 3.2854\tLR: 0.100000\nTraining Epoch: 16 [38016/50000]\tLoss: 3.1253\tLR: 0.100000\nTraining Epoch: 16 [38144/50000]\tLoss: 3.2385\tLR: 0.100000\nTraining Epoch: 16 [38272/50000]\tLoss: 3.4179\tLR: 0.100000\nTraining Epoch: 16 [38400/50000]\tLoss: 3.2538\tLR: 0.100000\nTraining Epoch: 16 [38528/50000]\tLoss: 3.4496\tLR: 0.100000\nTraining Epoch: 16 [38656/50000]\tLoss: 3.4743\tLR: 0.100000\nTraining Epoch: 16 [38784/50000]\tLoss: 3.4460\tLR: 0.100000\nTraining Epoch: 16 [38912/50000]\tLoss: 3.1238\tLR: 0.100000\nTraining Epoch: 16 [39040/50000]\tLoss: 3.4382\tLR: 0.100000\nTraining Epoch: 16 [39168/50000]\tLoss: 3.4478\tLR: 0.100000\nTraining Epoch: 16 [39296/50000]\tLoss: 3.2417\tLR: 0.100000\nTraining Epoch: 16 [39424/50000]\tLoss: 3.3808\tLR: 0.100000\nTraining Epoch: 16 [39552/50000]\tLoss: 3.4031\tLR: 0.100000\nTraining Epoch: 16 [39680/50000]\tLoss: 3.2801\tLR: 0.100000\nTraining Epoch: 16 [39808/50000]\tLoss: 3.2671\tLR: 0.100000\nTraining Epoch: 16 [39936/50000]\tLoss: 3.3300\tLR: 0.100000\nTraining Epoch: 16 [40064/50000]\tLoss: 3.3945\tLR: 0.100000\nTraining Epoch: 16 [40192/50000]\tLoss: 3.6822\tLR: 0.100000\nTraining Epoch: 16 [40320/50000]\tLoss: 3.3718\tLR: 0.100000\nTraining Epoch: 16 [40448/50000]\tLoss: 3.4005\tLR: 0.100000\nTraining Epoch: 16 [40576/50000]\tLoss: 3.3079\tLR: 0.100000\nTraining Epoch: 16 [40704/50000]\tLoss: 3.1531\tLR: 0.100000\nTraining Epoch: 16 [40832/50000]\tLoss: 3.3927\tLR: 0.100000\nTraining Epoch: 16 [40960/50000]\tLoss: 3.3515\tLR: 0.100000\nTraining Epoch: 16 [41088/50000]\tLoss: 3.5105\tLR: 0.100000\nTraining Epoch: 16 [41216/50000]\tLoss: 3.5554\tLR: 0.100000\nTraining Epoch: 16 [41344/50000]\tLoss: 3.3480\tLR: 0.100000\nTraining Epoch: 16 [41472/50000]\tLoss: 3.5139\tLR: 0.100000\nTraining Epoch: 16 [41600/50000]\tLoss: 3.1972\tLR: 0.100000\nTraining Epoch: 16 [41728/50000]\tLoss: 3.3863\tLR: 0.100000\n","name":"stdout"},{"output_type":"stream","text":"Training Epoch: 16 [41856/50000]\tLoss: 3.3774\tLR: 0.100000\nTraining Epoch: 16 [41984/50000]\tLoss: 3.1064\tLR: 0.100000\nTraining Epoch: 16 [42112/50000]\tLoss: 3.2942\tLR: 0.100000\nTraining Epoch: 16 [42240/50000]\tLoss: 3.3883\tLR: 0.100000\nTraining Epoch: 16 [42368/50000]\tLoss: 3.3096\tLR: 0.100000\nTraining Epoch: 16 [42496/50000]\tLoss: 3.2673\tLR: 0.100000\nTraining Epoch: 16 [42624/50000]\tLoss: 3.3755\tLR: 0.100000\nTraining Epoch: 16 [42752/50000]\tLoss: 3.4738\tLR: 0.100000\nTraining Epoch: 16 [42880/50000]\tLoss: 3.3487\tLR: 0.100000\nTraining Epoch: 16 [43008/50000]\tLoss: 3.3855\tLR: 0.100000\nTraining Epoch: 16 [43136/50000]\tLoss: 3.4572\tLR: 0.100000\nTraining Epoch: 16 [43264/50000]\tLoss: 3.2383\tLR: 0.100000\nTraining Epoch: 16 [43392/50000]\tLoss: 3.4758\tLR: 0.100000\nTraining Epoch: 16 [43520/50000]\tLoss: 3.1268\tLR: 0.100000\nTraining Epoch: 16 [43648/50000]\tLoss: 3.2249\tLR: 0.100000\nTraining Epoch: 16 [43776/50000]\tLoss: 3.2765\tLR: 0.100000\nTraining Epoch: 16 [43904/50000]\tLoss: 3.2911\tLR: 0.100000\nTraining Epoch: 16 [44032/50000]\tLoss: 3.5813\tLR: 0.100000\nTraining Epoch: 16 [44160/50000]\tLoss: 3.5516\tLR: 0.100000\nTraining Epoch: 16 [44288/50000]\tLoss: 3.2417\tLR: 0.100000\nTraining Epoch: 16 [44416/50000]\tLoss: 3.4717\tLR: 0.100000\nTraining Epoch: 16 [44544/50000]\tLoss: 3.2901\tLR: 0.100000\nTraining Epoch: 16 [44672/50000]\tLoss: 3.4242\tLR: 0.100000\nTraining Epoch: 16 [44800/50000]\tLoss: 3.6785\tLR: 0.100000\nTraining Epoch: 16 [44928/50000]\tLoss: 3.3211\tLR: 0.100000\nTraining Epoch: 16 [45056/50000]\tLoss: 3.4951\tLR: 0.100000\nTraining Epoch: 16 [45184/50000]\tLoss: 3.1022\tLR: 0.100000\nTraining Epoch: 16 [45312/50000]\tLoss: 3.3090\tLR: 0.100000\nTraining Epoch: 16 [45440/50000]\tLoss: 3.2856\tLR: 0.100000\nTraining Epoch: 16 [45568/50000]\tLoss: 3.5561\tLR: 0.100000\nTraining Epoch: 16 [45696/50000]\tLoss: 3.2676\tLR: 0.100000\nTraining Epoch: 16 [45824/50000]\tLoss: 3.5550\tLR: 0.100000\nTraining Epoch: 16 [45952/50000]\tLoss: 3.6116\tLR: 0.100000\nTraining Epoch: 16 [46080/50000]\tLoss: 3.3653\tLR: 0.100000\nTraining Epoch: 16 [46208/50000]\tLoss: 3.5040\tLR: 0.100000\nTraining Epoch: 16 [46336/50000]\tLoss: 3.5533\tLR: 0.100000\nTraining Epoch: 16 [46464/50000]\tLoss: 3.4479\tLR: 0.100000\nTraining Epoch: 16 [46592/50000]\tLoss: 3.3993\tLR: 0.100000\nTraining Epoch: 16 [46720/50000]\tLoss: 3.4078\tLR: 0.100000\nTraining Epoch: 16 [46848/50000]\tLoss: 3.2864\tLR: 0.100000\nTraining Epoch: 16 [46976/50000]\tLoss: 3.2116\tLR: 0.100000\nTraining Epoch: 16 [47104/50000]\tLoss: 3.3290\tLR: 0.100000\nTraining Epoch: 16 [47232/50000]\tLoss: 3.4245\tLR: 0.100000\nTraining Epoch: 16 [47360/50000]\tLoss: 3.4042\tLR: 0.100000\nTraining Epoch: 16 [47488/50000]\tLoss: 3.3620\tLR: 0.100000\nTraining Epoch: 16 [47616/50000]\tLoss: 3.4249\tLR: 0.100000\nTraining Epoch: 16 [47744/50000]\tLoss: 3.4535\tLR: 0.100000\nTraining Epoch: 16 [47872/50000]\tLoss: 3.2333\tLR: 0.100000\nTraining Epoch: 16 [48000/50000]\tLoss: 3.2465\tLR: 0.100000\nTraining Epoch: 16 [48128/50000]\tLoss: 3.4948\tLR: 0.100000\nTraining Epoch: 16 [48256/50000]\tLoss: 3.3707\tLR: 0.100000\nTraining Epoch: 16 [48384/50000]\tLoss: 3.4572\tLR: 0.100000\nTraining Epoch: 16 [48512/50000]\tLoss: 3.1300\tLR: 0.100000\nTraining Epoch: 16 [48640/50000]\tLoss: 3.5746\tLR: 0.100000\nTraining Epoch: 16 [48768/50000]\tLoss: 3.3382\tLR: 0.100000\nTraining Epoch: 16 [48896/50000]\tLoss: 3.3469\tLR: 0.100000\nTraining Epoch: 16 [49024/50000]\tLoss: 3.3243\tLR: 0.100000\nTraining Epoch: 16 [49152/50000]\tLoss: 3.6055\tLR: 0.100000\nTraining Epoch: 16 [49280/50000]\tLoss: 3.2371\tLR: 0.100000\nTraining Epoch: 16 [49408/50000]\tLoss: 3.2305\tLR: 0.100000\nTraining Epoch: 16 [49536/50000]\tLoss: 3.2116\tLR: 0.100000\nTraining Epoch: 16 [49664/50000]\tLoss: 3.3902\tLR: 0.100000\nTraining Epoch: 16 [49792/50000]\tLoss: 3.3616\tLR: 0.100000\nTraining Epoch: 16 [49920/50000]\tLoss: 3.2460\tLR: 0.100000\nTraining Epoch: 16 [50000/50000]\tLoss: 3.3605\tLR: 0.100000\n=========================================================================================\nTraining Epoch: 17 [128/50000]\tLoss: 3.1792\tLR: 0.100000\nTraining Epoch: 17 [256/50000]\tLoss: 3.3489\tLR: 0.100000\nTraining Epoch: 17 [384/50000]\tLoss: 3.3222\tLR: 0.100000\nTraining Epoch: 17 [512/50000]\tLoss: 3.2912\tLR: 0.100000\nTraining Epoch: 17 [640/50000]\tLoss: 3.5118\tLR: 0.100000\nTraining Epoch: 17 [768/50000]\tLoss: 3.1854\tLR: 0.100000\nTraining Epoch: 17 [896/50000]\tLoss: 3.6600\tLR: 0.100000\nTraining Epoch: 17 [1024/50000]\tLoss: 3.4387\tLR: 0.100000\nTraining Epoch: 17 [1152/50000]\tLoss: 3.3074\tLR: 0.100000\nTraining Epoch: 17 [1280/50000]\tLoss: 3.3092\tLR: 0.100000\nTraining Epoch: 17 [1408/50000]\tLoss: 3.5327\tLR: 0.100000\nTraining Epoch: 17 [1536/50000]\tLoss: 3.3935\tLR: 0.100000\nTraining Epoch: 17 [1664/50000]\tLoss: 3.3985\tLR: 0.100000\nTraining Epoch: 17 [1792/50000]\tLoss: 3.4596\tLR: 0.100000\nTraining Epoch: 17 [1920/50000]\tLoss: 3.4102\tLR: 0.100000\nTraining Epoch: 17 [2048/50000]\tLoss: 3.3650\tLR: 0.100000\nTraining Epoch: 17 [2176/50000]\tLoss: 3.1231\tLR: 0.100000\nTraining Epoch: 17 [2304/50000]\tLoss: 3.4888\tLR: 0.100000\nTraining Epoch: 17 [2432/50000]\tLoss: 3.6079\tLR: 0.100000\nTraining Epoch: 17 [2560/50000]\tLoss: 3.5726\tLR: 0.100000\nTraining Epoch: 17 [2688/50000]\tLoss: 3.2670\tLR: 0.100000\nTraining Epoch: 17 [2816/50000]\tLoss: 3.2567\tLR: 0.100000\nTraining Epoch: 17 [2944/50000]\tLoss: 3.3591\tLR: 0.100000\nTraining Epoch: 17 [3072/50000]\tLoss: 3.3808\tLR: 0.100000\nTraining Epoch: 17 [3200/50000]\tLoss: 3.3233\tLR: 0.100000\nTraining Epoch: 17 [3328/50000]\tLoss: 3.4080\tLR: 0.100000\nTraining Epoch: 17 [3456/50000]\tLoss: 3.2902\tLR: 0.100000\nTraining Epoch: 17 [3584/50000]\tLoss: 3.2279\tLR: 0.100000\nTraining Epoch: 17 [3712/50000]\tLoss: 3.3464\tLR: 0.100000\nTraining Epoch: 17 [3840/50000]\tLoss: 3.2694\tLR: 0.100000\nTraining Epoch: 17 [3968/50000]\tLoss: 3.3713\tLR: 0.100000\nTraining Epoch: 17 [4096/50000]\tLoss: 3.4544\tLR: 0.100000\nTraining Epoch: 17 [4224/50000]\tLoss: 3.2294\tLR: 0.100000\nTraining Epoch: 17 [4352/50000]\tLoss: 3.3919\tLR: 0.100000\nTraining Epoch: 17 [4480/50000]\tLoss: 3.2934\tLR: 0.100000\nTraining Epoch: 17 [4608/50000]\tLoss: 3.4957\tLR: 0.100000\nTraining Epoch: 17 [4736/50000]\tLoss: 3.3876\tLR: 0.100000\nTraining Epoch: 17 [4864/50000]\tLoss: 3.2420\tLR: 0.100000\nTraining Epoch: 17 [4992/50000]\tLoss: 3.2637\tLR: 0.100000\nTraining Epoch: 17 [5120/50000]\tLoss: 3.3814\tLR: 0.100000\nTraining Epoch: 17 [5248/50000]\tLoss: 3.1821\tLR: 0.100000\nTraining Epoch: 17 [5376/50000]\tLoss: 3.1865\tLR: 0.100000\nTraining Epoch: 17 [5504/50000]\tLoss: 3.6048\tLR: 0.100000\nTraining Epoch: 17 [5632/50000]\tLoss: 3.6966\tLR: 0.100000\nTraining Epoch: 17 [5760/50000]\tLoss: 3.4422\tLR: 0.100000\nTraining Epoch: 17 [5888/50000]\tLoss: 3.2636\tLR: 0.100000\nTraining Epoch: 17 [6016/50000]\tLoss: 3.3046\tLR: 0.100000\nTraining Epoch: 17 [6144/50000]\tLoss: 3.2679\tLR: 0.100000\nTraining Epoch: 17 [6272/50000]\tLoss: 3.3611\tLR: 0.100000\nTraining Epoch: 17 [6400/50000]\tLoss: 3.2210\tLR: 0.100000\nTraining Epoch: 17 [6528/50000]\tLoss: 3.2196\tLR: 0.100000\nTraining Epoch: 17 [6656/50000]\tLoss: 3.2555\tLR: 0.100000\nTraining Epoch: 17 [6784/50000]\tLoss: 3.5872\tLR: 0.100000\nTraining Epoch: 17 [6912/50000]\tLoss: 3.4005\tLR: 0.100000\nTraining Epoch: 17 [7040/50000]\tLoss: 3.2914\tLR: 0.100000\nTraining Epoch: 17 [7168/50000]\tLoss: 3.4348\tLR: 0.100000\nTraining Epoch: 17 [7296/50000]\tLoss: 3.3751\tLR: 0.100000\nTraining Epoch: 17 [7424/50000]\tLoss: 3.3128\tLR: 0.100000\nTraining Epoch: 17 [7552/50000]\tLoss: 3.6535\tLR: 0.100000\nTraining Epoch: 17 [7680/50000]\tLoss: 3.2612\tLR: 0.100000\nTraining Epoch: 17 [7808/50000]\tLoss: 3.3668\tLR: 0.100000\nTraining Epoch: 17 [7936/50000]\tLoss: 3.5222\tLR: 0.100000\nTraining Epoch: 17 [8064/50000]\tLoss: 3.2555\tLR: 0.100000\nTraining Epoch: 17 [8192/50000]\tLoss: 3.4584\tLR: 0.100000\nTraining Epoch: 17 [8320/50000]\tLoss: 3.2475\tLR: 0.100000\nTraining Epoch: 17 [8448/50000]\tLoss: 3.3373\tLR: 0.100000\nTraining Epoch: 17 [8576/50000]\tLoss: 3.3219\tLR: 0.100000\nTraining Epoch: 17 [8704/50000]\tLoss: 3.2429\tLR: 0.100000\nTraining Epoch: 17 [8832/50000]\tLoss: 3.1250\tLR: 0.100000\nTraining Epoch: 17 [8960/50000]\tLoss: 3.2762\tLR: 0.100000\nTraining Epoch: 17 [9088/50000]\tLoss: 3.1941\tLR: 0.100000\nTraining Epoch: 17 [9216/50000]\tLoss: 3.3592\tLR: 0.100000\nTraining Epoch: 17 [9344/50000]\tLoss: 3.4236\tLR: 0.100000\nTraining Epoch: 17 [9472/50000]\tLoss: 3.4845\tLR: 0.100000\nTraining Epoch: 17 [9600/50000]\tLoss: 3.1726\tLR: 0.100000\nTraining Epoch: 17 [9728/50000]\tLoss: 3.3653\tLR: 0.100000\n","name":"stdout"},{"output_type":"stream","text":"Training Epoch: 17 [9856/50000]\tLoss: 3.4209\tLR: 0.100000\nTraining Epoch: 17 [9984/50000]\tLoss: 3.2192\tLR: 0.100000\nTraining Epoch: 17 [10112/50000]\tLoss: 3.2168\tLR: 0.100000\nTraining Epoch: 17 [10240/50000]\tLoss: 3.3305\tLR: 0.100000\nTraining Epoch: 17 [10368/50000]\tLoss: 3.3050\tLR: 0.100000\nTraining Epoch: 17 [10496/50000]\tLoss: 3.5012\tLR: 0.100000\nTraining Epoch: 17 [10624/50000]\tLoss: 3.3584\tLR: 0.100000\nTraining Epoch: 17 [10752/50000]\tLoss: 3.4435\tLR: 0.100000\nTraining Epoch: 17 [10880/50000]\tLoss: 3.5612\tLR: 0.100000\nTraining Epoch: 17 [11008/50000]\tLoss: 3.3956\tLR: 0.100000\nTraining Epoch: 17 [11136/50000]\tLoss: 3.3311\tLR: 0.100000\nTraining Epoch: 17 [11264/50000]\tLoss: 3.3046\tLR: 0.100000\nTraining Epoch: 17 [11392/50000]\tLoss: 3.2035\tLR: 0.100000\nTraining Epoch: 17 [11520/50000]\tLoss: 3.5713\tLR: 0.100000\nTraining Epoch: 17 [11648/50000]\tLoss: 3.3136\tLR: 0.100000\nTraining Epoch: 17 [11776/50000]\tLoss: 3.5123\tLR: 0.100000\nTraining Epoch: 17 [11904/50000]\tLoss: 3.2867\tLR: 0.100000\nTraining Epoch: 17 [12032/50000]\tLoss: 3.3136\tLR: 0.100000\nTraining Epoch: 17 [12160/50000]\tLoss: 3.2020\tLR: 0.100000\nTraining Epoch: 17 [12288/50000]\tLoss: 3.2514\tLR: 0.100000\nTraining Epoch: 17 [12416/50000]\tLoss: 3.3524\tLR: 0.100000\nTraining Epoch: 17 [12544/50000]\tLoss: 3.4317\tLR: 0.100000\nTraining Epoch: 17 [12672/50000]\tLoss: 3.4329\tLR: 0.100000\nTraining Epoch: 17 [12800/50000]\tLoss: 3.2029\tLR: 0.100000\nTraining Epoch: 17 [12928/50000]\tLoss: 3.5350\tLR: 0.100000\nTraining Epoch: 17 [13056/50000]\tLoss: 3.3321\tLR: 0.100000\nTraining Epoch: 17 [13184/50000]\tLoss: 3.4804\tLR: 0.100000\nTraining Epoch: 17 [13312/50000]\tLoss: 3.5574\tLR: 0.100000\nTraining Epoch: 17 [13440/50000]\tLoss: 3.3861\tLR: 0.100000\nTraining Epoch: 17 [13568/50000]\tLoss: 3.4475\tLR: 0.100000\nTraining Epoch: 17 [13696/50000]\tLoss: 3.4018\tLR: 0.100000\nTraining Epoch: 17 [13824/50000]\tLoss: 3.4262\tLR: 0.100000\nTraining Epoch: 17 [13952/50000]\tLoss: 3.7310\tLR: 0.100000\nTraining Epoch: 17 [14080/50000]\tLoss: 3.4794\tLR: 0.100000\nTraining Epoch: 17 [14208/50000]\tLoss: 3.4509\tLR: 0.100000\nTraining Epoch: 17 [14336/50000]\tLoss: 3.3576\tLR: 0.100000\nTraining Epoch: 17 [14464/50000]\tLoss: 3.2321\tLR: 0.100000\nTraining Epoch: 17 [14592/50000]\tLoss: 3.2365\tLR: 0.100000\nTraining Epoch: 17 [14720/50000]\tLoss: 3.3843\tLR: 0.100000\nTraining Epoch: 17 [14848/50000]\tLoss: 3.1643\tLR: 0.100000\nTraining Epoch: 17 [14976/50000]\tLoss: 3.3800\tLR: 0.100000\nTraining Epoch: 17 [15104/50000]\tLoss: 3.4875\tLR: 0.100000\nTraining Epoch: 17 [15232/50000]\tLoss: 3.5209\tLR: 0.100000\nTraining Epoch: 17 [15360/50000]\tLoss: 3.4293\tLR: 0.100000\nTraining Epoch: 17 [15488/50000]\tLoss: 3.6498\tLR: 0.100000\nTraining Epoch: 17 [15616/50000]\tLoss: 3.4788\tLR: 0.100000\nTraining Epoch: 17 [15744/50000]\tLoss: 3.4709\tLR: 0.100000\nTraining Epoch: 17 [15872/50000]\tLoss: 3.5367\tLR: 0.100000\nTraining Epoch: 17 [16000/50000]\tLoss: 3.3583\tLR: 0.100000\nTraining Epoch: 17 [16128/50000]\tLoss: 3.4635\tLR: 0.100000\nTraining Epoch: 17 [16256/50000]\tLoss: 3.4559\tLR: 0.100000\nTraining Epoch: 17 [16384/50000]\tLoss: 3.5791\tLR: 0.100000\nTraining Epoch: 17 [16512/50000]\tLoss: 3.3768\tLR: 0.100000\nTraining Epoch: 17 [16640/50000]\tLoss: 3.4734\tLR: 0.100000\nTraining Epoch: 17 [16768/50000]\tLoss: 3.4847\tLR: 0.100000\nTraining Epoch: 17 [16896/50000]\tLoss: 3.3981\tLR: 0.100000\nTraining Epoch: 17 [17024/50000]\tLoss: 3.2943\tLR: 0.100000\nTraining Epoch: 17 [17152/50000]\tLoss: 3.4218\tLR: 0.100000\nTraining Epoch: 17 [17280/50000]\tLoss: 3.1404\tLR: 0.100000\nTraining Epoch: 17 [17408/50000]\tLoss: 3.4661\tLR: 0.100000\nTraining Epoch: 17 [17536/50000]\tLoss: 3.2166\tLR: 0.100000\nTraining Epoch: 17 [17664/50000]\tLoss: 3.2251\tLR: 0.100000\nTraining Epoch: 17 [17792/50000]\tLoss: 3.2892\tLR: 0.100000\nTraining Epoch: 17 [17920/50000]\tLoss: 3.6048\tLR: 0.100000\nTraining Epoch: 17 [18048/50000]\tLoss: 3.2484\tLR: 0.100000\nTraining Epoch: 17 [18176/50000]\tLoss: 3.2278\tLR: 0.100000\nTraining Epoch: 17 [18304/50000]\tLoss: 3.1680\tLR: 0.100000\nTraining Epoch: 17 [18432/50000]\tLoss: 3.0561\tLR: 0.100000\nTraining Epoch: 17 [18560/50000]\tLoss: 3.1154\tLR: 0.100000\nTraining Epoch: 17 [18688/50000]\tLoss: 3.3955\tLR: 0.100000\nTraining Epoch: 17 [18816/50000]\tLoss: 3.2434\tLR: 0.100000\nTraining Epoch: 17 [18944/50000]\tLoss: 3.1861\tLR: 0.100000\nTraining Epoch: 17 [19072/50000]\tLoss: 3.1859\tLR: 0.100000\nTraining Epoch: 17 [19200/50000]\tLoss: 3.4116\tLR: 0.100000\nTraining Epoch: 17 [19328/50000]\tLoss: 3.3883\tLR: 0.100000\nTraining Epoch: 17 [19456/50000]\tLoss: 3.3952\tLR: 0.100000\nTraining Epoch: 17 [19584/50000]\tLoss: 3.2345\tLR: 0.100000\nTraining Epoch: 17 [19712/50000]\tLoss: 3.2444\tLR: 0.100000\nTraining Epoch: 17 [19840/50000]\tLoss: 3.2234\tLR: 0.100000\nTraining Epoch: 17 [19968/50000]\tLoss: 3.5568\tLR: 0.100000\nTraining Epoch: 17 [20096/50000]\tLoss: 3.0445\tLR: 0.100000\nTraining Epoch: 17 [20224/50000]\tLoss: 3.5017\tLR: 0.100000\nTraining Epoch: 17 [20352/50000]\tLoss: 3.2462\tLR: 0.100000\nTraining Epoch: 17 [20480/50000]\tLoss: 3.3954\tLR: 0.100000\nTraining Epoch: 17 [20608/50000]\tLoss: 3.3054\tLR: 0.100000\nTraining Epoch: 17 [20736/50000]\tLoss: 3.5757\tLR: 0.100000\nTraining Epoch: 17 [20864/50000]\tLoss: 3.4604\tLR: 0.100000\nTraining Epoch: 17 [20992/50000]\tLoss: 3.4895\tLR: 0.100000\nTraining Epoch: 17 [21120/50000]\tLoss: 3.2675\tLR: 0.100000\nTraining Epoch: 17 [21248/50000]\tLoss: 3.3657\tLR: 0.100000\nTraining Epoch: 17 [21376/50000]\tLoss: 3.5359\tLR: 0.100000\nTraining Epoch: 17 [21504/50000]\tLoss: 3.2990\tLR: 0.100000\nTraining Epoch: 17 [21632/50000]\tLoss: 3.1850\tLR: 0.100000\nTraining Epoch: 17 [21760/50000]\tLoss: 3.6804\tLR: 0.100000\nTraining Epoch: 17 [21888/50000]\tLoss: 3.4964\tLR: 0.100000\nTraining Epoch: 17 [22016/50000]\tLoss: 3.3693\tLR: 0.100000\nTraining Epoch: 17 [22144/50000]\tLoss: 3.3495\tLR: 0.100000\nTraining Epoch: 17 [22272/50000]\tLoss: 3.2753\tLR: 0.100000\nTraining Epoch: 17 [22400/50000]\tLoss: 3.4930\tLR: 0.100000\nTraining Epoch: 17 [22528/50000]\tLoss: 3.3243\tLR: 0.100000\nTraining Epoch: 17 [22656/50000]\tLoss: 3.4894\tLR: 0.100000\nTraining Epoch: 17 [22784/50000]\tLoss: 3.2286\tLR: 0.100000\nTraining Epoch: 17 [22912/50000]\tLoss: 3.1672\tLR: 0.100000\nTraining Epoch: 17 [23040/50000]\tLoss: 3.4317\tLR: 0.100000\nTraining Epoch: 17 [23168/50000]\tLoss: 3.3105\tLR: 0.100000\nTraining Epoch: 17 [23296/50000]\tLoss: 3.4883\tLR: 0.100000\nTraining Epoch: 17 [23424/50000]\tLoss: 3.3131\tLR: 0.100000\nTraining Epoch: 17 [23552/50000]\tLoss: 3.4091\tLR: 0.100000\nTraining Epoch: 17 [23680/50000]\tLoss: 3.2969\tLR: 0.100000\nTraining Epoch: 17 [23808/50000]\tLoss: 3.3356\tLR: 0.100000\nTraining Epoch: 17 [23936/50000]\tLoss: 3.2493\tLR: 0.100000\nTraining Epoch: 17 [24064/50000]\tLoss: 3.5270\tLR: 0.100000\nTraining Epoch: 17 [24192/50000]\tLoss: 3.2613\tLR: 0.100000\nTraining Epoch: 17 [24320/50000]\tLoss: 3.6549\tLR: 0.100000\nTraining Epoch: 17 [24448/50000]\tLoss: 3.4025\tLR: 0.100000\nTraining Epoch: 17 [24576/50000]\tLoss: 3.3544\tLR: 0.100000\nTraining Epoch: 17 [24704/50000]\tLoss: 3.2327\tLR: 0.100000\nTraining Epoch: 17 [24832/50000]\tLoss: 3.3474\tLR: 0.100000\nTraining Epoch: 17 [24960/50000]\tLoss: 3.4467\tLR: 0.100000\nTraining Epoch: 17 [25088/50000]\tLoss: 3.6626\tLR: 0.100000\nTraining Epoch: 17 [25216/50000]\tLoss: 3.1451\tLR: 0.100000\nTraining Epoch: 17 [25344/50000]\tLoss: 3.3908\tLR: 0.100000\nTraining Epoch: 17 [25472/50000]\tLoss: 3.3071\tLR: 0.100000\nTraining Epoch: 17 [25600/50000]\tLoss: 3.1633\tLR: 0.100000\nTraining Epoch: 17 [25728/50000]\tLoss: 3.2587\tLR: 0.100000\nTraining Epoch: 17 [25856/50000]\tLoss: 3.5272\tLR: 0.100000\nTraining Epoch: 17 [25984/50000]\tLoss: 3.5100\tLR: 0.100000\nTraining Epoch: 17 [26112/50000]\tLoss: 3.1387\tLR: 0.100000\nTraining Epoch: 17 [26240/50000]\tLoss: 3.2873\tLR: 0.100000\nTraining Epoch: 17 [26368/50000]\tLoss: 3.3223\tLR: 0.100000\nTraining Epoch: 17 [26496/50000]\tLoss: 3.5284\tLR: 0.100000\nTraining Epoch: 17 [26624/50000]\tLoss: 3.6232\tLR: 0.100000\nTraining Epoch: 17 [26752/50000]\tLoss: 3.5742\tLR: 0.100000\nTraining Epoch: 17 [26880/50000]\tLoss: 3.2773\tLR: 0.100000\nTraining Epoch: 17 [27008/50000]\tLoss: 3.1039\tLR: 0.100000\nTraining Epoch: 17 [27136/50000]\tLoss: 3.1944\tLR: 0.100000\nTraining Epoch: 17 [27264/50000]\tLoss: 3.6078\tLR: 0.100000\nTraining Epoch: 17 [27392/50000]\tLoss: 3.3868\tLR: 0.100000\nTraining Epoch: 17 [27520/50000]\tLoss: 3.1341\tLR: 0.100000\nTraining Epoch: 17 [27648/50000]\tLoss: 3.4030\tLR: 0.100000\nTraining Epoch: 17 [27776/50000]\tLoss: 3.2762\tLR: 0.100000\n","name":"stdout"},{"output_type":"stream","text":"Training Epoch: 17 [27904/50000]\tLoss: 3.4685\tLR: 0.100000\nTraining Epoch: 17 [28032/50000]\tLoss: 3.2566\tLR: 0.100000\nTraining Epoch: 17 [28160/50000]\tLoss: 3.6620\tLR: 0.100000\nTraining Epoch: 17 [28288/50000]\tLoss: 3.3096\tLR: 0.100000\nTraining Epoch: 17 [28416/50000]\tLoss: 3.1836\tLR: 0.100000\nTraining Epoch: 17 [28544/50000]\tLoss: 3.3906\tLR: 0.100000\nTraining Epoch: 17 [28672/50000]\tLoss: 3.1605\tLR: 0.100000\nTraining Epoch: 17 [28800/50000]\tLoss: 3.1930\tLR: 0.100000\nTraining Epoch: 17 [28928/50000]\tLoss: 3.0634\tLR: 0.100000\nTraining Epoch: 17 [29056/50000]\tLoss: 3.5552\tLR: 0.100000\nTraining Epoch: 17 [29184/50000]\tLoss: 3.5920\tLR: 0.100000\nTraining Epoch: 17 [29312/50000]\tLoss: 3.2766\tLR: 0.100000\nTraining Epoch: 17 [29440/50000]\tLoss: 3.4292\tLR: 0.100000\nTraining Epoch: 17 [29568/50000]\tLoss: 3.4538\tLR: 0.100000\nTraining Epoch: 17 [29696/50000]\tLoss: 3.4859\tLR: 0.100000\nTraining Epoch: 17 [29824/50000]\tLoss: 3.2251\tLR: 0.100000\nTraining Epoch: 17 [29952/50000]\tLoss: 3.3215\tLR: 0.100000\nTraining Epoch: 17 [30080/50000]\tLoss: 3.4029\tLR: 0.100000\nTraining Epoch: 17 [30208/50000]\tLoss: 3.0820\tLR: 0.100000\nTraining Epoch: 17 [30336/50000]\tLoss: 3.2716\tLR: 0.100000\nTraining Epoch: 17 [30464/50000]\tLoss: 3.4658\tLR: 0.100000\nTraining Epoch: 17 [30592/50000]\tLoss: 3.3006\tLR: 0.100000\nTraining Epoch: 17 [30720/50000]\tLoss: 3.4275\tLR: 0.100000\nTraining Epoch: 17 [30848/50000]\tLoss: 3.2608\tLR: 0.100000\nTraining Epoch: 17 [30976/50000]\tLoss: 3.3925\tLR: 0.100000\nTraining Epoch: 17 [31104/50000]\tLoss: 3.1762\tLR: 0.100000\nTraining Epoch: 17 [31232/50000]\tLoss: 3.3844\tLR: 0.100000\nTraining Epoch: 17 [31360/50000]\tLoss: 3.1505\tLR: 0.100000\nTraining Epoch: 17 [31488/50000]\tLoss: 3.4188\tLR: 0.100000\nTraining Epoch: 17 [31616/50000]\tLoss: 3.3044\tLR: 0.100000\nTraining Epoch: 17 [31744/50000]\tLoss: 3.5581\tLR: 0.100000\nTraining Epoch: 17 [31872/50000]\tLoss: 3.4278\tLR: 0.100000\nTraining Epoch: 17 [32000/50000]\tLoss: 3.4443\tLR: 0.100000\nTraining Epoch: 17 [32128/50000]\tLoss: 3.6749\tLR: 0.100000\nTraining Epoch: 17 [32256/50000]\tLoss: 3.4234\tLR: 0.100000\nTraining Epoch: 17 [32384/50000]\tLoss: 3.3317\tLR: 0.100000\nTraining Epoch: 17 [32512/50000]\tLoss: 3.4047\tLR: 0.100000\nTraining Epoch: 17 [32640/50000]\tLoss: 3.3423\tLR: 0.100000\nTraining Epoch: 17 [32768/50000]\tLoss: 3.2145\tLR: 0.100000\nTraining Epoch: 17 [32896/50000]\tLoss: 3.4779\tLR: 0.100000\nTraining Epoch: 17 [33024/50000]\tLoss: 3.5304\tLR: 0.100000\nTraining Epoch: 17 [33152/50000]\tLoss: 3.4349\tLR: 0.100000\nTraining Epoch: 17 [33280/50000]\tLoss: 3.4258\tLR: 0.100000\nTraining Epoch: 17 [33408/50000]\tLoss: 3.3947\tLR: 0.100000\nTraining Epoch: 17 [33536/50000]\tLoss: 3.3010\tLR: 0.100000\nTraining Epoch: 17 [33664/50000]\tLoss: 3.2361\tLR: 0.100000\nTraining Epoch: 17 [33792/50000]\tLoss: 3.4656\tLR: 0.100000\nTraining Epoch: 17 [33920/50000]\tLoss: 3.1749\tLR: 0.100000\nTraining Epoch: 17 [34048/50000]\tLoss: 3.5640\tLR: 0.100000\nTraining Epoch: 17 [34176/50000]\tLoss: 3.3553\tLR: 0.100000\nTraining Epoch: 17 [34304/50000]\tLoss: 3.5227\tLR: 0.100000\nTraining Epoch: 17 [34432/50000]\tLoss: 3.3748\tLR: 0.100000\nTraining Epoch: 17 [34560/50000]\tLoss: 3.5098\tLR: 0.100000\nTraining Epoch: 17 [34688/50000]\tLoss: 3.3433\tLR: 0.100000\nTraining Epoch: 17 [34816/50000]\tLoss: 3.5808\tLR: 0.100000\nTraining Epoch: 17 [34944/50000]\tLoss: 3.5054\tLR: 0.100000\nTraining Epoch: 17 [35072/50000]\tLoss: 3.2876\tLR: 0.100000\nTraining Epoch: 17 [35200/50000]\tLoss: 3.5411\tLR: 0.100000\nTraining Epoch: 17 [35328/50000]\tLoss: 3.3378\tLR: 0.100000\nTraining Epoch: 17 [35456/50000]\tLoss: 3.2907\tLR: 0.100000\nTraining Epoch: 17 [35584/50000]\tLoss: 3.6487\tLR: 0.100000\nTraining Epoch: 17 [35712/50000]\tLoss: 3.3692\tLR: 0.100000\nTraining Epoch: 17 [35840/50000]\tLoss: 3.3203\tLR: 0.100000\nTraining Epoch: 17 [35968/50000]\tLoss: 3.3518\tLR: 0.100000\nTraining Epoch: 17 [36096/50000]\tLoss: 3.1676\tLR: 0.100000\nTraining Epoch: 17 [36224/50000]\tLoss: 3.0878\tLR: 0.100000\nTraining Epoch: 17 [36352/50000]\tLoss: 3.2422\tLR: 0.100000\nTraining Epoch: 17 [36480/50000]\tLoss: 3.2376\tLR: 0.100000\nTraining Epoch: 17 [36608/50000]\tLoss: 3.4757\tLR: 0.100000\nTraining Epoch: 17 [36736/50000]\tLoss: 3.2761\tLR: 0.100000\nTraining Epoch: 17 [36864/50000]\tLoss: 3.2763\tLR: 0.100000\nTraining Epoch: 17 [36992/50000]\tLoss: 3.2544\tLR: 0.100000\nTraining Epoch: 17 [37120/50000]\tLoss: 3.4962\tLR: 0.100000\nTraining Epoch: 17 [37248/50000]\tLoss: 3.1816\tLR: 0.100000\nTraining Epoch: 17 [37376/50000]\tLoss: 3.4205\tLR: 0.100000\nTraining Epoch: 17 [37504/50000]\tLoss: 3.3817\tLR: 0.100000\nTraining Epoch: 17 [37632/50000]\tLoss: 3.4535\tLR: 0.100000\nTraining Epoch: 17 [37760/50000]\tLoss: 3.1084\tLR: 0.100000\nTraining Epoch: 17 [37888/50000]\tLoss: 3.3080\tLR: 0.100000\nTraining Epoch: 17 [38016/50000]\tLoss: 3.5405\tLR: 0.100000\nTraining Epoch: 17 [38144/50000]\tLoss: 3.2313\tLR: 0.100000\nTraining Epoch: 17 [38272/50000]\tLoss: 3.3651\tLR: 0.100000\nTraining Epoch: 17 [38400/50000]\tLoss: 3.3378\tLR: 0.100000\nTraining Epoch: 17 [38528/50000]\tLoss: 3.3725\tLR: 0.100000\nTraining Epoch: 17 [38656/50000]\tLoss: 3.5131\tLR: 0.100000\nTraining Epoch: 17 [38784/50000]\tLoss: 3.5157\tLR: 0.100000\nTraining Epoch: 17 [38912/50000]\tLoss: 3.2005\tLR: 0.100000\nTraining Epoch: 17 [39040/50000]\tLoss: 3.2850\tLR: 0.100000\nTraining Epoch: 17 [39168/50000]\tLoss: 3.3856\tLR: 0.100000\nTraining Epoch: 17 [39296/50000]\tLoss: 3.3304\tLR: 0.100000\nTraining Epoch: 17 [39424/50000]\tLoss: 3.4302\tLR: 0.100000\nTraining Epoch: 17 [39552/50000]\tLoss: 3.3607\tLR: 0.100000\nTraining Epoch: 17 [39680/50000]\tLoss: 3.2427\tLR: 0.100000\nTraining Epoch: 17 [39808/50000]\tLoss: 2.9976\tLR: 0.100000\nTraining Epoch: 17 [39936/50000]\tLoss: 3.2425\tLR: 0.100000\nTraining Epoch: 17 [40064/50000]\tLoss: 3.3349\tLR: 0.100000\nTraining Epoch: 17 [40192/50000]\tLoss: 3.2495\tLR: 0.100000\nTraining Epoch: 17 [40320/50000]\tLoss: 3.4588\tLR: 0.100000\nTraining Epoch: 17 [40448/50000]\tLoss: 3.4300\tLR: 0.100000\nTraining Epoch: 17 [40576/50000]\tLoss: 3.2617\tLR: 0.100000\nTraining Epoch: 17 [40704/50000]\tLoss: 3.5221\tLR: 0.100000\nTraining Epoch: 17 [40832/50000]\tLoss: 3.2054\tLR: 0.100000\nTraining Epoch: 17 [40960/50000]\tLoss: 3.4228\tLR: 0.100000\nTraining Epoch: 17 [41088/50000]\tLoss: 3.3236\tLR: 0.100000\nTraining Epoch: 17 [41216/50000]\tLoss: 3.3604\tLR: 0.100000\nTraining Epoch: 17 [41344/50000]\tLoss: 3.3722\tLR: 0.100000\nTraining Epoch: 17 [41472/50000]\tLoss: 3.3685\tLR: 0.100000\nTraining Epoch: 17 [41600/50000]\tLoss: 3.3560\tLR: 0.100000\nTraining Epoch: 17 [41728/50000]\tLoss: 3.5425\tLR: 0.100000\nTraining Epoch: 17 [41856/50000]\tLoss: 3.4231\tLR: 0.100000\nTraining Epoch: 17 [41984/50000]\tLoss: 3.3200\tLR: 0.100000\nTraining Epoch: 17 [42112/50000]\tLoss: 3.4320\tLR: 0.100000\nTraining Epoch: 17 [42240/50000]\tLoss: 3.2213\tLR: 0.100000\nTraining Epoch: 17 [42368/50000]\tLoss: 3.3855\tLR: 0.100000\nTraining Epoch: 17 [42496/50000]\tLoss: 3.4754\tLR: 0.100000\nTraining Epoch: 17 [42624/50000]\tLoss: 3.3061\tLR: 0.100000\nTraining Epoch: 17 [42752/50000]\tLoss: 3.2919\tLR: 0.100000\nTraining Epoch: 17 [42880/50000]\tLoss: 3.2907\tLR: 0.100000\nTraining Epoch: 17 [43008/50000]\tLoss: 3.4017\tLR: 0.100000\nTraining Epoch: 17 [43136/50000]\tLoss: 3.5128\tLR: 0.100000\nTraining Epoch: 17 [43264/50000]\tLoss: 3.4375\tLR: 0.100000\nTraining Epoch: 17 [43392/50000]\tLoss: 3.3389\tLR: 0.100000\nTraining Epoch: 17 [43520/50000]\tLoss: 3.3759\tLR: 0.100000\nTraining Epoch: 17 [43648/50000]\tLoss: 3.3998\tLR: 0.100000\nTraining Epoch: 17 [43776/50000]\tLoss: 3.3634\tLR: 0.100000\nTraining Epoch: 17 [43904/50000]\tLoss: 3.5762\tLR: 0.100000\nTraining Epoch: 17 [44032/50000]\tLoss: 3.3180\tLR: 0.100000\nTraining Epoch: 17 [44160/50000]\tLoss: 3.2483\tLR: 0.100000\nTraining Epoch: 17 [44288/50000]\tLoss: 3.1896\tLR: 0.100000\nTraining Epoch: 17 [44416/50000]\tLoss: 3.1249\tLR: 0.100000\nTraining Epoch: 17 [44544/50000]\tLoss: 3.0809\tLR: 0.100000\nTraining Epoch: 17 [44672/50000]\tLoss: 3.2200\tLR: 0.100000\nTraining Epoch: 17 [44800/50000]\tLoss: 3.1956\tLR: 0.100000\nTraining Epoch: 17 [44928/50000]\tLoss: 3.4143\tLR: 0.100000\nTraining Epoch: 17 [45056/50000]\tLoss: 3.4667\tLR: 0.100000\nTraining Epoch: 17 [45184/50000]\tLoss: 3.4470\tLR: 0.100000\nTraining Epoch: 17 [45312/50000]\tLoss: 3.3619\tLR: 0.100000\nTraining Epoch: 17 [45440/50000]\tLoss: 3.2329\tLR: 0.100000\nTraining Epoch: 17 [45568/50000]\tLoss: 3.3399\tLR: 0.100000\nTraining Epoch: 17 [45696/50000]\tLoss: 3.2059\tLR: 0.100000\nTraining Epoch: 17 [45824/50000]\tLoss: 3.2955\tLR: 0.100000\n","name":"stdout"},{"output_type":"stream","text":"Training Epoch: 17 [45952/50000]\tLoss: 3.5165\tLR: 0.100000\nTraining Epoch: 17 [46080/50000]\tLoss: 3.2651\tLR: 0.100000\nTraining Epoch: 17 [46208/50000]\tLoss: 3.3628\tLR: 0.100000\nTraining Epoch: 17 [46336/50000]\tLoss: 3.0976\tLR: 0.100000\nTraining Epoch: 17 [46464/50000]\tLoss: 3.4816\tLR: 0.100000\nTraining Epoch: 17 [46592/50000]\tLoss: 3.4305\tLR: 0.100000\nTraining Epoch: 17 [46720/50000]\tLoss: 3.2894\tLR: 0.100000\nTraining Epoch: 17 [46848/50000]\tLoss: 3.3519\tLR: 0.100000\nTraining Epoch: 17 [46976/50000]\tLoss: 3.3187\tLR: 0.100000\nTraining Epoch: 17 [47104/50000]\tLoss: 3.3102\tLR: 0.100000\nTraining Epoch: 17 [47232/50000]\tLoss: 3.2241\tLR: 0.100000\nTraining Epoch: 17 [47360/50000]\tLoss: 3.3779\tLR: 0.100000\nTraining Epoch: 17 [47488/50000]\tLoss: 3.5925\tLR: 0.100000\nTraining Epoch: 17 [47616/50000]\tLoss: 3.2831\tLR: 0.100000\nTraining Epoch: 17 [47744/50000]\tLoss: 3.3087\tLR: 0.100000\nTraining Epoch: 17 [47872/50000]\tLoss: 3.3829\tLR: 0.100000\nTraining Epoch: 17 [48000/50000]\tLoss: 3.2015\tLR: 0.100000\nTraining Epoch: 17 [48128/50000]\tLoss: 3.3791\tLR: 0.100000\nTraining Epoch: 17 [48256/50000]\tLoss: 3.3927\tLR: 0.100000\nTraining Epoch: 17 [48384/50000]\tLoss: 3.2554\tLR: 0.100000\nTraining Epoch: 17 [48512/50000]\tLoss: 3.6182\tLR: 0.100000\nTraining Epoch: 17 [48640/50000]\tLoss: 3.1430\tLR: 0.100000\nTraining Epoch: 17 [48768/50000]\tLoss: 3.3383\tLR: 0.100000\nTraining Epoch: 17 [48896/50000]\tLoss: 3.4521\tLR: 0.100000\nTraining Epoch: 17 [49024/50000]\tLoss: 3.2107\tLR: 0.100000\nTraining Epoch: 17 [49152/50000]\tLoss: 3.4815\tLR: 0.100000\nTraining Epoch: 17 [49280/50000]\tLoss: 3.5487\tLR: 0.100000\nTraining Epoch: 17 [49408/50000]\tLoss: 3.3496\tLR: 0.100000\nTraining Epoch: 17 [49536/50000]\tLoss: 3.5250\tLR: 0.100000\nTraining Epoch: 17 [49664/50000]\tLoss: 3.1585\tLR: 0.100000\nTraining Epoch: 17 [49792/50000]\tLoss: 3.0144\tLR: 0.100000\nTraining Epoch: 17 [49920/50000]\tLoss: 3.2374\tLR: 0.100000\nTraining Epoch: 17 [50000/50000]\tLoss: 3.3103\tLR: 0.100000\n=========================================================================================\nTraining Epoch: 18 [128/50000]\tLoss: 3.1999\tLR: 0.100000\nTraining Epoch: 18 [256/50000]\tLoss: 3.3255\tLR: 0.100000\nTraining Epoch: 18 [384/50000]\tLoss: 3.3535\tLR: 0.100000\nTraining Epoch: 18 [512/50000]\tLoss: 3.2040\tLR: 0.100000\nTraining Epoch: 18 [640/50000]\tLoss: 3.5261\tLR: 0.100000\nTraining Epoch: 18 [768/50000]\tLoss: 3.2246\tLR: 0.100000\nTraining Epoch: 18 [896/50000]\tLoss: 3.4796\tLR: 0.100000\nTraining Epoch: 18 [1024/50000]\tLoss: 3.2706\tLR: 0.100000\nTraining Epoch: 18 [1152/50000]\tLoss: 3.3262\tLR: 0.100000\nTraining Epoch: 18 [1280/50000]\tLoss: 3.1723\tLR: 0.100000\nTraining Epoch: 18 [1408/50000]\tLoss: 3.4424\tLR: 0.100000\nTraining Epoch: 18 [1536/50000]\tLoss: 3.2486\tLR: 0.100000\nTraining Epoch: 18 [1664/50000]\tLoss: 3.1342\tLR: 0.100000\nTraining Epoch: 18 [1792/50000]\tLoss: 3.1372\tLR: 0.100000\nTraining Epoch: 18 [1920/50000]\tLoss: 3.6873\tLR: 0.100000\nTraining Epoch: 18 [2048/50000]\tLoss: 3.5778\tLR: 0.100000\nTraining Epoch: 18 [2176/50000]\tLoss: 3.1666\tLR: 0.100000\nTraining Epoch: 18 [2304/50000]\tLoss: 3.1570\tLR: 0.100000\nTraining Epoch: 18 [2432/50000]\tLoss: 3.3117\tLR: 0.100000\nTraining Epoch: 18 [2560/50000]\tLoss: 3.2151\tLR: 0.100000\nTraining Epoch: 18 [2688/50000]\tLoss: 3.7114\tLR: 0.100000\nTraining Epoch: 18 [2816/50000]\tLoss: 3.3100\tLR: 0.100000\nTraining Epoch: 18 [2944/50000]\tLoss: 3.1240\tLR: 0.100000\nTraining Epoch: 18 [3072/50000]\tLoss: 3.3926\tLR: 0.100000\nTraining Epoch: 18 [3200/50000]\tLoss: 3.2896\tLR: 0.100000\nTraining Epoch: 18 [3328/50000]\tLoss: 3.4150\tLR: 0.100000\nTraining Epoch: 18 [3456/50000]\tLoss: 3.2769\tLR: 0.100000\nTraining Epoch: 18 [3584/50000]\tLoss: 3.5464\tLR: 0.100000\nTraining Epoch: 18 [3712/50000]\tLoss: 3.5897\tLR: 0.100000\nTraining Epoch: 18 [3840/50000]\tLoss: 3.1693\tLR: 0.100000\nTraining Epoch: 18 [3968/50000]\tLoss: 3.4459\tLR: 0.100000\nTraining Epoch: 18 [4096/50000]\tLoss: 3.4003\tLR: 0.100000\nTraining Epoch: 18 [4224/50000]\tLoss: 3.5112\tLR: 0.100000\nTraining Epoch: 18 [4352/50000]\tLoss: 3.5672\tLR: 0.100000\nTraining Epoch: 18 [4480/50000]\tLoss: 3.3082\tLR: 0.100000\nTraining Epoch: 18 [4608/50000]\tLoss: 3.1961\tLR: 0.100000\nTraining Epoch: 18 [4736/50000]\tLoss: 3.6614\tLR: 0.100000\nTraining Epoch: 18 [4864/50000]\tLoss: 3.4000\tLR: 0.100000\nTraining Epoch: 18 [4992/50000]\tLoss: 3.3454\tLR: 0.100000\nTraining Epoch: 18 [5120/50000]\tLoss: 3.1467\tLR: 0.100000\nTraining Epoch: 18 [5248/50000]\tLoss: 3.3700\tLR: 0.100000\nTraining Epoch: 18 [5376/50000]\tLoss: 3.3713\tLR: 0.100000\nTraining Epoch: 18 [5504/50000]\tLoss: 3.2803\tLR: 0.100000\nTraining Epoch: 18 [5632/50000]\tLoss: 3.4275\tLR: 0.100000\nTraining Epoch: 18 [5760/50000]\tLoss: 3.0767\tLR: 0.100000\nTraining Epoch: 18 [5888/50000]\tLoss: 3.4939\tLR: 0.100000\nTraining Epoch: 18 [6016/50000]\tLoss: 3.3268\tLR: 0.100000\nTraining Epoch: 18 [6144/50000]\tLoss: 3.3711\tLR: 0.100000\nTraining Epoch: 18 [6272/50000]\tLoss: 3.2061\tLR: 0.100000\nTraining Epoch: 18 [6400/50000]\tLoss: 3.3858\tLR: 0.100000\nTraining Epoch: 18 [6528/50000]\tLoss: 3.1653\tLR: 0.100000\nTraining Epoch: 18 [6656/50000]\tLoss: 3.1758\tLR: 0.100000\nTraining Epoch: 18 [6784/50000]\tLoss: 3.3176\tLR: 0.100000\nTraining Epoch: 18 [6912/50000]\tLoss: 3.3761\tLR: 0.100000\nTraining Epoch: 18 [7040/50000]\tLoss: 3.4395\tLR: 0.100000\nTraining Epoch: 18 [7168/50000]\tLoss: 3.3221\tLR: 0.100000\nTraining Epoch: 18 [7296/50000]\tLoss: 3.2396\tLR: 0.100000\nTraining Epoch: 18 [7424/50000]\tLoss: 3.4194\tLR: 0.100000\nTraining Epoch: 18 [7552/50000]\tLoss: 3.3908\tLR: 0.100000\nTraining Epoch: 18 [7680/50000]\tLoss: 3.3886\tLR: 0.100000\nTraining Epoch: 18 [7808/50000]\tLoss: 3.1394\tLR: 0.100000\nTraining Epoch: 18 [7936/50000]\tLoss: 3.4618\tLR: 0.100000\nTraining Epoch: 18 [8064/50000]\tLoss: 3.5955\tLR: 0.100000\nTraining Epoch: 18 [8192/50000]\tLoss: 3.4522\tLR: 0.100000\nTraining Epoch: 18 [8320/50000]\tLoss: 3.3925\tLR: 0.100000\nTraining Epoch: 18 [8448/50000]\tLoss: 3.1533\tLR: 0.100000\nTraining Epoch: 18 [8576/50000]\tLoss: 3.6209\tLR: 0.100000\nTraining Epoch: 18 [8704/50000]\tLoss: 3.5106\tLR: 0.100000\nTraining Epoch: 18 [8832/50000]\tLoss: 3.6575\tLR: 0.100000\nTraining Epoch: 18 [8960/50000]\tLoss: 3.5471\tLR: 0.100000\nTraining Epoch: 18 [9088/50000]\tLoss: 3.4563\tLR: 0.100000\nTraining Epoch: 18 [9216/50000]\tLoss: 3.2894\tLR: 0.100000\nTraining Epoch: 18 [9344/50000]\tLoss: 3.3441\tLR: 0.100000\nTraining Epoch: 18 [9472/50000]\tLoss: 3.1657\tLR: 0.100000\nTraining Epoch: 18 [9600/50000]\tLoss: 3.5387\tLR: 0.100000\nTraining Epoch: 18 [9728/50000]\tLoss: 3.3715\tLR: 0.100000\nTraining Epoch: 18 [9856/50000]\tLoss: 3.2860\tLR: 0.100000\nTraining Epoch: 18 [9984/50000]\tLoss: 3.4850\tLR: 0.100000\nTraining Epoch: 18 [10112/50000]\tLoss: 3.1548\tLR: 0.100000\nTraining Epoch: 18 [10240/50000]\tLoss: 3.2666\tLR: 0.100000\nTraining Epoch: 18 [10368/50000]\tLoss: 3.3740\tLR: 0.100000\nTraining Epoch: 18 [10496/50000]\tLoss: 3.5087\tLR: 0.100000\nTraining Epoch: 18 [10624/50000]\tLoss: 3.0103\tLR: 0.100000\nTraining Epoch: 18 [10752/50000]\tLoss: 3.2247\tLR: 0.100000\nTraining Epoch: 18 [10880/50000]\tLoss: 3.2738\tLR: 0.100000\nTraining Epoch: 18 [11008/50000]\tLoss: 3.5055\tLR: 0.100000\nTraining Epoch: 18 [11136/50000]\tLoss: 3.4896\tLR: 0.100000\nTraining Epoch: 18 [11264/50000]\tLoss: 3.3325\tLR: 0.100000\nTraining Epoch: 18 [11392/50000]\tLoss: 3.2755\tLR: 0.100000\nTraining Epoch: 18 [11520/50000]\tLoss: 3.2439\tLR: 0.100000\nTraining Epoch: 18 [11648/50000]\tLoss: 3.2483\tLR: 0.100000\nTraining Epoch: 18 [11776/50000]\tLoss: 3.3864\tLR: 0.100000\nTraining Epoch: 18 [11904/50000]\tLoss: 3.4247\tLR: 0.100000\nTraining Epoch: 18 [12032/50000]\tLoss: 3.5874\tLR: 0.100000\nTraining Epoch: 18 [12160/50000]\tLoss: 3.4563\tLR: 0.100000\nTraining Epoch: 18 [12288/50000]\tLoss: 3.2100\tLR: 0.100000\nTraining Epoch: 18 [12416/50000]\tLoss: 3.2617\tLR: 0.100000\nTraining Epoch: 18 [12544/50000]\tLoss: 3.2024\tLR: 0.100000\nTraining Epoch: 18 [12672/50000]\tLoss: 3.4548\tLR: 0.100000\nTraining Epoch: 18 [12800/50000]\tLoss: 3.4951\tLR: 0.100000\nTraining Epoch: 18 [12928/50000]\tLoss: 3.1548\tLR: 0.100000\nTraining Epoch: 18 [13056/50000]\tLoss: 3.6170\tLR: 0.100000\nTraining Epoch: 18 [13184/50000]\tLoss: 3.3616\tLR: 0.100000\nTraining Epoch: 18 [13312/50000]\tLoss: 3.3165\tLR: 0.100000\nTraining Epoch: 18 [13440/50000]\tLoss: 3.1699\tLR: 0.100000\nTraining Epoch: 18 [13568/50000]\tLoss: 3.2763\tLR: 0.100000\nTraining Epoch: 18 [13696/50000]\tLoss: 3.3704\tLR: 0.100000\nTraining Epoch: 18 [13824/50000]\tLoss: 3.2384\tLR: 0.100000\n","name":"stdout"},{"output_type":"stream","text":"Training Epoch: 18 [13952/50000]\tLoss: 3.3561\tLR: 0.100000\nTraining Epoch: 18 [14080/50000]\tLoss: 3.5020\tLR: 0.100000\nTraining Epoch: 18 [14208/50000]\tLoss: 3.2527\tLR: 0.100000\nTraining Epoch: 18 [14336/50000]\tLoss: 3.3573\tLR: 0.100000\nTraining Epoch: 18 [14464/50000]\tLoss: 3.3014\tLR: 0.100000\nTraining Epoch: 18 [14592/50000]\tLoss: 3.3988\tLR: 0.100000\nTraining Epoch: 18 [14720/50000]\tLoss: 3.1056\tLR: 0.100000\nTraining Epoch: 18 [14848/50000]\tLoss: 3.3429\tLR: 0.100000\nTraining Epoch: 18 [14976/50000]\tLoss: 3.4826\tLR: 0.100000\nTraining Epoch: 18 [15104/50000]\tLoss: 3.1729\tLR: 0.100000\nTraining Epoch: 18 [15232/50000]\tLoss: 3.0429\tLR: 0.100000\nTraining Epoch: 18 [15360/50000]\tLoss: 3.4048\tLR: 0.100000\nTraining Epoch: 18 [15488/50000]\tLoss: 3.4052\tLR: 0.100000\nTraining Epoch: 18 [15616/50000]\tLoss: 3.1716\tLR: 0.100000\nTraining Epoch: 18 [15744/50000]\tLoss: 3.4097\tLR: 0.100000\nTraining Epoch: 18 [15872/50000]\tLoss: 3.3361\tLR: 0.100000\nTraining Epoch: 18 [16000/50000]\tLoss: 3.1785\tLR: 0.100000\nTraining Epoch: 18 [16128/50000]\tLoss: 3.2316\tLR: 0.100000\nTraining Epoch: 18 [16256/50000]\tLoss: 3.2024\tLR: 0.100000\nTraining Epoch: 18 [16384/50000]\tLoss: 3.6397\tLR: 0.100000\nTraining Epoch: 18 [16512/50000]\tLoss: 3.3060\tLR: 0.100000\nTraining Epoch: 18 [16640/50000]\tLoss: 3.2296\tLR: 0.100000\nTraining Epoch: 18 [16768/50000]\tLoss: 3.2896\tLR: 0.100000\nTraining Epoch: 18 [16896/50000]\tLoss: 3.3803\tLR: 0.100000\nTraining Epoch: 18 [17024/50000]\tLoss: 3.3398\tLR: 0.100000\nTraining Epoch: 18 [17152/50000]\tLoss: 3.1881\tLR: 0.100000\nTraining Epoch: 18 [17280/50000]\tLoss: 3.2334\tLR: 0.100000\nTraining Epoch: 18 [17408/50000]\tLoss: 3.2981\tLR: 0.100000\nTraining Epoch: 18 [17536/50000]\tLoss: 3.2841\tLR: 0.100000\nTraining Epoch: 18 [17664/50000]\tLoss: 3.4398\tLR: 0.100000\nTraining Epoch: 18 [17792/50000]\tLoss: 3.4642\tLR: 0.100000\nTraining Epoch: 18 [17920/50000]\tLoss: 3.5076\tLR: 0.100000\nTraining Epoch: 18 [18048/50000]\tLoss: 3.3886\tLR: 0.100000\nTraining Epoch: 18 [18176/50000]\tLoss: 3.2943\tLR: 0.100000\nTraining Epoch: 18 [18304/50000]\tLoss: 3.4364\tLR: 0.100000\nTraining Epoch: 18 [18432/50000]\tLoss: 3.4379\tLR: 0.100000\nTraining Epoch: 18 [18560/50000]\tLoss: 3.5900\tLR: 0.100000\nTraining Epoch: 18 [18688/50000]\tLoss: 3.4684\tLR: 0.100000\nTraining Epoch: 18 [18816/50000]\tLoss: 3.3918\tLR: 0.100000\nTraining Epoch: 18 [18944/50000]\tLoss: 3.3830\tLR: 0.100000\nTraining Epoch: 18 [19072/50000]\tLoss: 3.4502\tLR: 0.100000\nTraining Epoch: 18 [19200/50000]\tLoss: 3.2858\tLR: 0.100000\nTraining Epoch: 18 [19328/50000]\tLoss: 3.4261\tLR: 0.100000\nTraining Epoch: 18 [19456/50000]\tLoss: 3.5783\tLR: 0.100000\nTraining Epoch: 18 [19584/50000]\tLoss: 3.2919\tLR: 0.100000\nTraining Epoch: 18 [19712/50000]\tLoss: 3.4251\tLR: 0.100000\nTraining Epoch: 18 [19840/50000]\tLoss: 3.2841\tLR: 0.100000\nTraining Epoch: 18 [19968/50000]\tLoss: 3.2520\tLR: 0.100000\nTraining Epoch: 18 [20096/50000]\tLoss: 3.5088\tLR: 0.100000\nTraining Epoch: 18 [20224/50000]\tLoss: 3.3096\tLR: 0.100000\nTraining Epoch: 18 [20352/50000]\tLoss: 3.0538\tLR: 0.100000\nTraining Epoch: 18 [20480/50000]\tLoss: 3.5389\tLR: 0.100000\nTraining Epoch: 18 [20608/50000]\tLoss: 3.6183\tLR: 0.100000\nTraining Epoch: 18 [20736/50000]\tLoss: 3.6155\tLR: 0.100000\nTraining Epoch: 18 [20864/50000]\tLoss: 3.7669\tLR: 0.100000\nTraining Epoch: 18 [20992/50000]\tLoss: 3.5057\tLR: 0.100000\nTraining Epoch: 18 [21120/50000]\tLoss: 3.4474\tLR: 0.100000\nTraining Epoch: 18 [21248/50000]\tLoss: 3.2626\tLR: 0.100000\nTraining Epoch: 18 [21376/50000]\tLoss: 3.1673\tLR: 0.100000\nTraining Epoch: 18 [21504/50000]\tLoss: 3.2891\tLR: 0.100000\nTraining Epoch: 18 [21632/50000]\tLoss: 3.2780\tLR: 0.100000\nTraining Epoch: 18 [21760/50000]\tLoss: 3.3784\tLR: 0.100000\nTraining Epoch: 18 [21888/50000]\tLoss: 3.3299\tLR: 0.100000\nTraining Epoch: 18 [22016/50000]\tLoss: 3.3550\tLR: 0.100000\nTraining Epoch: 18 [22144/50000]\tLoss: 3.2772\tLR: 0.100000\nTraining Epoch: 18 [22272/50000]\tLoss: 3.4076\tLR: 0.100000\nTraining Epoch: 18 [22400/50000]\tLoss: 3.3563\tLR: 0.100000\nTraining Epoch: 18 [22528/50000]\tLoss: 3.2961\tLR: 0.100000\nTraining Epoch: 18 [22656/50000]\tLoss: 3.0739\tLR: 0.100000\nTraining Epoch: 18 [22784/50000]\tLoss: 3.1257\tLR: 0.100000\nTraining Epoch: 18 [22912/50000]\tLoss: 3.1210\tLR: 0.100000\nTraining Epoch: 18 [23040/50000]\tLoss: 3.3475\tLR: 0.100000\nTraining Epoch: 18 [23168/50000]\tLoss: 3.1599\tLR: 0.100000\nTraining Epoch: 18 [23296/50000]\tLoss: 3.3151\tLR: 0.100000\nTraining Epoch: 18 [23424/50000]\tLoss: 3.3473\tLR: 0.100000\nTraining Epoch: 18 [23552/50000]\tLoss: 3.2236\tLR: 0.100000\nTraining Epoch: 18 [23680/50000]\tLoss: 3.3711\tLR: 0.100000\nTraining Epoch: 18 [23808/50000]\tLoss: 3.5041\tLR: 0.100000\nTraining Epoch: 18 [23936/50000]\tLoss: 3.2198\tLR: 0.100000\nTraining Epoch: 18 [24064/50000]\tLoss: 3.4364\tLR: 0.100000\nTraining Epoch: 18 [24192/50000]\tLoss: 3.3573\tLR: 0.100000\nTraining Epoch: 18 [24320/50000]\tLoss: 3.3109\tLR: 0.100000\nTraining Epoch: 18 [24448/50000]\tLoss: 3.3505\tLR: 0.100000\nTraining Epoch: 18 [24576/50000]\tLoss: 3.1460\tLR: 0.100000\nTraining Epoch: 18 [24704/50000]\tLoss: 3.3713\tLR: 0.100000\nTraining Epoch: 18 [24832/50000]\tLoss: 3.5934\tLR: 0.100000\nTraining Epoch: 18 [24960/50000]\tLoss: 3.2608\tLR: 0.100000\nTraining Epoch: 18 [25088/50000]\tLoss: 3.4185\tLR: 0.100000\nTraining Epoch: 18 [25216/50000]\tLoss: 3.2829\tLR: 0.100000\nTraining Epoch: 18 [25344/50000]\tLoss: 3.2764\tLR: 0.100000\nTraining Epoch: 18 [25472/50000]\tLoss: 3.3586\tLR: 0.100000\nTraining Epoch: 18 [25600/50000]\tLoss: 3.3649\tLR: 0.100000\nTraining Epoch: 18 [25728/50000]\tLoss: 3.3912\tLR: 0.100000\nTraining Epoch: 18 [25856/50000]\tLoss: 3.2919\tLR: 0.100000\nTraining Epoch: 18 [25984/50000]\tLoss: 3.5306\tLR: 0.100000\nTraining Epoch: 18 [26112/50000]\tLoss: 3.4551\tLR: 0.100000\nTraining Epoch: 18 [26240/50000]\tLoss: 3.1278\tLR: 0.100000\nTraining Epoch: 18 [26368/50000]\tLoss: 3.3210\tLR: 0.100000\nTraining Epoch: 18 [26496/50000]\tLoss: 3.3057\tLR: 0.100000\nTraining Epoch: 18 [26624/50000]\tLoss: 3.3749\tLR: 0.100000\nTraining Epoch: 18 [26752/50000]\tLoss: 3.2392\tLR: 0.100000\nTraining Epoch: 18 [26880/50000]\tLoss: 3.2117\tLR: 0.100000\nTraining Epoch: 18 [27008/50000]\tLoss: 3.3147\tLR: 0.100000\nTraining Epoch: 18 [27136/50000]\tLoss: 3.1503\tLR: 0.100000\nTraining Epoch: 18 [27264/50000]\tLoss: 3.1273\tLR: 0.100000\nTraining Epoch: 18 [27392/50000]\tLoss: 3.3157\tLR: 0.100000\nTraining Epoch: 18 [27520/50000]\tLoss: 3.2090\tLR: 0.100000\nTraining Epoch: 18 [27648/50000]\tLoss: 3.3573\tLR: 0.100000\nTraining Epoch: 18 [27776/50000]\tLoss: 3.2901\tLR: 0.100000\nTraining Epoch: 18 [27904/50000]\tLoss: 3.2759\tLR: 0.100000\nTraining Epoch: 18 [28032/50000]\tLoss: 3.2502\tLR: 0.100000\nTraining Epoch: 18 [28160/50000]\tLoss: 3.2699\tLR: 0.100000\nTraining Epoch: 18 [28288/50000]\tLoss: 3.4035\tLR: 0.100000\nTraining Epoch: 18 [28416/50000]\tLoss: 3.2371\tLR: 0.100000\nTraining Epoch: 18 [28544/50000]\tLoss: 3.3300\tLR: 0.100000\nTraining Epoch: 18 [28672/50000]\tLoss: 3.1301\tLR: 0.100000\nTraining Epoch: 18 [28800/50000]\tLoss: 3.0307\tLR: 0.100000\nTraining Epoch: 18 [28928/50000]\tLoss: 3.3122\tLR: 0.100000\nTraining Epoch: 18 [29056/50000]\tLoss: 3.3328\tLR: 0.100000\nTraining Epoch: 18 [29184/50000]\tLoss: 3.4608\tLR: 0.100000\nTraining Epoch: 18 [29312/50000]\tLoss: 3.2997\tLR: 0.100000\nTraining Epoch: 18 [29440/50000]\tLoss: 3.2216\tLR: 0.100000\nTraining Epoch: 18 [29568/50000]\tLoss: 3.4816\tLR: 0.100000\nTraining Epoch: 18 [29696/50000]\tLoss: 3.1236\tLR: 0.100000\nTraining Epoch: 18 [29824/50000]\tLoss: 3.2189\tLR: 0.100000\nTraining Epoch: 18 [29952/50000]\tLoss: 3.1811\tLR: 0.100000\nTraining Epoch: 18 [30080/50000]\tLoss: 3.2024\tLR: 0.100000\nTraining Epoch: 18 [30208/50000]\tLoss: 3.2330\tLR: 0.100000\nTraining Epoch: 18 [30336/50000]\tLoss: 3.3807\tLR: 0.100000\nTraining Epoch: 18 [30464/50000]\tLoss: 3.3226\tLR: 0.100000\nTraining Epoch: 18 [30592/50000]\tLoss: 3.2496\tLR: 0.100000\nTraining Epoch: 18 [30720/50000]\tLoss: 3.3543\tLR: 0.100000\nTraining Epoch: 18 [30848/50000]\tLoss: 3.2748\tLR: 0.100000\nTraining Epoch: 18 [30976/50000]\tLoss: 3.2582\tLR: 0.100000\nTraining Epoch: 18 [31104/50000]\tLoss: 3.5593\tLR: 0.100000\nTraining Epoch: 18 [31232/50000]\tLoss: 3.1742\tLR: 0.100000\nTraining Epoch: 18 [31360/50000]\tLoss: 3.2239\tLR: 0.100000\nTraining Epoch: 18 [31488/50000]\tLoss: 3.2079\tLR: 0.100000\nTraining Epoch: 18 [31616/50000]\tLoss: 3.3455\tLR: 0.100000\nTraining Epoch: 18 [31744/50000]\tLoss: 3.3145\tLR: 0.100000\nTraining Epoch: 18 [31872/50000]\tLoss: 3.2521\tLR: 0.100000\n","name":"stdout"},{"output_type":"stream","text":"Training Epoch: 18 [32000/50000]\tLoss: 3.1299\tLR: 0.100000\nTraining Epoch: 18 [32128/50000]\tLoss: 3.2854\tLR: 0.100000\nTraining Epoch: 18 [32256/50000]\tLoss: 3.1843\tLR: 0.100000\nTraining Epoch: 18 [32384/50000]\tLoss: 3.1624\tLR: 0.100000\nTraining Epoch: 18 [32512/50000]\tLoss: 3.0489\tLR: 0.100000\nTraining Epoch: 18 [32640/50000]\tLoss: 3.2962\tLR: 0.100000\nTraining Epoch: 18 [32768/50000]\tLoss: 3.1951\tLR: 0.100000\nTraining Epoch: 18 [32896/50000]\tLoss: 3.3284\tLR: 0.100000\nTraining Epoch: 18 [33024/50000]\tLoss: 3.1901\tLR: 0.100000\nTraining Epoch: 18 [33152/50000]\tLoss: 3.3699\tLR: 0.100000\nTraining Epoch: 18 [33280/50000]\tLoss: 3.2232\tLR: 0.100000\nTraining Epoch: 18 [33408/50000]\tLoss: 3.3144\tLR: 0.100000\nTraining Epoch: 18 [33536/50000]\tLoss: 3.1662\tLR: 0.100000\nTraining Epoch: 18 [33664/50000]\tLoss: 3.2930\tLR: 0.100000\nTraining Epoch: 18 [33792/50000]\tLoss: 3.5123\tLR: 0.100000\nTraining Epoch: 18 [33920/50000]\tLoss: 3.4062\tLR: 0.100000\nTraining Epoch: 18 [34048/50000]\tLoss: 3.0703\tLR: 0.100000\nTraining Epoch: 18 [34176/50000]\tLoss: 3.3654\tLR: 0.100000\nTraining Epoch: 18 [34304/50000]\tLoss: 3.2870\tLR: 0.100000\nTraining Epoch: 18 [34432/50000]\tLoss: 3.5450\tLR: 0.100000\nTraining Epoch: 18 [34560/50000]\tLoss: 3.1684\tLR: 0.100000\nTraining Epoch: 18 [34688/50000]\tLoss: 3.2529\tLR: 0.100000\nTraining Epoch: 18 [34816/50000]\tLoss: 3.3071\tLR: 0.100000\nTraining Epoch: 18 [34944/50000]\tLoss: 3.3357\tLR: 0.100000\nTraining Epoch: 18 [35072/50000]\tLoss: 3.0846\tLR: 0.100000\nTraining Epoch: 18 [35200/50000]\tLoss: 3.2563\tLR: 0.100000\nTraining Epoch: 18 [35328/50000]\tLoss: 3.4246\tLR: 0.100000\nTraining Epoch: 18 [35456/50000]\tLoss: 3.1486\tLR: 0.100000\nTraining Epoch: 18 [35584/50000]\tLoss: 3.2865\tLR: 0.100000\nTraining Epoch: 18 [35712/50000]\tLoss: 3.4845\tLR: 0.100000\nTraining Epoch: 18 [35840/50000]\tLoss: 3.4493\tLR: 0.100000\nTraining Epoch: 18 [35968/50000]\tLoss: 3.4282\tLR: 0.100000\nTraining Epoch: 18 [36096/50000]\tLoss: 3.5744\tLR: 0.100000\nTraining Epoch: 18 [36224/50000]\tLoss: 3.2442\tLR: 0.100000\nTraining Epoch: 18 [36352/50000]\tLoss: 3.5478\tLR: 0.100000\nTraining Epoch: 18 [36480/50000]\tLoss: 3.3762\tLR: 0.100000\nTraining Epoch: 18 [36608/50000]\tLoss: 3.2864\tLR: 0.100000\nTraining Epoch: 18 [36736/50000]\tLoss: 3.3074\tLR: 0.100000\nTraining Epoch: 18 [36864/50000]\tLoss: 3.1561\tLR: 0.100000\nTraining Epoch: 18 [36992/50000]\tLoss: 3.2820\tLR: 0.100000\nTraining Epoch: 18 [37120/50000]\tLoss: 3.4289\tLR: 0.100000\nTraining Epoch: 18 [37248/50000]\tLoss: 3.2069\tLR: 0.100000\nTraining Epoch: 18 [37376/50000]\tLoss: 3.5957\tLR: 0.100000\nTraining Epoch: 18 [37504/50000]\tLoss: 3.3593\tLR: 0.100000\nTraining Epoch: 18 [37632/50000]\tLoss: 3.3487\tLR: 0.100000\nTraining Epoch: 18 [37760/50000]\tLoss: 3.4659\tLR: 0.100000\nTraining Epoch: 18 [37888/50000]\tLoss: 3.2355\tLR: 0.100000\nTraining Epoch: 18 [38016/50000]\tLoss: 3.2661\tLR: 0.100000\nTraining Epoch: 18 [38144/50000]\tLoss: 3.3818\tLR: 0.100000\nTraining Epoch: 18 [38272/50000]\tLoss: 3.4379\tLR: 0.100000\nTraining Epoch: 18 [38400/50000]\tLoss: 3.0236\tLR: 0.100000\nTraining Epoch: 18 [38528/50000]\tLoss: 3.5532\tLR: 0.100000\nTraining Epoch: 18 [38656/50000]\tLoss: 3.5205\tLR: 0.100000\nTraining Epoch: 18 [38784/50000]\tLoss: 3.0904\tLR: 0.100000\nTraining Epoch: 18 [38912/50000]\tLoss: 3.2904\tLR: 0.100000\nTraining Epoch: 18 [39040/50000]\tLoss: 3.3062\tLR: 0.100000\nTraining Epoch: 18 [39168/50000]\tLoss: 3.3027\tLR: 0.100000\nTraining Epoch: 18 [39296/50000]\tLoss: 3.3138\tLR: 0.100000\nTraining Epoch: 18 [39424/50000]\tLoss: 3.4224\tLR: 0.100000\nTraining Epoch: 18 [39552/50000]\tLoss: 3.5574\tLR: 0.100000\nTraining Epoch: 18 [39680/50000]\tLoss: 3.0613\tLR: 0.100000\nTraining Epoch: 18 [39808/50000]\tLoss: 3.2266\tLR: 0.100000\nTraining Epoch: 18 [39936/50000]\tLoss: 3.4175\tLR: 0.100000\nTraining Epoch: 18 [40064/50000]\tLoss: 3.1617\tLR: 0.100000\nTraining Epoch: 18 [40192/50000]\tLoss: 3.4398\tLR: 0.100000\nTraining Epoch: 18 [40320/50000]\tLoss: 3.5993\tLR: 0.100000\nTraining Epoch: 18 [40448/50000]\tLoss: 3.3088\tLR: 0.100000\nTraining Epoch: 18 [40576/50000]\tLoss: 3.1993\tLR: 0.100000\nTraining Epoch: 18 [40704/50000]\tLoss: 3.3150\tLR: 0.100000\nTraining Epoch: 18 [40832/50000]\tLoss: 3.3073\tLR: 0.100000\nTraining Epoch: 18 [40960/50000]\tLoss: 3.3790\tLR: 0.100000\nTraining Epoch: 18 [41088/50000]\tLoss: 3.3153\tLR: 0.100000\nTraining Epoch: 18 [41216/50000]\tLoss: 3.1894\tLR: 0.100000\nTraining Epoch: 18 [41344/50000]\tLoss: 3.1133\tLR: 0.100000\nTraining Epoch: 18 [41472/50000]\tLoss: 3.5586\tLR: 0.100000\nTraining Epoch: 18 [41600/50000]\tLoss: 3.4343\tLR: 0.100000\nTraining Epoch: 18 [41728/50000]\tLoss: 3.4058\tLR: 0.100000\nTraining Epoch: 18 [41856/50000]\tLoss: 3.3244\tLR: 0.100000\nTraining Epoch: 18 [41984/50000]\tLoss: 3.2441\tLR: 0.100000\nTraining Epoch: 18 [42112/50000]\tLoss: 3.3765\tLR: 0.100000\nTraining Epoch: 18 [42240/50000]\tLoss: 2.9379\tLR: 0.100000\nTraining Epoch: 18 [42368/50000]\tLoss: 3.4126\tLR: 0.100000\nTraining Epoch: 18 [42496/50000]\tLoss: 3.0186\tLR: 0.100000\nTraining Epoch: 18 [42624/50000]\tLoss: 3.3204\tLR: 0.100000\nTraining Epoch: 18 [42752/50000]\tLoss: 3.0117\tLR: 0.100000\nTraining Epoch: 18 [42880/50000]\tLoss: 3.1963\tLR: 0.100000\nTraining Epoch: 18 [43008/50000]\tLoss: 3.1131\tLR: 0.100000\nTraining Epoch: 18 [43136/50000]\tLoss: 3.3057\tLR: 0.100000\nTraining Epoch: 18 [43264/50000]\tLoss: 3.1104\tLR: 0.100000\nTraining Epoch: 18 [43392/50000]\tLoss: 3.4045\tLR: 0.100000\nTraining Epoch: 18 [43520/50000]\tLoss: 3.2348\tLR: 0.100000\nTraining Epoch: 18 [43648/50000]\tLoss: 3.3482\tLR: 0.100000\nTraining Epoch: 18 [43776/50000]\tLoss: 3.5466\tLR: 0.100000\nTraining Epoch: 18 [43904/50000]\tLoss: 3.3197\tLR: 0.100000\nTraining Epoch: 18 [44032/50000]\tLoss: 3.0407\tLR: 0.100000\nTraining Epoch: 18 [44160/50000]\tLoss: 3.3256\tLR: 0.100000\nTraining Epoch: 18 [44288/50000]\tLoss: 3.1656\tLR: 0.100000\nTraining Epoch: 18 [44416/50000]\tLoss: 3.1421\tLR: 0.100000\nTraining Epoch: 18 [44544/50000]\tLoss: 3.3494\tLR: 0.100000\nTraining Epoch: 18 [44672/50000]\tLoss: 3.2968\tLR: 0.100000\nTraining Epoch: 18 [44800/50000]\tLoss: 3.1540\tLR: 0.100000\nTraining Epoch: 18 [44928/50000]\tLoss: 3.2166\tLR: 0.100000\nTraining Epoch: 18 [45056/50000]\tLoss: 3.1961\tLR: 0.100000\nTraining Epoch: 18 [45184/50000]\tLoss: 3.2253\tLR: 0.100000\nTraining Epoch: 18 [45312/50000]\tLoss: 3.4235\tLR: 0.100000\nTraining Epoch: 18 [45440/50000]\tLoss: 3.3428\tLR: 0.100000\nTraining Epoch: 18 [45568/50000]\tLoss: 3.4415\tLR: 0.100000\nTraining Epoch: 18 [45696/50000]\tLoss: 3.4553\tLR: 0.100000\nTraining Epoch: 18 [45824/50000]\tLoss: 3.2561\tLR: 0.100000\nTraining Epoch: 18 [45952/50000]\tLoss: 3.5039\tLR: 0.100000\nTraining Epoch: 18 [46080/50000]\tLoss: 3.2973\tLR: 0.100000\nTraining Epoch: 18 [46208/50000]\tLoss: 3.4612\tLR: 0.100000\nTraining Epoch: 18 [46336/50000]\tLoss: 3.2186\tLR: 0.100000\nTraining Epoch: 18 [46464/50000]\tLoss: 3.4503\tLR: 0.100000\nTraining Epoch: 18 [46592/50000]\tLoss: 3.4809\tLR: 0.100000\nTraining Epoch: 18 [46720/50000]\tLoss: 3.5788\tLR: 0.100000\nTraining Epoch: 18 [46848/50000]\tLoss: 3.3944\tLR: 0.100000\nTraining Epoch: 18 [46976/50000]\tLoss: 3.2752\tLR: 0.100000\nTraining Epoch: 18 [47104/50000]\tLoss: 3.3838\tLR: 0.100000\nTraining Epoch: 18 [47232/50000]\tLoss: 3.5444\tLR: 0.100000\nTraining Epoch: 18 [47360/50000]\tLoss: 3.2769\tLR: 0.100000\nTraining Epoch: 18 [47488/50000]\tLoss: 3.2257\tLR: 0.100000\nTraining Epoch: 18 [47616/50000]\tLoss: 3.2545\tLR: 0.100000\nTraining Epoch: 18 [47744/50000]\tLoss: 3.3427\tLR: 0.100000\nTraining Epoch: 18 [47872/50000]\tLoss: 3.4872\tLR: 0.100000\nTraining Epoch: 18 [48000/50000]\tLoss: 3.1749\tLR: 0.100000\nTraining Epoch: 18 [48128/50000]\tLoss: 3.3497\tLR: 0.100000\nTraining Epoch: 18 [48256/50000]\tLoss: 3.2199\tLR: 0.100000\nTraining Epoch: 18 [48384/50000]\tLoss: 3.5987\tLR: 0.100000\nTraining Epoch: 18 [48512/50000]\tLoss: 3.4827\tLR: 0.100000\nTraining Epoch: 18 [48640/50000]\tLoss: 3.1726\tLR: 0.100000\nTraining Epoch: 18 [48768/50000]\tLoss: 3.4116\tLR: 0.100000\nTraining Epoch: 18 [48896/50000]\tLoss: 3.5207\tLR: 0.100000\nTraining Epoch: 18 [49024/50000]\tLoss: 3.1961\tLR: 0.100000\nTraining Epoch: 18 [49152/50000]\tLoss: 3.2546\tLR: 0.100000\nTraining Epoch: 18 [49280/50000]\tLoss: 3.3276\tLR: 0.100000\nTraining Epoch: 18 [49408/50000]\tLoss: 3.2922\tLR: 0.100000\nTraining Epoch: 18 [49536/50000]\tLoss: 3.3027\tLR: 0.100000\nTraining Epoch: 18 [49664/50000]\tLoss: 3.3669\tLR: 0.100000\nTraining Epoch: 18 [49792/50000]\tLoss: 3.1645\tLR: 0.100000\nTraining Epoch: 18 [49920/50000]\tLoss: 3.3893\tLR: 0.100000\nTraining Epoch: 18 [50000/50000]\tLoss: 3.3567\tLR: 0.100000\n=========================================================================================\n","name":"stdout"},{"output_type":"stream","text":"Training Epoch: 19 [128/50000]\tLoss: 3.4924\tLR: 0.100000\nTraining Epoch: 19 [256/50000]\tLoss: 3.2524\tLR: 0.100000\nTraining Epoch: 19 [384/50000]\tLoss: 3.3659\tLR: 0.100000\nTraining Epoch: 19 [512/50000]\tLoss: 3.2611\tLR: 0.100000\nTraining Epoch: 19 [640/50000]\tLoss: 3.1274\tLR: 0.100000\nTraining Epoch: 19 [768/50000]\tLoss: 3.4294\tLR: 0.100000\nTraining Epoch: 19 [896/50000]\tLoss: 3.1733\tLR: 0.100000\nTraining Epoch: 19 [1024/50000]\tLoss: 3.3354\tLR: 0.100000\nTraining Epoch: 19 [1152/50000]\tLoss: 3.2702\tLR: 0.100000\nTraining Epoch: 19 [1280/50000]\tLoss: 3.4619\tLR: 0.100000\nTraining Epoch: 19 [1408/50000]\tLoss: 3.1905\tLR: 0.100000\nTraining Epoch: 19 [1536/50000]\tLoss: 3.0028\tLR: 0.100000\nTraining Epoch: 19 [1664/50000]\tLoss: 3.3766\tLR: 0.100000\nTraining Epoch: 19 [1792/50000]\tLoss: 3.5848\tLR: 0.100000\nTraining Epoch: 19 [1920/50000]\tLoss: 3.2645\tLR: 0.100000\nTraining Epoch: 19 [2048/50000]\tLoss: 3.3430\tLR: 0.100000\nTraining Epoch: 19 [2176/50000]\tLoss: 3.1444\tLR: 0.100000\nTraining Epoch: 19 [2304/50000]\tLoss: 3.2188\tLR: 0.100000\nTraining Epoch: 19 [2432/50000]\tLoss: 3.2794\tLR: 0.100000\nTraining Epoch: 19 [2560/50000]\tLoss: 3.2968\tLR: 0.100000\nTraining Epoch: 19 [2688/50000]\tLoss: 3.2463\tLR: 0.100000\nTraining Epoch: 19 [2816/50000]\tLoss: 3.4246\tLR: 0.100000\nTraining Epoch: 19 [2944/50000]\tLoss: 3.1307\tLR: 0.100000\nTraining Epoch: 19 [3072/50000]\tLoss: 3.3521\tLR: 0.100000\nTraining Epoch: 19 [3200/50000]\tLoss: 3.3894\tLR: 0.100000\nTraining Epoch: 19 [3328/50000]\tLoss: 3.3061\tLR: 0.100000\nTraining Epoch: 19 [3456/50000]\tLoss: 3.1558\tLR: 0.100000\nTraining Epoch: 19 [3584/50000]\tLoss: 3.2987\tLR: 0.100000\nTraining Epoch: 19 [3712/50000]\tLoss: 3.1765\tLR: 0.100000\nTraining Epoch: 19 [3840/50000]\tLoss: 3.2892\tLR: 0.100000\nTraining Epoch: 19 [3968/50000]\tLoss: 3.4502\tLR: 0.100000\nTraining Epoch: 19 [4096/50000]\tLoss: 3.2951\tLR: 0.100000\nTraining Epoch: 19 [4224/50000]\tLoss: 3.3552\tLR: 0.100000\nTraining Epoch: 19 [4352/50000]\tLoss: 3.4602\tLR: 0.100000\nTraining Epoch: 19 [4480/50000]\tLoss: 3.3993\tLR: 0.100000\nTraining Epoch: 19 [4608/50000]\tLoss: 3.2519\tLR: 0.100000\nTraining Epoch: 19 [4736/50000]\tLoss: 2.8902\tLR: 0.100000\nTraining Epoch: 19 [4864/50000]\tLoss: 3.2739\tLR: 0.100000\nTraining Epoch: 19 [4992/50000]\tLoss: 3.2835\tLR: 0.100000\nTraining Epoch: 19 [5120/50000]\tLoss: 3.1804\tLR: 0.100000\nTraining Epoch: 19 [5248/50000]\tLoss: 3.3387\tLR: 0.100000\nTraining Epoch: 19 [5376/50000]\tLoss: 3.5098\tLR: 0.100000\nTraining Epoch: 19 [5504/50000]\tLoss: 3.2091\tLR: 0.100000\nTraining Epoch: 19 [5632/50000]\tLoss: 3.3595\tLR: 0.100000\nTraining Epoch: 19 [5760/50000]\tLoss: 3.3094\tLR: 0.100000\nTraining Epoch: 19 [5888/50000]\tLoss: 3.2458\tLR: 0.100000\nTraining Epoch: 19 [6016/50000]\tLoss: 3.1614\tLR: 0.100000\nTraining Epoch: 19 [6144/50000]\tLoss: 3.3116\tLR: 0.100000\nTraining Epoch: 19 [6272/50000]\tLoss: 3.3661\tLR: 0.100000\nTraining Epoch: 19 [6400/50000]\tLoss: 3.1165\tLR: 0.100000\nTraining Epoch: 19 [6528/50000]\tLoss: 3.3669\tLR: 0.100000\nTraining Epoch: 19 [6656/50000]\tLoss: 3.1986\tLR: 0.100000\nTraining Epoch: 19 [6784/50000]\tLoss: 3.1486\tLR: 0.100000\nTraining Epoch: 19 [6912/50000]\tLoss: 3.1841\tLR: 0.100000\nTraining Epoch: 19 [7040/50000]\tLoss: 3.3249\tLR: 0.100000\nTraining Epoch: 19 [7168/50000]\tLoss: 3.2482\tLR: 0.100000\nTraining Epoch: 19 [7296/50000]\tLoss: 3.3696\tLR: 0.100000\nTraining Epoch: 19 [7424/50000]\tLoss: 3.4709\tLR: 0.100000\nTraining Epoch: 19 [7552/50000]\tLoss: 3.2362\tLR: 0.100000\nTraining Epoch: 19 [7680/50000]\tLoss: 2.8735\tLR: 0.100000\nTraining Epoch: 19 [7808/50000]\tLoss: 3.4767\tLR: 0.100000\nTraining Epoch: 19 [7936/50000]\tLoss: 3.1506\tLR: 0.100000\nTraining Epoch: 19 [8064/50000]\tLoss: 3.2742\tLR: 0.100000\nTraining Epoch: 19 [8192/50000]\tLoss: 3.1065\tLR: 0.100000\nTraining Epoch: 19 [8320/50000]\tLoss: 2.8691\tLR: 0.100000\nTraining Epoch: 19 [8448/50000]\tLoss: 3.5982\tLR: 0.100000\nTraining Epoch: 19 [8576/50000]\tLoss: 3.3523\tLR: 0.100000\nTraining Epoch: 19 [8704/50000]\tLoss: 3.2891\tLR: 0.100000\nTraining Epoch: 19 [8832/50000]\tLoss: 3.1274\tLR: 0.100000\nTraining Epoch: 19 [8960/50000]\tLoss: 3.4164\tLR: 0.100000\nTraining Epoch: 19 [9088/50000]\tLoss: 3.4404\tLR: 0.100000\nTraining Epoch: 19 [9216/50000]\tLoss: 3.4181\tLR: 0.100000\nTraining Epoch: 19 [9344/50000]\tLoss: 3.1582\tLR: 0.100000\nTraining Epoch: 19 [9472/50000]\tLoss: 3.1296\tLR: 0.100000\nTraining Epoch: 19 [9600/50000]\tLoss: 3.2152\tLR: 0.100000\nTraining Epoch: 19 [9728/50000]\tLoss: 3.3158\tLR: 0.100000\nTraining Epoch: 19 [9856/50000]\tLoss: 3.3121\tLR: 0.100000\nTraining Epoch: 19 [9984/50000]\tLoss: 3.2765\tLR: 0.100000\nTraining Epoch: 19 [10112/50000]\tLoss: 3.2603\tLR: 0.100000\nTraining Epoch: 19 [10240/50000]\tLoss: 3.1803\tLR: 0.100000\nTraining Epoch: 19 [10368/50000]\tLoss: 3.2637\tLR: 0.100000\nTraining Epoch: 19 [10496/50000]\tLoss: 3.2434\tLR: 0.100000\nTraining Epoch: 19 [10624/50000]\tLoss: 3.1385\tLR: 0.100000\nTraining Epoch: 19 [10752/50000]\tLoss: 3.3784\tLR: 0.100000\nTraining Epoch: 19 [10880/50000]\tLoss: 3.6055\tLR: 0.100000\nTraining Epoch: 19 [11008/50000]\tLoss: 3.0608\tLR: 0.100000\nTraining Epoch: 19 [11136/50000]\tLoss: 3.0660\tLR: 0.100000\nTraining Epoch: 19 [11264/50000]\tLoss: 3.3530\tLR: 0.100000\nTraining Epoch: 19 [11392/50000]\tLoss: 3.1795\tLR: 0.100000\nTraining Epoch: 19 [11520/50000]\tLoss: 3.4243\tLR: 0.100000\nTraining Epoch: 19 [11648/50000]\tLoss: 3.6877\tLR: 0.100000\nTraining Epoch: 19 [11776/50000]\tLoss: 3.4552\tLR: 0.100000\nTraining Epoch: 19 [11904/50000]\tLoss: 3.1753\tLR: 0.100000\nTraining Epoch: 19 [12032/50000]\tLoss: 3.2824\tLR: 0.100000\nTraining Epoch: 19 [12160/50000]\tLoss: 3.2389\tLR: 0.100000\nTraining Epoch: 19 [12288/50000]\tLoss: 3.3334\tLR: 0.100000\nTraining Epoch: 19 [12416/50000]\tLoss: 3.3677\tLR: 0.100000\nTraining Epoch: 19 [12544/50000]\tLoss: 3.3979\tLR: 0.100000\nTraining Epoch: 19 [12672/50000]\tLoss: 3.2996\tLR: 0.100000\nTraining Epoch: 19 [12800/50000]\tLoss: 3.3740\tLR: 0.100000\nTraining Epoch: 19 [12928/50000]\tLoss: 3.5893\tLR: 0.100000\nTraining Epoch: 19 [13056/50000]\tLoss: 3.4303\tLR: 0.100000\nTraining Epoch: 19 [13184/50000]\tLoss: 3.3207\tLR: 0.100000\nTraining Epoch: 19 [13312/50000]\tLoss: 3.2241\tLR: 0.100000\nTraining Epoch: 19 [13440/50000]\tLoss: 3.1715\tLR: 0.100000\nTraining Epoch: 19 [13568/50000]\tLoss: 3.3030\tLR: 0.100000\nTraining Epoch: 19 [13696/50000]\tLoss: 3.1564\tLR: 0.100000\nTraining Epoch: 19 [13824/50000]\tLoss: 3.5072\tLR: 0.100000\nTraining Epoch: 19 [13952/50000]\tLoss: 3.2611\tLR: 0.100000\nTraining Epoch: 19 [14080/50000]\tLoss: 3.3478\tLR: 0.100000\nTraining Epoch: 19 [14208/50000]\tLoss: 3.4528\tLR: 0.100000\nTraining Epoch: 19 [14336/50000]\tLoss: 3.1116\tLR: 0.100000\nTraining Epoch: 19 [14464/50000]\tLoss: 3.2081\tLR: 0.100000\nTraining Epoch: 19 [14592/50000]\tLoss: 3.3718\tLR: 0.100000\nTraining Epoch: 19 [14720/50000]\tLoss: 3.2202\tLR: 0.100000\nTraining Epoch: 19 [14848/50000]\tLoss: 3.2337\tLR: 0.100000\nTraining Epoch: 19 [14976/50000]\tLoss: 3.1602\tLR: 0.100000\nTraining Epoch: 19 [15104/50000]\tLoss: 3.2680\tLR: 0.100000\nTraining Epoch: 19 [15232/50000]\tLoss: 3.1749\tLR: 0.100000\nTraining Epoch: 19 [15360/50000]\tLoss: 3.4237\tLR: 0.100000\nTraining Epoch: 19 [15488/50000]\tLoss: 3.3497\tLR: 0.100000\nTraining Epoch: 19 [15616/50000]\tLoss: 3.4446\tLR: 0.100000\nTraining Epoch: 19 [15744/50000]\tLoss: 3.1594\tLR: 0.100000\nTraining Epoch: 19 [15872/50000]\tLoss: 3.2355\tLR: 0.100000\nTraining Epoch: 19 [16000/50000]\tLoss: 3.3538\tLR: 0.100000\nTraining Epoch: 19 [16128/50000]\tLoss: 3.2486\tLR: 0.100000\nTraining Epoch: 19 [16256/50000]\tLoss: 2.9109\tLR: 0.100000\nTraining Epoch: 19 [16384/50000]\tLoss: 3.3087\tLR: 0.100000\nTraining Epoch: 19 [16512/50000]\tLoss: 3.2358\tLR: 0.100000\nTraining Epoch: 19 [16640/50000]\tLoss: 3.4852\tLR: 0.100000\nTraining Epoch: 19 [16768/50000]\tLoss: 3.4122\tLR: 0.100000\nTraining Epoch: 19 [16896/50000]\tLoss: 3.1982\tLR: 0.100000\nTraining Epoch: 19 [17024/50000]\tLoss: 3.1741\tLR: 0.100000\nTraining Epoch: 19 [17152/50000]\tLoss: 3.0946\tLR: 0.100000\nTraining Epoch: 19 [17280/50000]\tLoss: 3.2935\tLR: 0.100000\nTraining Epoch: 19 [17408/50000]\tLoss: 3.4273\tLR: 0.100000\nTraining Epoch: 19 [17536/50000]\tLoss: 3.5409\tLR: 0.100000\nTraining Epoch: 19 [17664/50000]\tLoss: 3.2942\tLR: 0.100000\nTraining Epoch: 19 [17792/50000]\tLoss: 3.1920\tLR: 0.100000\nTraining Epoch: 19 [17920/50000]\tLoss: 3.4627\tLR: 0.100000\nTraining Epoch: 19 [18048/50000]\tLoss: 3.1878\tLR: 0.100000\n","name":"stdout"},{"output_type":"stream","text":"Training Epoch: 19 [18176/50000]\tLoss: 3.1881\tLR: 0.100000\nTraining Epoch: 19 [18304/50000]\tLoss: 3.1547\tLR: 0.100000\nTraining Epoch: 19 [18432/50000]\tLoss: 3.4728\tLR: 0.100000\nTraining Epoch: 19 [18560/50000]\tLoss: 3.3015\tLR: 0.100000\nTraining Epoch: 19 [18688/50000]\tLoss: 3.3425\tLR: 0.100000\nTraining Epoch: 19 [18816/50000]\tLoss: 3.2140\tLR: 0.100000\nTraining Epoch: 19 [18944/50000]\tLoss: 3.2822\tLR: 0.100000\nTraining Epoch: 19 [19072/50000]\tLoss: 3.3108\tLR: 0.100000\nTraining Epoch: 19 [19200/50000]\tLoss: 3.2237\tLR: 0.100000\nTraining Epoch: 19 [19328/50000]\tLoss: 3.3341\tLR: 0.100000\nTraining Epoch: 19 [19456/50000]\tLoss: 3.3864\tLR: 0.100000\nTraining Epoch: 19 [19584/50000]\tLoss: 3.2193\tLR: 0.100000\nTraining Epoch: 19 [19712/50000]\tLoss: 3.1483\tLR: 0.100000\nTraining Epoch: 19 [19840/50000]\tLoss: 3.5249\tLR: 0.100000\nTraining Epoch: 19 [19968/50000]\tLoss: 3.2981\tLR: 0.100000\nTraining Epoch: 19 [20096/50000]\tLoss: 3.4285\tLR: 0.100000\nTraining Epoch: 19 [20224/50000]\tLoss: 3.2310\tLR: 0.100000\nTraining Epoch: 19 [20352/50000]\tLoss: 3.5834\tLR: 0.100000\nTraining Epoch: 19 [20480/50000]\tLoss: 3.4969\tLR: 0.100000\nTraining Epoch: 19 [20608/50000]\tLoss: 3.4045\tLR: 0.100000\nTraining Epoch: 19 [20736/50000]\tLoss: 3.2173\tLR: 0.100000\nTraining Epoch: 19 [20864/50000]\tLoss: 3.1988\tLR: 0.100000\nTraining Epoch: 19 [20992/50000]\tLoss: 3.1746\tLR: 0.100000\nTraining Epoch: 19 [21120/50000]\tLoss: 3.0765\tLR: 0.100000\nTraining Epoch: 19 [21248/50000]\tLoss: 3.2498\tLR: 0.100000\nTraining Epoch: 19 [21376/50000]\tLoss: 3.3625\tLR: 0.100000\nTraining Epoch: 19 [21504/50000]\tLoss: 3.3971\tLR: 0.100000\nTraining Epoch: 19 [21632/50000]\tLoss: 3.0462\tLR: 0.100000\nTraining Epoch: 19 [21760/50000]\tLoss: 3.3799\tLR: 0.100000\nTraining Epoch: 19 [21888/50000]\tLoss: 3.4103\tLR: 0.100000\nTraining Epoch: 19 [22016/50000]\tLoss: 3.2720\tLR: 0.100000\nTraining Epoch: 19 [22144/50000]\tLoss: 3.3127\tLR: 0.100000\nTraining Epoch: 19 [22272/50000]\tLoss: 3.6033\tLR: 0.100000\nTraining Epoch: 19 [22400/50000]\tLoss: 3.0682\tLR: 0.100000\nTraining Epoch: 19 [22528/50000]\tLoss: 3.2740\tLR: 0.100000\nTraining Epoch: 19 [22656/50000]\tLoss: 3.4458\tLR: 0.100000\nTraining Epoch: 19 [22784/50000]\tLoss: 3.1653\tLR: 0.100000\nTraining Epoch: 19 [22912/50000]\tLoss: 3.3540\tLR: 0.100000\nTraining Epoch: 19 [23040/50000]\tLoss: 3.3700\tLR: 0.100000\nTraining Epoch: 19 [23168/50000]\tLoss: 3.1745\tLR: 0.100000\nTraining Epoch: 19 [23296/50000]\tLoss: 3.0675\tLR: 0.100000\nTraining Epoch: 19 [23424/50000]\tLoss: 3.5118\tLR: 0.100000\nTraining Epoch: 19 [23552/50000]\tLoss: 3.4004\tLR: 0.100000\nTraining Epoch: 19 [23680/50000]\tLoss: 3.1488\tLR: 0.100000\nTraining Epoch: 19 [23808/50000]\tLoss: 3.3678\tLR: 0.100000\nTraining Epoch: 19 [23936/50000]\tLoss: 3.5521\tLR: 0.100000\nTraining Epoch: 19 [24064/50000]\tLoss: 3.2711\tLR: 0.100000\nTraining Epoch: 19 [24192/50000]\tLoss: 3.5234\tLR: 0.100000\nTraining Epoch: 19 [24320/50000]\tLoss: 3.4814\tLR: 0.100000\nTraining Epoch: 19 [24448/50000]\tLoss: 3.4140\tLR: 0.100000\nTraining Epoch: 19 [24576/50000]\tLoss: 3.2165\tLR: 0.100000\nTraining Epoch: 19 [24704/50000]\tLoss: 3.0559\tLR: 0.100000\nTraining Epoch: 19 [24832/50000]\tLoss: 3.3190\tLR: 0.100000\nTraining Epoch: 19 [24960/50000]\tLoss: 3.3072\tLR: 0.100000\nTraining Epoch: 19 [25088/50000]\tLoss: 3.1644\tLR: 0.100000\nTraining Epoch: 19 [25216/50000]\tLoss: 3.4214\tLR: 0.100000\nTraining Epoch: 19 [25344/50000]\tLoss: 3.3957\tLR: 0.100000\nTraining Epoch: 19 [25472/50000]\tLoss: 3.1692\tLR: 0.100000\nTraining Epoch: 19 [25600/50000]\tLoss: 3.0518\tLR: 0.100000\nTraining Epoch: 19 [25728/50000]\tLoss: 3.3010\tLR: 0.100000\nTraining Epoch: 19 [25856/50000]\tLoss: 3.2572\tLR: 0.100000\nTraining Epoch: 19 [25984/50000]\tLoss: 3.0916\tLR: 0.100000\nTraining Epoch: 19 [26112/50000]\tLoss: 3.4435\tLR: 0.100000\nTraining Epoch: 19 [26240/50000]\tLoss: 3.3313\tLR: 0.100000\nTraining Epoch: 19 [26368/50000]\tLoss: 3.3370\tLR: 0.100000\nTraining Epoch: 19 [26496/50000]\tLoss: 3.3238\tLR: 0.100000\nTraining Epoch: 19 [26624/50000]\tLoss: 3.3153\tLR: 0.100000\nTraining Epoch: 19 [26752/50000]\tLoss: 3.2481\tLR: 0.100000\nTraining Epoch: 19 [26880/50000]\tLoss: 3.2508\tLR: 0.100000\nTraining Epoch: 19 [27008/50000]\tLoss: 3.2139\tLR: 0.100000\nTraining Epoch: 19 [27136/50000]\tLoss: 3.3329\tLR: 0.100000\nTraining Epoch: 19 [27264/50000]\tLoss: 3.3879\tLR: 0.100000\nTraining Epoch: 19 [27392/50000]\tLoss: 3.2004\tLR: 0.100000\nTraining Epoch: 19 [27520/50000]\tLoss: 3.5845\tLR: 0.100000\nTraining Epoch: 19 [27648/50000]\tLoss: 3.2549\tLR: 0.100000\nTraining Epoch: 19 [27776/50000]\tLoss: 3.3127\tLR: 0.100000\nTraining Epoch: 19 [27904/50000]\tLoss: 3.5134\tLR: 0.100000\nTraining Epoch: 19 [28032/50000]\tLoss: 3.3815\tLR: 0.100000\nTraining Epoch: 19 [28160/50000]\tLoss: 3.4287\tLR: 0.100000\nTraining Epoch: 19 [28288/50000]\tLoss: 3.0569\tLR: 0.100000\nTraining Epoch: 19 [28416/50000]\tLoss: 3.2370\tLR: 0.100000\nTraining Epoch: 19 [28544/50000]\tLoss: 3.4238\tLR: 0.100000\nTraining Epoch: 19 [28672/50000]\tLoss: 3.2707\tLR: 0.100000\nTraining Epoch: 19 [28800/50000]\tLoss: 3.0874\tLR: 0.100000\nTraining Epoch: 19 [28928/50000]\tLoss: 3.3132\tLR: 0.100000\nTraining Epoch: 19 [29056/50000]\tLoss: 3.2422\tLR: 0.100000\nTraining Epoch: 19 [29184/50000]\tLoss: 3.2958\tLR: 0.100000\nTraining Epoch: 19 [29312/50000]\tLoss: 3.2026\tLR: 0.100000\nTraining Epoch: 19 [29440/50000]\tLoss: 3.3571\tLR: 0.100000\nTraining Epoch: 19 [29568/50000]\tLoss: 3.2053\tLR: 0.100000\nTraining Epoch: 19 [29696/50000]\tLoss: 3.5689\tLR: 0.100000\nTraining Epoch: 19 [29824/50000]\tLoss: 3.0432\tLR: 0.100000\nTraining Epoch: 19 [29952/50000]\tLoss: 3.1199\tLR: 0.100000\nTraining Epoch: 19 [30080/50000]\tLoss: 3.3429\tLR: 0.100000\nTraining Epoch: 19 [30208/50000]\tLoss: 3.2970\tLR: 0.100000\nTraining Epoch: 19 [30336/50000]\tLoss: 3.5087\tLR: 0.100000\nTraining Epoch: 19 [30464/50000]\tLoss: 3.0547\tLR: 0.100000\nTraining Epoch: 19 [30592/50000]\tLoss: 3.1881\tLR: 0.100000\nTraining Epoch: 19 [30720/50000]\tLoss: 3.3954\tLR: 0.100000\nTraining Epoch: 19 [30848/50000]\tLoss: 3.3817\tLR: 0.100000\nTraining Epoch: 19 [30976/50000]\tLoss: 3.5198\tLR: 0.100000\nTraining Epoch: 19 [31104/50000]\tLoss: 3.5051\tLR: 0.100000\nTraining Epoch: 19 [31232/50000]\tLoss: 3.2918\tLR: 0.100000\nTraining Epoch: 19 [31360/50000]\tLoss: 3.0252\tLR: 0.100000\nTraining Epoch: 19 [31488/50000]\tLoss: 3.1929\tLR: 0.100000\nTraining Epoch: 19 [31616/50000]\tLoss: 3.2511\tLR: 0.100000\nTraining Epoch: 19 [31744/50000]\tLoss: 3.1907\tLR: 0.100000\nTraining Epoch: 19 [31872/50000]\tLoss: 3.4353\tLR: 0.100000\nTraining Epoch: 19 [32000/50000]\tLoss: 3.3177\tLR: 0.100000\nTraining Epoch: 19 [32128/50000]\tLoss: 3.3213\tLR: 0.100000\nTraining Epoch: 19 [32256/50000]\tLoss: 3.2439\tLR: 0.100000\nTraining Epoch: 19 [32384/50000]\tLoss: 3.3634\tLR: 0.100000\nTraining Epoch: 19 [32512/50000]\tLoss: 3.1021\tLR: 0.100000\nTraining Epoch: 19 [32640/50000]\tLoss: 3.3292\tLR: 0.100000\nTraining Epoch: 19 [32768/50000]\tLoss: 3.1713\tLR: 0.100000\nTraining Epoch: 19 [32896/50000]\tLoss: 3.0927\tLR: 0.100000\nTraining Epoch: 19 [33024/50000]\tLoss: 3.0544\tLR: 0.100000\nTraining Epoch: 19 [33152/50000]\tLoss: 3.2337\tLR: 0.100000\nTraining Epoch: 19 [33280/50000]\tLoss: 3.2202\tLR: 0.100000\nTraining Epoch: 19 [33408/50000]\tLoss: 3.2263\tLR: 0.100000\nTraining Epoch: 19 [33536/50000]\tLoss: 3.2929\tLR: 0.100000\nTraining Epoch: 19 [33664/50000]\tLoss: 3.3694\tLR: 0.100000\nTraining Epoch: 19 [33792/50000]\tLoss: 3.3003\tLR: 0.100000\nTraining Epoch: 19 [33920/50000]\tLoss: 3.4207\tLR: 0.100000\nTraining Epoch: 19 [34048/50000]\tLoss: 3.4427\tLR: 0.100000\nTraining Epoch: 19 [34176/50000]\tLoss: 3.2055\tLR: 0.100000\nTraining Epoch: 19 [34304/50000]\tLoss: 3.2000\tLR: 0.100000\nTraining Epoch: 19 [34432/50000]\tLoss: 3.1930\tLR: 0.100000\nTraining Epoch: 19 [34560/50000]\tLoss: 3.4004\tLR: 0.100000\nTraining Epoch: 19 [34688/50000]\tLoss: 3.3095\tLR: 0.100000\nTraining Epoch: 19 [34816/50000]\tLoss: 3.3294\tLR: 0.100000\nTraining Epoch: 19 [34944/50000]\tLoss: 3.4693\tLR: 0.100000\nTraining Epoch: 19 [35072/50000]\tLoss: 3.2688\tLR: 0.100000\nTraining Epoch: 19 [35200/50000]\tLoss: 3.2073\tLR: 0.100000\nTraining Epoch: 19 [35328/50000]\tLoss: 3.4719\tLR: 0.100000\nTraining Epoch: 19 [35456/50000]\tLoss: 3.4254\tLR: 0.100000\nTraining Epoch: 19 [35584/50000]\tLoss: 3.3606\tLR: 0.100000\nTraining Epoch: 19 [35712/50000]\tLoss: 3.4785\tLR: 0.100000\nTraining Epoch: 19 [35840/50000]\tLoss: 3.4325\tLR: 0.100000\nTraining Epoch: 19 [35968/50000]\tLoss: 3.0739\tLR: 0.100000\nTraining Epoch: 19 [36096/50000]\tLoss: 3.5371\tLR: 0.100000\n","name":"stdout"},{"output_type":"stream","text":"Training Epoch: 19 [36224/50000]\tLoss: 3.4590\tLR: 0.100000\nTraining Epoch: 19 [36352/50000]\tLoss: 3.2737\tLR: 0.100000\nTraining Epoch: 19 [36480/50000]\tLoss: 3.2493\tLR: 0.100000\nTraining Epoch: 19 [36608/50000]\tLoss: 3.3453\tLR: 0.100000\nTraining Epoch: 19 [36736/50000]\tLoss: 3.2011\tLR: 0.100000\nTraining Epoch: 19 [36864/50000]\tLoss: 3.4326\tLR: 0.100000\nTraining Epoch: 19 [36992/50000]\tLoss: 3.2853\tLR: 0.100000\nTraining Epoch: 19 [37120/50000]\tLoss: 3.0976\tLR: 0.100000\nTraining Epoch: 19 [37248/50000]\tLoss: 3.3225\tLR: 0.100000\nTraining Epoch: 19 [37376/50000]\tLoss: 3.3932\tLR: 0.100000\nTraining Epoch: 19 [37504/50000]\tLoss: 3.3562\tLR: 0.100000\nTraining Epoch: 19 [37632/50000]\tLoss: 3.3791\tLR: 0.100000\nTraining Epoch: 19 [37760/50000]\tLoss: 3.1925\tLR: 0.100000\nTraining Epoch: 19 [37888/50000]\tLoss: 3.5126\tLR: 0.100000\nTraining Epoch: 19 [38016/50000]\tLoss: 3.4164\tLR: 0.100000\nTraining Epoch: 19 [38144/50000]\tLoss: 3.1567\tLR: 0.100000\nTraining Epoch: 19 [38272/50000]\tLoss: 3.0517\tLR: 0.100000\nTraining Epoch: 19 [38400/50000]\tLoss: 3.3842\tLR: 0.100000\nTraining Epoch: 19 [38528/50000]\tLoss: 3.2671\tLR: 0.100000\nTraining Epoch: 19 [38656/50000]\tLoss: 3.2176\tLR: 0.100000\nTraining Epoch: 19 [38784/50000]\tLoss: 3.3233\tLR: 0.100000\nTraining Epoch: 19 [38912/50000]\tLoss: 3.3374\tLR: 0.100000\nTraining Epoch: 19 [39040/50000]\tLoss: 3.3292\tLR: 0.100000\nTraining Epoch: 19 [39168/50000]\tLoss: 3.3875\tLR: 0.100000\nTraining Epoch: 19 [39296/50000]\tLoss: 3.2149\tLR: 0.100000\nTraining Epoch: 19 [39424/50000]\tLoss: 3.0757\tLR: 0.100000\nTraining Epoch: 19 [39552/50000]\tLoss: 3.1288\tLR: 0.100000\nTraining Epoch: 19 [39680/50000]\tLoss: 3.1686\tLR: 0.100000\nTraining Epoch: 19 [39808/50000]\tLoss: 3.0739\tLR: 0.100000\nTraining Epoch: 19 [39936/50000]\tLoss: 3.4526\tLR: 0.100000\nTraining Epoch: 19 [40064/50000]\tLoss: 3.2158\tLR: 0.100000\nTraining Epoch: 19 [40192/50000]\tLoss: 3.2198\tLR: 0.100000\nTraining Epoch: 19 [40320/50000]\tLoss: 3.3143\tLR: 0.100000\nTraining Epoch: 19 [40448/50000]\tLoss: 3.5843\tLR: 0.100000\nTraining Epoch: 19 [40576/50000]\tLoss: 3.3285\tLR: 0.100000\nTraining Epoch: 19 [40704/50000]\tLoss: 3.2111\tLR: 0.100000\nTraining Epoch: 19 [40832/50000]\tLoss: 3.2801\tLR: 0.100000\nTraining Epoch: 19 [40960/50000]\tLoss: 3.4566\tLR: 0.100000\nTraining Epoch: 19 [41088/50000]\tLoss: 3.2365\tLR: 0.100000\nTraining Epoch: 19 [41216/50000]\tLoss: 3.2119\tLR: 0.100000\nTraining Epoch: 19 [41344/50000]\tLoss: 3.2294\tLR: 0.100000\nTraining Epoch: 19 [41472/50000]\tLoss: 3.3521\tLR: 0.100000\nTraining Epoch: 19 [41600/50000]\tLoss: 3.2296\tLR: 0.100000\nTraining Epoch: 19 [41728/50000]\tLoss: 3.3545\tLR: 0.100000\nTraining Epoch: 19 [41856/50000]\tLoss: 3.2405\tLR: 0.100000\nTraining Epoch: 19 [41984/50000]\tLoss: 3.3682\tLR: 0.100000\nTraining Epoch: 19 [42112/50000]\tLoss: 3.2786\tLR: 0.100000\nTraining Epoch: 19 [42240/50000]\tLoss: 3.5548\tLR: 0.100000\nTraining Epoch: 19 [42368/50000]\tLoss: 3.2424\tLR: 0.100000\nTraining Epoch: 19 [42496/50000]\tLoss: 3.4875\tLR: 0.100000\nTraining Epoch: 19 [42624/50000]\tLoss: 3.1948\tLR: 0.100000\nTraining Epoch: 19 [42752/50000]\tLoss: 3.3260\tLR: 0.100000\nTraining Epoch: 19 [42880/50000]\tLoss: 3.0499\tLR: 0.100000\nTraining Epoch: 19 [43008/50000]\tLoss: 3.0319\tLR: 0.100000\nTraining Epoch: 19 [43136/50000]\tLoss: 3.0559\tLR: 0.100000\nTraining Epoch: 19 [43264/50000]\tLoss: 3.3049\tLR: 0.100000\nTraining Epoch: 19 [43392/50000]\tLoss: 3.0380\tLR: 0.100000\nTraining Epoch: 19 [43520/50000]\tLoss: 3.3459\tLR: 0.100000\nTraining Epoch: 19 [43648/50000]\tLoss: 3.1659\tLR: 0.100000\nTraining Epoch: 19 [43776/50000]\tLoss: 3.3997\tLR: 0.100000\nTraining Epoch: 19 [43904/50000]\tLoss: 3.1522\tLR: 0.100000\nTraining Epoch: 19 [44032/50000]\tLoss: 3.0967\tLR: 0.100000\nTraining Epoch: 19 [44160/50000]\tLoss: 3.2417\tLR: 0.100000\nTraining Epoch: 19 [44288/50000]\tLoss: 3.1161\tLR: 0.100000\nTraining Epoch: 19 [44416/50000]\tLoss: 3.0412\tLR: 0.100000\nTraining Epoch: 19 [44544/50000]\tLoss: 3.0334\tLR: 0.100000\nTraining Epoch: 19 [44672/50000]\tLoss: 3.4565\tLR: 0.100000\nTraining Epoch: 19 [44800/50000]\tLoss: 3.2680\tLR: 0.100000\nTraining Epoch: 19 [44928/50000]\tLoss: 3.2318\tLR: 0.100000\nTraining Epoch: 19 [45056/50000]\tLoss: 3.2841\tLR: 0.100000\nTraining Epoch: 19 [45184/50000]\tLoss: 3.3719\tLR: 0.100000\nTraining Epoch: 19 [45312/50000]\tLoss: 3.1576\tLR: 0.100000\nTraining Epoch: 19 [45440/50000]\tLoss: 3.3200\tLR: 0.100000\nTraining Epoch: 19 [45568/50000]\tLoss: 3.2817\tLR: 0.100000\nTraining Epoch: 19 [45696/50000]\tLoss: 3.3597\tLR: 0.100000\nTraining Epoch: 19 [45824/50000]\tLoss: 3.1732\tLR: 0.100000\nTraining Epoch: 19 [45952/50000]\tLoss: 3.5355\tLR: 0.100000\nTraining Epoch: 19 [46080/50000]\tLoss: 3.4281\tLR: 0.100000\nTraining Epoch: 19 [46208/50000]\tLoss: 3.4158\tLR: 0.100000\nTraining Epoch: 19 [46336/50000]\tLoss: 3.2197\tLR: 0.100000\nTraining Epoch: 19 [46464/50000]\tLoss: 3.4281\tLR: 0.100000\nTraining Epoch: 19 [46592/50000]\tLoss: 3.5425\tLR: 0.100000\nTraining Epoch: 19 [46720/50000]\tLoss: 3.3344\tLR: 0.100000\nTraining Epoch: 19 [46848/50000]\tLoss: 3.2953\tLR: 0.100000\nTraining Epoch: 19 [46976/50000]\tLoss: 3.3135\tLR: 0.100000\nTraining Epoch: 19 [47104/50000]\tLoss: 3.6328\tLR: 0.100000\nTraining Epoch: 19 [47232/50000]\tLoss: 3.2764\tLR: 0.100000\nTraining Epoch: 19 [47360/50000]\tLoss: 3.2534\tLR: 0.100000\nTraining Epoch: 19 [47488/50000]\tLoss: 3.2924\tLR: 0.100000\nTraining Epoch: 19 [47616/50000]\tLoss: 3.3046\tLR: 0.100000\nTraining Epoch: 19 [47744/50000]\tLoss: 3.4687\tLR: 0.100000\nTraining Epoch: 19 [47872/50000]\tLoss: 3.2703\tLR: 0.100000\nTraining Epoch: 19 [48000/50000]\tLoss: 3.0477\tLR: 0.100000\nTraining Epoch: 19 [48128/50000]\tLoss: 3.5153\tLR: 0.100000\nTraining Epoch: 19 [48256/50000]\tLoss: 3.3005\tLR: 0.100000\nTraining Epoch: 19 [48384/50000]\tLoss: 3.3459\tLR: 0.100000\nTraining Epoch: 19 [48512/50000]\tLoss: 3.3012\tLR: 0.100000\nTraining Epoch: 19 [48640/50000]\tLoss: 3.0530\tLR: 0.100000\nTraining Epoch: 19 [48768/50000]\tLoss: 3.4037\tLR: 0.100000\nTraining Epoch: 19 [48896/50000]\tLoss: 3.3479\tLR: 0.100000\nTraining Epoch: 19 [49024/50000]\tLoss: 3.2822\tLR: 0.100000\nTraining Epoch: 19 [49152/50000]\tLoss: 3.1909\tLR: 0.100000\nTraining Epoch: 19 [49280/50000]\tLoss: 3.3745\tLR: 0.100000\nTraining Epoch: 19 [49408/50000]\tLoss: 3.4960\tLR: 0.100000\nTraining Epoch: 19 [49536/50000]\tLoss: 3.4732\tLR: 0.100000\nTraining Epoch: 19 [49664/50000]\tLoss: 3.3217\tLR: 0.100000\nTraining Epoch: 19 [49792/50000]\tLoss: 3.2013\tLR: 0.100000\nTraining Epoch: 19 [49920/50000]\tLoss: 3.3733\tLR: 0.100000\nTraining Epoch: 19 [50000/50000]\tLoss: 3.6052\tLR: 0.100000\nFinished in 287.5993835926056\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<h3>Results</h3>\nTrained for up to 20 epochs on Kaggle. Time taken ~ 9 minutes"},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save({\n            'epoch': epoch+1,\n            'model_state_dict': net.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            }, 'weights_googlenet_20ep.pth')\nprint('saved')","execution_count":85,"outputs":[{"output_type":"stream","text":"saved\n","name":"stdout"}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}