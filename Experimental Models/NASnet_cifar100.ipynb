{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>NASnet-A implementation</h2>\n",
    "\n",
    "<p>Training NASnet on CIFAR-100.<p>\n",
    "<pre>\n",
    "<i>[1] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, Quoc V. Le,\n",
    "   \"Learning Transferable Architectures for Scalable Image Recognition\"\n",
    "   <a href=\"https://arxiv.org/abs/1707.07012\">https://arxiv.org/abs/1707.07012</a>\n",
    "</i>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchvision import datasets,transforms, models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import _LRScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Hyperparameter Settings</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 128\n",
    "max_epochs = 10\n",
    "l_r = 0.1\n",
    "w = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Loading and preprocessing data:</h3>\n",
    "<p>Let us calculate the mean and standard deviation of the dataset. We'll use this in the transforms later.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_std(dataset):\n",
    "    \n",
    "    data_r = np.dstack([np.array(im)[:, :, 0]/255 for im,label in dataset])\n",
    "    data_g = np.dstack([np.array(im)[:, :, 1]/255 for im,label in dataset])\n",
    "    data_b = np.dstack([np.array(im)[:, :, 2]/255 for im,label in dataset])\n",
    "    \n",
    "    mean = [np.asscalar(np.mean(data_r)), np.asscalar(np.mean(data_g)), np.asscalar(np.mean(data_b))]\n",
    "    std = [np.asscalar(np.std(data_r)), np.asscalar(np.std(data_g)), np.asscalar(np.std(data_b))]\n",
    "    \n",
    "    return mean,std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5070751592371341, 0.48654887331495067, 0.4409178433670344] [0.2673342858792403, 0.2564384629170882, 0.27615047132568393]\n",
      "[0.508796412760417, 0.48739301317401906, 0.4419422112438727] [0.2682515741720801, 0.25736373644781246, 0.2770957707973041]\n"
     ]
    }
   ],
   "source": [
    "#cifar_norm_train=datasets.CIFAR100(data_dir, train=True, transform=None, target_transform=None, download=False)\n",
    "#cifar_norm_test=datasets.CIFAR100(data_dir, train=False, transform=None, target_transform=None, download=False)\n",
    "#train_mean,train_std = compute_mean_std(cifar_norm_train)\n",
    "#test_mean, test_std = compute_mean_std(cifar_norm_test)\n",
    "#print(train_mean,train_std)\n",
    "#print(test_mean,test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No need to calculate this multiple times, by saving the mean/std values\n",
    "train_mean,train_std = [0.5070751592371341, 0.48654887331495067, 0.4409178433670344],[0.2673342858792403, 0.2564384629170882, 0.27615047132568393]\n",
    "test_mean, test_std = [0.508796412760417, 0.48739301317401906, 0.4419422112438727],[0.2682515741720801, 0.25736373644781246, 0.2770957707973041]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding with 4 zeros, taking 32 by 32 crops, random flipping and normalization by values found above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([transforms.Pad(4,fill=0),\n",
    "                                     transforms.RandomResizedCrop(32),\n",
    "                                     transforms.RandomHorizontalFlip(),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(train_mean,train_std)])\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.ToTensor(),\n",
    "                                     transforms.Normalize(test_mean,test_std)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir='../input/cifar-100-python/'\n",
    "cifar_train=datasets.CIFAR100(data_dir, train=True, transform=train_transforms, target_transform=None, download=False)\n",
    "cifar_test=datasets.CIFAR100(data_dir, train=False, transform=test_transforms, target_transform=None, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(cifar_train, batch_size=batchsize, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(cifar_test, batch_size=batchsize, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Visualizing CIFAR</h3>\n",
    "<p>Just to see what's actually in there.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for inputs,labels in testloader:\n",
    "    break\n",
    "images = inputs.numpy()\n",
    "for i in range(3):\n",
    "    plt.figure()\n",
    "    plt.imshow(images[i].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>NASnet Class Implementation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeperableConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, input_channels, output_channels, kernel_size, **kwargs):\n",
    "\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv2d(\n",
    "            input_channels,\n",
    "            input_channels,\n",
    "            kernel_size,\n",
    "            groups=input_channels,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        self.pointwise = nn.Conv2d(\n",
    "            input_channels,\n",
    "            output_channels,\n",
    "            1\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class SeperableBranch(nn.Module):\n",
    "\n",
    "    def __init__(self, input_channels, output_channels, kernel_size, **kwargs):\n",
    "        \"\"\"Adds 2 blocks of [relu-separable conv-batchnorm].\"\"\"\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            SeperableConv2d(input_channels, output_channels, kernel_size, **kwargs),\n",
    "            nn.BatchNorm2d(output_channels)\n",
    "        )\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            SeperableConv2d(output_channels, output_channels, kernel_size, stride=1, padding=int(kernel_size / 2)),\n",
    "            nn.BatchNorm2d(output_channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Fit(nn.Module):\n",
    "    \"\"\"Make the cell outputs compatible\n",
    "\n",
    "    Args:\n",
    "        prev_filters: filter number of tensor prev, needs to be modified\n",
    "        filters: filter number of normal cell branch output filters\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, prev_filters, filters):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.p1 = nn.Sequential(\n",
    "            nn.AvgPool2d(1, stride=2),\n",
    "            nn.Conv2d(prev_filters, int(filters / 2), 1)\n",
    "        )\n",
    "\n",
    "        #make sure there is no information loss\n",
    "        self.p2 = nn.Sequential(\n",
    "            nn.ConstantPad2d((0, 1, 0, 1), 0),\n",
    "            nn.ConstantPad2d((-1, 0, -1, 0), 0),   #cropping\n",
    "            nn.AvgPool2d(1, stride=2),\n",
    "            nn.Conv2d(prev_filters, int(filters / 2), 1)\n",
    "        )\n",
    "\n",
    "        self.bn = nn.BatchNorm2d(filters)\n",
    "\n",
    "        self.dim_reduce = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(prev_filters, filters, 1),\n",
    "            nn.BatchNorm2d(filters)\n",
    "        )\n",
    "\n",
    "        self.filters = filters\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        x, prev = inputs\n",
    "        if prev is None:\n",
    "            return x\n",
    "\n",
    "        #image size does not match\n",
    "        elif x.size(2) != prev.size(2):\n",
    "            prev = self.relu(prev)\n",
    "            p1 = self.p1(prev)\n",
    "            p2 = self.p2(prev)\n",
    "            prev = torch.cat([p1, p2], 1)\n",
    "            prev = self.bn(prev)\n",
    "\n",
    "        elif prev.size(1) != self.filters:\n",
    "            prev = self.dim_reduce(prev)\n",
    "\n",
    "        return prev\n",
    "\n",
    "\n",
    "class NormalCell(nn.Module):\n",
    "\n",
    "    def __init__(self, x_in, prev_in, output_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dem_reduce = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(x_in, output_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(output_channels)\n",
    "        )\n",
    "\n",
    "        self.block1_left = SeperableBranch(\n",
    "            output_channels, \n",
    "            output_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            bias=False\n",
    "        )\n",
    "        self.block1_right = nn.Sequential()\n",
    "\n",
    "        self.block2_left = SeperableBranch(\n",
    "            output_channels,\n",
    "            output_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            bias=False\n",
    "        )\n",
    "        self.block2_right = SeperableBranch(\n",
    "            output_channels,\n",
    "            output_channels,\n",
    "            kernel_size=5,\n",
    "            padding=2,\n",
    "            bias=False\n",
    "        )\n",
    "\n",
    "        self.block3_left = nn.AvgPool2d(3, stride=1, padding=1)\n",
    "        self.block3_right = nn.Sequential()\n",
    "\n",
    "        self.block4_left = nn.AvgPool2d(3, stride=1, padding=1)\n",
    "        self.block4_right = nn.AvgPool2d(3, stride=1, padding=1)\n",
    "\n",
    "        self.block5_left = SeperableBranch(\n",
    "            output_channels,\n",
    "            output_channels,\n",
    "            kernel_size=5,\n",
    "            padding=2,\n",
    "            bias=False\n",
    "        )\n",
    "        self.block5_right = SeperableBranch(\n",
    "            output_channels,\n",
    "            output_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            bias=False\n",
    "        )\n",
    "\n",
    "        self.fit = Fit(prev_in, output_channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, prev = x\n",
    "\n",
    "        #return transformed x as new x, and original x as prev \n",
    "        #only prev tensor needs to be modified\n",
    "        prev = self.fit((x, prev)) \n",
    "\n",
    "        h = self.dem_reduce(x)\n",
    "\n",
    "        x1 = self.block1_left(h) + self.block1_right(h)\n",
    "        x2 = self.block2_left(prev) + self.block2_right(h)\n",
    "        x3 = self.block3_left(h) + self.block3_right(h)\n",
    "        x4 = self.block4_left(prev) + self.block4_right(prev)\n",
    "        x5 = self.block5_left(prev) + self.block5_right(prev)\n",
    "\n",
    "        return torch.cat([prev, x1, x2, x3, x4, x5], 1), x\n",
    "\n",
    "class ReductionCell(nn.Module):\n",
    "\n",
    "    def __init__(self, x_in, prev_in, output_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim_reduce = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(x_in, output_channels, 1),\n",
    "            nn.BatchNorm2d(output_channels)\n",
    "        )\n",
    "\n",
    "        #block1\n",
    "        self.layer1block1_left = SeperableBranch(output_channels, output_channels, 7, stride=2, padding=3)\n",
    "        self.layer1block1_right = SeperableBranch(output_channels, output_channels, 5, stride=2, padding=2)\n",
    "\n",
    "        #block2\n",
    "        self.layer1block2_left = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "        self.layer1block2_right = SeperableBranch(output_channels, output_channels, 7, stride=2, padding=3)\n",
    "\n",
    "        #block3\n",
    "        self.layer1block3_left = nn.AvgPool2d(3, 2, 1)\n",
    "        self.layer1block3_right = SeperableBranch(output_channels, output_channels, 5, stride=2, padding=2)\n",
    "\n",
    "        #block5\n",
    "        self.layer2block1_left = nn.MaxPool2d(3, 2, 1)\n",
    "        self.layer2block1_right = SeperableBranch(output_channels, output_channels, 3, stride=1, padding=1)\n",
    "\n",
    "        #block4\n",
    "        self.layer2block2_left = nn.AvgPool2d(3, 1, 1)\n",
    "        self.layer2block2_right = nn.Sequential()\n",
    "    \n",
    "        self.fit = Fit(prev_in, output_channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, prev = x\n",
    "        prev = self.fit((x, prev))\n",
    "\n",
    "        h = self.dim_reduce(x)\n",
    "\n",
    "        layer1block1 = self.layer1block1_left(prev) + self.layer1block1_right(h)\n",
    "        layer1block2 = self.layer1block2_left(h) + self.layer1block2_right(prev)\n",
    "        layer1block3 = self.layer1block3_left(h) + self.layer1block3_right(prev)\n",
    "        layer2block1 = self.layer2block1_left(h) + self.layer2block1_right(layer1block1)\n",
    "        layer2block2 = self.layer2block2_left(layer1block1) + self.layer2block2_right(layer1block2)\n",
    "\n",
    "        return torch.cat([\n",
    "            layer1block2, #https://github.com/keras-team/keras-applications/blob/master/keras_applications/nasnet.py line 739\n",
    "            layer1block3,\n",
    "            layer2block1,\n",
    "            layer2block2\n",
    "        ], 1), x\n",
    "\n",
    "\n",
    "class NasNetA(nn.Module):\n",
    "\n",
    "    def __init__(self, repeat_cell_num, reduction_num, filters, stemfilter, class_num=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, stemfilter, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(stemfilter)\n",
    "        )\n",
    "\n",
    "        self.prev_filters = stemfilter\n",
    "        self.x_filters = stemfilter\n",
    "        self.filters = filters\n",
    "\n",
    "        self.cell_layers = self._make_layers(repeat_cell_num, reduction_num)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.avg = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(self.filters * 6, class_num)\n",
    "    \n",
    "    \n",
    "    def _make_normal(self, block, repeat, output):\n",
    "        \"\"\"make normal cell\n",
    "        Args:\n",
    "            block: cell type\n",
    "            repeat: number of repeated normal cell\n",
    "            output: output filters for each branch in normal cell\n",
    "        Returns:\n",
    "            stacked normal cells\n",
    "        \"\"\"\n",
    "\n",
    "        layers = [] \n",
    "        for r in range(repeat):\n",
    "            layers.append(block(self.x_filters, self.prev_filters, output))\n",
    "            self.prev_filters = self.x_filters\n",
    "            self.x_filters = output * 6 #concatenate 6 branches\n",
    "        \n",
    "        return layers\n",
    "\n",
    "    def _make_reduction(self, block, output):\n",
    "        \"\"\"make normal cell\n",
    "        Args:\n",
    "            block: cell type\n",
    "            output: output filters for each branch in reduction cell\n",
    "        Returns:\n",
    "            reduction cell\n",
    "        \"\"\"\n",
    "\n",
    "        reduction = block(self.x_filters, self.prev_filters, output)\n",
    "        self.prev_filters = self.x_filters\n",
    "        self.x_filters = output * 4 #stack for 4 branches\n",
    "\n",
    "        return reduction\n",
    "    \n",
    "    def _make_layers(self, repeat_cell_num, reduction_num):\n",
    "\n",
    "        layers = []\n",
    "        for i in range(reduction_num):\n",
    "\n",
    "            layers.extend(self._make_normal(NormalCell, repeat_cell_num, self.filters))\n",
    "            self.filters *= 2\n",
    "            layers.append(self._make_reduction(ReductionCell, self.filters))\n",
    "        \n",
    "        layers.extend(self._make_normal(NormalCell, repeat_cell_num, self.filters))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.stem(x)\n",
    "        prev = None\n",
    "        x, prev = self.cell_layers((x, prev))\n",
    "        x = self.relu(x)\n",
    "        x = self.avg(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "        \n",
    "        \n",
    "def nasnet():\n",
    "\n",
    "    #stem filters must be 44, it's a pytorch workaround, cant change to other number\n",
    "    return NasNetA(4, 2, 44, 44) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmUpLR(_LRScheduler):\n",
    "    \"\"\"warmup_training learning rate scheduler\n",
    "    Args:\n",
    "        optimizer: optimzier(e.g. SGD)\n",
    "        total_iters: totoal_iters of warmup phase\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, total_iters, last_epoch=-1):\n",
    "        \n",
    "        self.total_iters = total_iters\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        \"\"\"we will use the first m batches, and set the learning\n",
    "        rate to base_lr * m / total_iters\n",
    "        \"\"\"\n",
    "        return [base_lr * self.last_epoch / (self.total_iters + 1e-8) for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    net.train()\n",
    "    \n",
    "    for batch_index, (images, labels) in enumerate(trainloader):\n",
    "        if epoch <= w:\n",
    "            warmup_scheduler.step()\n",
    "            print('warmed up')\n",
    "        \n",
    "        print('start')\n",
    "        images = Variable(images)\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        labels = labels.cuda()\n",
    "        images = images.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        n_iter = (epoch - 1) * len(trainloader) + batch_index + 1\n",
    "\n",
    "        last_layer = list(net.children())[-1]\n",
    "\n",
    "        print('Training Epoch: {epoch} [{trained_samples}/{total_samples}]\\tLoss: {:0.4f}\\tLR: {:0.6f}'.format(\n",
    "            loss.item(),\n",
    "            optimizer.param_groups[0]['lr'],\n",
    "            epoch=epoch,\n",
    "            trained_samples=batch_index * batchsize + len(images),\n",
    "            total_samples=len(trainloader.dataset)\n",
    "        ))\n",
    "        \n",
    "    \n",
    "def eval_training(epoch):\n",
    "    net.eval()\n",
    "\n",
    "    test_loss = 0.0 # cost function error\n",
    "    correct = 0.0\n",
    "    k= 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            #images = Variable(images)\n",
    "            #labels = Variable(labels)\n",
    "       \n",
    "            temp = net(images)\n",
    "            loss = loss_function(temp, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, preds = temp.max(1)\n",
    "            del temp\n",
    "        torch.cuda.empty_cache()\n",
    "        correct += preds.eq(labels).sum()\n",
    "        k=k+1\n",
    "        if k%10 == 0:\n",
    "            print(k)\n",
    "\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {:.4f}'.format(\n",
    "        test_loss / len(trainloader.dataset),\n",
    "        correct.float() / len(testloader.dataset)\n",
    "    ))\n",
    "    return correct.float() / len(testloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cheatloss(epoch):\n",
    "    for batch_index, (images, labels) in enumerate(trainloader):\n",
    "        print('start')\n",
    "        outputs = net(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [128/50000]\tLoss: 4.6222\tLR: 0.000256\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [256/50000]\tLoss: 4.6528\tLR: 0.000512\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [384/50000]\tLoss: 4.6002\tLR: 0.000767\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [512/50000]\tLoss: 4.6249\tLR: 0.001023\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [640/50000]\tLoss: 4.6113\tLR: 0.001279\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [768/50000]\tLoss: 4.6222\tLR: 0.001535\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [896/50000]\tLoss: 4.6270\tLR: 0.001790\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [1024/50000]\tLoss: 4.6044\tLR: 0.002046\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [1152/50000]\tLoss: 4.5853\tLR: 0.002302\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [1280/50000]\tLoss: 4.6182\tLR: 0.002558\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [1408/50000]\tLoss: 4.6216\tLR: 0.002813\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [1536/50000]\tLoss: 4.5830\tLR: 0.003069\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [1664/50000]\tLoss: 4.5906\tLR: 0.003325\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [1792/50000]\tLoss: 4.5478\tLR: 0.003581\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [1920/50000]\tLoss: 4.5570\tLR: 0.003836\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [2048/50000]\tLoss: 4.5392\tLR: 0.004092\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [2176/50000]\tLoss: 4.5497\tLR: 0.004348\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [2304/50000]\tLoss: 4.5125\tLR: 0.004604\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [2432/50000]\tLoss: 4.5594\tLR: 0.004859\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [2560/50000]\tLoss: 4.6390\tLR: 0.005115\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [2688/50000]\tLoss: 4.5406\tLR: 0.005371\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [2816/50000]\tLoss: 4.5099\tLR: 0.005627\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [2944/50000]\tLoss: 4.4674\tLR: 0.005882\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [3072/50000]\tLoss: 4.6434\tLR: 0.006138\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [3200/50000]\tLoss: 4.5307\tLR: 0.006394\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [3328/50000]\tLoss: 4.6048\tLR: 0.006650\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [3456/50000]\tLoss: 4.5005\tLR: 0.006905\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [3584/50000]\tLoss: 4.4501\tLR: 0.007161\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [3712/50000]\tLoss: 4.5479\tLR: 0.007417\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [3840/50000]\tLoss: 4.4866\tLR: 0.007673\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [3968/50000]\tLoss: 4.5458\tLR: 0.007928\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [4096/50000]\tLoss: 4.5178\tLR: 0.008184\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [4224/50000]\tLoss: 4.5656\tLR: 0.008440\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [4352/50000]\tLoss: 4.5693\tLR: 0.008696\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [4480/50000]\tLoss: 4.4743\tLR: 0.008951\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [4608/50000]\tLoss: 4.5060\tLR: 0.009207\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [4736/50000]\tLoss: 4.4473\tLR: 0.009463\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [4864/50000]\tLoss: 4.4367\tLR: 0.009719\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [4992/50000]\tLoss: 4.4263\tLR: 0.009974\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [5120/50000]\tLoss: 4.5218\tLR: 0.010230\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [5248/50000]\tLoss: 4.3724\tLR: 0.010486\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [5376/50000]\tLoss: 4.4789\tLR: 0.010742\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [5504/50000]\tLoss: 4.4262\tLR: 0.010997\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [5632/50000]\tLoss: 4.4881\tLR: 0.011253\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [5760/50000]\tLoss: 4.3763\tLR: 0.011509\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [5888/50000]\tLoss: 4.3640\tLR: 0.011765\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [6016/50000]\tLoss: 4.3864\tLR: 0.012020\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [6144/50000]\tLoss: 4.3394\tLR: 0.012276\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [6272/50000]\tLoss: 4.3370\tLR: 0.012532\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [6400/50000]\tLoss: 4.3357\tLR: 0.012788\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [6528/50000]\tLoss: 4.4126\tLR: 0.013043\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [6656/50000]\tLoss: 4.4022\tLR: 0.013299\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [6784/50000]\tLoss: 4.2013\tLR: 0.013555\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [6912/50000]\tLoss: 4.4605\tLR: 0.013811\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [7040/50000]\tLoss: 4.2733\tLR: 0.014066\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [7168/50000]\tLoss: 4.3802\tLR: 0.014322\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [7296/50000]\tLoss: 4.4469\tLR: 0.014578\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [7424/50000]\tLoss: 4.2498\tLR: 0.014834\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [7552/50000]\tLoss: 4.3865\tLR: 0.015090\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [7680/50000]\tLoss: 4.4173\tLR: 0.015345\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [7808/50000]\tLoss: 4.2723\tLR: 0.015601\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [7936/50000]\tLoss: 4.3922\tLR: 0.015857\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [8064/50000]\tLoss: 4.2656\tLR: 0.016113\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [8192/50000]\tLoss: 4.3285\tLR: 0.016368\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [8320/50000]\tLoss: 4.3042\tLR: 0.016624\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [8448/50000]\tLoss: 4.2585\tLR: 0.016880\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [8576/50000]\tLoss: 4.1666\tLR: 0.017136\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [8704/50000]\tLoss: 4.3829\tLR: 0.017391\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [8832/50000]\tLoss: 4.3100\tLR: 0.017647\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [8960/50000]\tLoss: 4.1771\tLR: 0.017903\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [9088/50000]\tLoss: 4.2205\tLR: 0.018159\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [9216/50000]\tLoss: 4.2891\tLR: 0.018414\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [9344/50000]\tLoss: 4.2469\tLR: 0.018670\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [9472/50000]\tLoss: 4.1164\tLR: 0.018926\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [9600/50000]\tLoss: 4.2509\tLR: 0.019182\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [9728/50000]\tLoss: 4.2016\tLR: 0.019437\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [9856/50000]\tLoss: 4.2088\tLR: 0.019693\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [9984/50000]\tLoss: 4.1098\tLR: 0.019949\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [10112/50000]\tLoss: 4.2166\tLR: 0.020205\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [10240/50000]\tLoss: 4.0889\tLR: 0.020460\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [10368/50000]\tLoss: 4.0249\tLR: 0.020716\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [10496/50000]\tLoss: 4.3725\tLR: 0.020972\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [10624/50000]\tLoss: 4.2785\tLR: 0.021228\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [10752/50000]\tLoss: 4.2104\tLR: 0.021483\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [10880/50000]\tLoss: 4.3356\tLR: 0.021739\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [11008/50000]\tLoss: 4.3668\tLR: 0.021995\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [11136/50000]\tLoss: 4.1981\tLR: 0.022251\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [11264/50000]\tLoss: 4.1821\tLR: 0.022506\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [11392/50000]\tLoss: 4.1994\tLR: 0.022762\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [11520/50000]\tLoss: 4.0816\tLR: 0.023018\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [11648/50000]\tLoss: 4.2596\tLR: 0.023274\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [11776/50000]\tLoss: 4.1647\tLR: 0.023529\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [11904/50000]\tLoss: 4.1203\tLR: 0.023785\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [12032/50000]\tLoss: 4.2505\tLR: 0.024041\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [12160/50000]\tLoss: 4.1812\tLR: 0.024297\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [12288/50000]\tLoss: 4.0490\tLR: 0.024552\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [12416/50000]\tLoss: 4.1789\tLR: 0.024808\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [12544/50000]\tLoss: 4.0765\tLR: 0.025064\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [12672/50000]\tLoss: 4.1844\tLR: 0.025320\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [12800/50000]\tLoss: 4.1559\tLR: 0.025575\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [12928/50000]\tLoss: 4.0373\tLR: 0.025831\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [13056/50000]\tLoss: 4.0709\tLR: 0.026087\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [13184/50000]\tLoss: 4.0604\tLR: 0.026343\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [13312/50000]\tLoss: 4.3887\tLR: 0.026598\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [13440/50000]\tLoss: 3.9608\tLR: 0.026854\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [13568/50000]\tLoss: 4.1107\tLR: 0.027110\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [13696/50000]\tLoss: 4.1828\tLR: 0.027366\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [13824/50000]\tLoss: 4.0893\tLR: 0.027621\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [13952/50000]\tLoss: 4.0539\tLR: 0.027877\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [14080/50000]\tLoss: 3.9958\tLR: 0.028133\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [14208/50000]\tLoss: 4.2049\tLR: 0.028389\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [14336/50000]\tLoss: 4.1650\tLR: 0.028645\n",
      "warmed up\n",
      "start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [14464/50000]\tLoss: 4.3432\tLR: 0.028900\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [14592/50000]\tLoss: 4.1217\tLR: 0.029156\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [14720/50000]\tLoss: 4.2659\tLR: 0.029412\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [14848/50000]\tLoss: 4.1902\tLR: 0.029668\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [14976/50000]\tLoss: 4.1011\tLR: 0.029923\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [15104/50000]\tLoss: 4.3971\tLR: 0.030179\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [15232/50000]\tLoss: 4.1705\tLR: 0.030435\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [15360/50000]\tLoss: 4.0520\tLR: 0.030691\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [15488/50000]\tLoss: 4.2583\tLR: 0.030946\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [15616/50000]\tLoss: 4.1697\tLR: 0.031202\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [15744/50000]\tLoss: 4.3344\tLR: 0.031458\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [15872/50000]\tLoss: 4.2294\tLR: 0.031714\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [16000/50000]\tLoss: 4.2726\tLR: 0.031969\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [16128/50000]\tLoss: 4.1352\tLR: 0.032225\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [16256/50000]\tLoss: 4.1348\tLR: 0.032481\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [16384/50000]\tLoss: 3.9830\tLR: 0.032737\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [16512/50000]\tLoss: 4.2456\tLR: 0.032992\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [16640/50000]\tLoss: 4.1808\tLR: 0.033248\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [16768/50000]\tLoss: 4.2057\tLR: 0.033504\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [16896/50000]\tLoss: 4.2221\tLR: 0.033760\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [17024/50000]\tLoss: 4.2869\tLR: 0.034015\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [17152/50000]\tLoss: 4.1839\tLR: 0.034271\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [17280/50000]\tLoss: 4.0494\tLR: 0.034527\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [17408/50000]\tLoss: 4.2495\tLR: 0.034783\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [17536/50000]\tLoss: 4.1892\tLR: 0.035038\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [17664/50000]\tLoss: 4.1617\tLR: 0.035294\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [17792/50000]\tLoss: 4.0906\tLR: 0.035550\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [17920/50000]\tLoss: 4.0864\tLR: 0.035806\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [18048/50000]\tLoss: 3.9341\tLR: 0.036061\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [18176/50000]\tLoss: 4.1599\tLR: 0.036317\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [18304/50000]\tLoss: 4.1578\tLR: 0.036573\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [18432/50000]\tLoss: 4.0765\tLR: 0.036829\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [18560/50000]\tLoss: 4.1027\tLR: 0.037084\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [18688/50000]\tLoss: 3.9973\tLR: 0.037340\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [18816/50000]\tLoss: 4.1681\tLR: 0.037596\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [18944/50000]\tLoss: 4.0509\tLR: 0.037852\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [19072/50000]\tLoss: 4.1032\tLR: 0.038107\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [19200/50000]\tLoss: 4.0970\tLR: 0.038363\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [19328/50000]\tLoss: 4.2158\tLR: 0.038619\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [19456/50000]\tLoss: 4.1777\tLR: 0.038875\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [19584/50000]\tLoss: 3.9863\tLR: 0.039130\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [19712/50000]\tLoss: 4.2367\tLR: 0.039386\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [19840/50000]\tLoss: 4.1738\tLR: 0.039642\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [19968/50000]\tLoss: 4.1259\tLR: 0.039898\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [20096/50000]\tLoss: 4.0283\tLR: 0.040153\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [20224/50000]\tLoss: 4.2543\tLR: 0.040409\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [20352/50000]\tLoss: 4.1831\tLR: 0.040665\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [20480/50000]\tLoss: 4.2485\tLR: 0.040921\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [20608/50000]\tLoss: 4.1862\tLR: 0.041176\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [20736/50000]\tLoss: 4.0416\tLR: 0.041432\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [20864/50000]\tLoss: 3.9499\tLR: 0.041688\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [20992/50000]\tLoss: 4.0699\tLR: 0.041944\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [21120/50000]\tLoss: 4.3504\tLR: 0.042199\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [21248/50000]\tLoss: 4.2104\tLR: 0.042455\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [21376/50000]\tLoss: 3.9427\tLR: 0.042711\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [21504/50000]\tLoss: 4.1733\tLR: 0.042967\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [21632/50000]\tLoss: 4.2642\tLR: 0.043223\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [21760/50000]\tLoss: 4.0501\tLR: 0.043478\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [21888/50000]\tLoss: 4.2958\tLR: 0.043734\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [22016/50000]\tLoss: 4.1449\tLR: 0.043990\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [22144/50000]\tLoss: 4.1004\tLR: 0.044246\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [22272/50000]\tLoss: 4.1701\tLR: 0.044501\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [22400/50000]\tLoss: 4.0924\tLR: 0.044757\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [22528/50000]\tLoss: 4.0919\tLR: 0.045013\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [22656/50000]\tLoss: 4.3515\tLR: 0.045269\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [22784/50000]\tLoss: 4.0334\tLR: 0.045524\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [22912/50000]\tLoss: 3.9858\tLR: 0.045780\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [23040/50000]\tLoss: 3.9335\tLR: 0.046036\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [23168/50000]\tLoss: 4.0939\tLR: 0.046292\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [23296/50000]\tLoss: 4.1986\tLR: 0.046547\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [23424/50000]\tLoss: 4.0708\tLR: 0.046803\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [23552/50000]\tLoss: 4.2852\tLR: 0.047059\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [23680/50000]\tLoss: 4.0017\tLR: 0.047315\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [23808/50000]\tLoss: 4.0498\tLR: 0.047570\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [23936/50000]\tLoss: 4.0974\tLR: 0.047826\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [24064/50000]\tLoss: 4.0062\tLR: 0.048082\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [24192/50000]\tLoss: 3.8673\tLR: 0.048338\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [24320/50000]\tLoss: 3.9591\tLR: 0.048593\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [24448/50000]\tLoss: 4.0818\tLR: 0.048849\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [24576/50000]\tLoss: 3.9858\tLR: 0.049105\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [24704/50000]\tLoss: 4.1101\tLR: 0.049361\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [24832/50000]\tLoss: 4.0466\tLR: 0.049616\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [24960/50000]\tLoss: 4.1317\tLR: 0.049872\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [25088/50000]\tLoss: 4.0030\tLR: 0.050128\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [25216/50000]\tLoss: 3.9677\tLR: 0.050384\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [25344/50000]\tLoss: 3.9463\tLR: 0.050639\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [25472/50000]\tLoss: 4.2291\tLR: 0.050895\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [25600/50000]\tLoss: 3.9856\tLR: 0.051151\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [25728/50000]\tLoss: 4.0343\tLR: 0.051407\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [25856/50000]\tLoss: 3.8987\tLR: 0.051662\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [25984/50000]\tLoss: 3.9140\tLR: 0.051918\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [26112/50000]\tLoss: 4.0680\tLR: 0.052174\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [26240/50000]\tLoss: 4.0918\tLR: 0.052430\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [26368/50000]\tLoss: 3.9665\tLR: 0.052685\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [26496/50000]\tLoss: 3.9639\tLR: 0.052941\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [26624/50000]\tLoss: 4.0313\tLR: 0.053197\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [26752/50000]\tLoss: 3.9263\tLR: 0.053453\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [26880/50000]\tLoss: 4.0424\tLR: 0.053708\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [27008/50000]\tLoss: 4.0178\tLR: 0.053964\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [27136/50000]\tLoss: 4.0756\tLR: 0.054220\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [27264/50000]\tLoss: 4.0508\tLR: 0.054476\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [27392/50000]\tLoss: 3.9320\tLR: 0.054731\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [27520/50000]\tLoss: 4.1005\tLR: 0.054987\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [27648/50000]\tLoss: 4.0466\tLR: 0.055243\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [27776/50000]\tLoss: 4.1463\tLR: 0.055499\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [27904/50000]\tLoss: 3.9736\tLR: 0.055754\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [28032/50000]\tLoss: 4.0155\tLR: 0.056010\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [28160/50000]\tLoss: 4.1383\tLR: 0.056266\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [28288/50000]\tLoss: 3.9993\tLR: 0.056522\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [28416/50000]\tLoss: 4.0617\tLR: 0.056777\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [28544/50000]\tLoss: 3.8887\tLR: 0.057033\n",
      "warmed up\n",
      "start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [28672/50000]\tLoss: 3.8326\tLR: 0.057289\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [28800/50000]\tLoss: 3.9207\tLR: 0.057545\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [28928/50000]\tLoss: 4.1554\tLR: 0.057801\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [29056/50000]\tLoss: 4.0990\tLR: 0.058056\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [29184/50000]\tLoss: 4.1516\tLR: 0.058312\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [29312/50000]\tLoss: 3.9569\tLR: 0.058568\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [29440/50000]\tLoss: 4.0428\tLR: 0.058824\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [29568/50000]\tLoss: 4.0849\tLR: 0.059079\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [29696/50000]\tLoss: 4.0724\tLR: 0.059335\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [29824/50000]\tLoss: 4.1225\tLR: 0.059591\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [29952/50000]\tLoss: 3.9553\tLR: 0.059847\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [30080/50000]\tLoss: 3.8207\tLR: 0.060102\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [30208/50000]\tLoss: 3.8901\tLR: 0.060358\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [30336/50000]\tLoss: 3.9926\tLR: 0.060614\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [30464/50000]\tLoss: 4.0599\tLR: 0.060870\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [30592/50000]\tLoss: 3.9660\tLR: 0.061125\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [30720/50000]\tLoss: 3.9125\tLR: 0.061381\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [30848/50000]\tLoss: 3.9950\tLR: 0.061637\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [30976/50000]\tLoss: 3.9740\tLR: 0.061893\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [31104/50000]\tLoss: 4.0796\tLR: 0.062148\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [31232/50000]\tLoss: 4.1286\tLR: 0.062404\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [31360/50000]\tLoss: 3.9596\tLR: 0.062660\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [31488/50000]\tLoss: 3.9477\tLR: 0.062916\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [31616/50000]\tLoss: 3.9746\tLR: 0.063171\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [31744/50000]\tLoss: 3.7696\tLR: 0.063427\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [31872/50000]\tLoss: 3.9417\tLR: 0.063683\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [32000/50000]\tLoss: 4.0004\tLR: 0.063939\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [32128/50000]\tLoss: 3.6992\tLR: 0.064194\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [32256/50000]\tLoss: 4.1630\tLR: 0.064450\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [32384/50000]\tLoss: 4.1168\tLR: 0.064706\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [32512/50000]\tLoss: 4.0667\tLR: 0.064962\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [32640/50000]\tLoss: 3.9349\tLR: 0.065217\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [32768/50000]\tLoss: 3.9237\tLR: 0.065473\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [32896/50000]\tLoss: 4.0117\tLR: 0.065729\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [33024/50000]\tLoss: 4.0252\tLR: 0.065985\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [33152/50000]\tLoss: 3.7588\tLR: 0.066240\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [33280/50000]\tLoss: 3.9353\tLR: 0.066496\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [33408/50000]\tLoss: 4.0322\tLR: 0.066752\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [33536/50000]\tLoss: 3.9004\tLR: 0.067008\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [33664/50000]\tLoss: 4.0744\tLR: 0.067263\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [33792/50000]\tLoss: 3.9360\tLR: 0.067519\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [33920/50000]\tLoss: 3.8080\tLR: 0.067775\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [34048/50000]\tLoss: 3.9242\tLR: 0.068031\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [34176/50000]\tLoss: 4.0515\tLR: 0.068286\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [34304/50000]\tLoss: 3.7432\tLR: 0.068542\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [34432/50000]\tLoss: 3.9099\tLR: 0.068798\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [34560/50000]\tLoss: 3.9403\tLR: 0.069054\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [34688/50000]\tLoss: 3.8769\tLR: 0.069309\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [34816/50000]\tLoss: 3.8599\tLR: 0.069565\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [34944/50000]\tLoss: 3.9457\tLR: 0.069821\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [35072/50000]\tLoss: 3.8777\tLR: 0.070077\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [35200/50000]\tLoss: 4.0600\tLR: 0.070332\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [35328/50000]\tLoss: 3.9033\tLR: 0.070588\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [35456/50000]\tLoss: 3.8882\tLR: 0.070844\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [35584/50000]\tLoss: 3.9470\tLR: 0.071100\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [35712/50000]\tLoss: 3.8336\tLR: 0.071355\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [35840/50000]\tLoss: 4.0961\tLR: 0.071611\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [35968/50000]\tLoss: 3.9228\tLR: 0.071867\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [36096/50000]\tLoss: 3.9296\tLR: 0.072123\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [36224/50000]\tLoss: 4.0421\tLR: 0.072379\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [36352/50000]\tLoss: 4.1253\tLR: 0.072634\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [36480/50000]\tLoss: 3.9275\tLR: 0.072890\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [36608/50000]\tLoss: 3.9477\tLR: 0.073146\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [36736/50000]\tLoss: 3.9720\tLR: 0.073402\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [36864/50000]\tLoss: 3.9033\tLR: 0.073657\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [36992/50000]\tLoss: 3.9784\tLR: 0.073913\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [37120/50000]\tLoss: 3.8502\tLR: 0.074169\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [37248/50000]\tLoss: 4.0320\tLR: 0.074425\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [37376/50000]\tLoss: 3.7982\tLR: 0.074680\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [37504/50000]\tLoss: 4.0048\tLR: 0.074936\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [37632/50000]\tLoss: 4.0611\tLR: 0.075192\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [37760/50000]\tLoss: 3.9999\tLR: 0.075448\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [37888/50000]\tLoss: 4.1338\tLR: 0.075703\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [38016/50000]\tLoss: 3.8620\tLR: 0.075959\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [38144/50000]\tLoss: 3.9039\tLR: 0.076215\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [38272/50000]\tLoss: 3.9875\tLR: 0.076471\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [38400/50000]\tLoss: 4.0157\tLR: 0.076726\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [38528/50000]\tLoss: 3.9273\tLR: 0.076982\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [38656/50000]\tLoss: 3.8754\tLR: 0.077238\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [38784/50000]\tLoss: 3.9109\tLR: 0.077494\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [38912/50000]\tLoss: 4.1037\tLR: 0.077749\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [39040/50000]\tLoss: 3.9795\tLR: 0.078005\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [39168/50000]\tLoss: 3.8817\tLR: 0.078261\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [39296/50000]\tLoss: 3.8752\tLR: 0.078517\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [39424/50000]\tLoss: 3.9301\tLR: 0.078772\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [39552/50000]\tLoss: 4.0625\tLR: 0.079028\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [39680/50000]\tLoss: 3.8850\tLR: 0.079284\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [39808/50000]\tLoss: 3.9283\tLR: 0.079540\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [39936/50000]\tLoss: 3.7675\tLR: 0.079795\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [40064/50000]\tLoss: 4.0601\tLR: 0.080051\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [40192/50000]\tLoss: 3.8285\tLR: 0.080307\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [40320/50000]\tLoss: 4.1506\tLR: 0.080563\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [40448/50000]\tLoss: 3.9003\tLR: 0.080818\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [40576/50000]\tLoss: 3.8107\tLR: 0.081074\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [40704/50000]\tLoss: 3.8669\tLR: 0.081330\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [40832/50000]\tLoss: 4.0412\tLR: 0.081586\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [40960/50000]\tLoss: 4.0011\tLR: 0.081841\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [41088/50000]\tLoss: 3.8595\tLR: 0.082097\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [41216/50000]\tLoss: 3.8359\tLR: 0.082353\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [41344/50000]\tLoss: 3.9165\tLR: 0.082609\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [41472/50000]\tLoss: 3.9737\tLR: 0.082864\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [41600/50000]\tLoss: 3.9007\tLR: 0.083120\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [41728/50000]\tLoss: 3.9439\tLR: 0.083376\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [41856/50000]\tLoss: 3.8382\tLR: 0.083632\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [41984/50000]\tLoss: 3.8224\tLR: 0.083887\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [42112/50000]\tLoss: 3.9953\tLR: 0.084143\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [42240/50000]\tLoss: 3.9485\tLR: 0.084399\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [42368/50000]\tLoss: 4.0114\tLR: 0.084655\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [42496/50000]\tLoss: 3.7386\tLR: 0.084910\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [42624/50000]\tLoss: 3.7287\tLR: 0.085166\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [42752/50000]\tLoss: 3.7293\tLR: 0.085422\n",
      "warmed up\n",
      "start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [42880/50000]\tLoss: 3.9336\tLR: 0.085678\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [43008/50000]\tLoss: 3.7304\tLR: 0.085934\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [43136/50000]\tLoss: 3.9397\tLR: 0.086189\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [43264/50000]\tLoss: 3.9037\tLR: 0.086445\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [43392/50000]\tLoss: 4.0778\tLR: 0.086701\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [43520/50000]\tLoss: 3.8357\tLR: 0.086957\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [43648/50000]\tLoss: 4.0148\tLR: 0.087212\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [43776/50000]\tLoss: 4.0105\tLR: 0.087468\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [43904/50000]\tLoss: 3.8485\tLR: 0.087724\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [44032/50000]\tLoss: 3.9709\tLR: 0.087980\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [44160/50000]\tLoss: 3.7811\tLR: 0.088235\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [44288/50000]\tLoss: 3.9464\tLR: 0.088491\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [44416/50000]\tLoss: 3.7663\tLR: 0.088747\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [44544/50000]\tLoss: 3.9506\tLR: 0.089003\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [44672/50000]\tLoss: 3.7410\tLR: 0.089258\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [44800/50000]\tLoss: 3.7998\tLR: 0.089514\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [44928/50000]\tLoss: 3.7783\tLR: 0.089770\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [45056/50000]\tLoss: 3.8551\tLR: 0.090026\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [45184/50000]\tLoss: 3.8738\tLR: 0.090281\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [45312/50000]\tLoss: 4.0415\tLR: 0.090537\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [45440/50000]\tLoss: 3.8525\tLR: 0.090793\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [45568/50000]\tLoss: 3.9259\tLR: 0.091049\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [45696/50000]\tLoss: 3.9076\tLR: 0.091304\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [45824/50000]\tLoss: 3.6495\tLR: 0.091560\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [45952/50000]\tLoss: 3.8136\tLR: 0.091816\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [46080/50000]\tLoss: 4.0240\tLR: 0.092072\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [46208/50000]\tLoss: 4.0189\tLR: 0.092327\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [46336/50000]\tLoss: 3.7530\tLR: 0.092583\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [46464/50000]\tLoss: 3.6765\tLR: 0.092839\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [46592/50000]\tLoss: 3.8668\tLR: 0.093095\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [46720/50000]\tLoss: 3.7050\tLR: 0.093350\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [46848/50000]\tLoss: 3.7111\tLR: 0.093606\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [46976/50000]\tLoss: 4.0118\tLR: 0.093862\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [47104/50000]\tLoss: 3.7190\tLR: 0.094118\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [47232/50000]\tLoss: 4.0015\tLR: 0.094373\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [47360/50000]\tLoss: 4.0667\tLR: 0.094629\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [47488/50000]\tLoss: 3.8578\tLR: 0.094885\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [47616/50000]\tLoss: 3.8276\tLR: 0.095141\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [47744/50000]\tLoss: 3.6549\tLR: 0.095396\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [47872/50000]\tLoss: 3.8717\tLR: 0.095652\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [48000/50000]\tLoss: 3.9209\tLR: 0.095908\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [48128/50000]\tLoss: 3.9552\tLR: 0.096164\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [48256/50000]\tLoss: 3.8552\tLR: 0.096419\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [48384/50000]\tLoss: 3.9226\tLR: 0.096675\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [48512/50000]\tLoss: 3.7573\tLR: 0.096931\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [48640/50000]\tLoss: 3.8771\tLR: 0.097187\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [48768/50000]\tLoss: 3.7561\tLR: 0.097442\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [48896/50000]\tLoss: 3.8651\tLR: 0.097698\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [49024/50000]\tLoss: 3.7543\tLR: 0.097954\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [49152/50000]\tLoss: 3.6772\tLR: 0.098210\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [49280/50000]\tLoss: 4.1378\tLR: 0.098465\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [49408/50000]\tLoss: 3.9637\tLR: 0.098721\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [49536/50000]\tLoss: 3.7597\tLR: 0.098977\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [49664/50000]\tLoss: 3.8941\tLR: 0.099233\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [49792/50000]\tLoss: 3.8669\tLR: 0.099488\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [49920/50000]\tLoss: 3.7890\tLR: 0.099744\n",
      "warmed up\n",
      "start\n",
      "Training Epoch: 1 [50000/50000]\tLoss: 3.9756\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [128/50000]\tLoss: 3.7823\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [256/50000]\tLoss: 3.7840\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [384/50000]\tLoss: 3.8147\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [512/50000]\tLoss: 3.9215\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [640/50000]\tLoss: 3.8575\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [768/50000]\tLoss: 3.6665\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [896/50000]\tLoss: 3.8229\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [1024/50000]\tLoss: 3.7877\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [1152/50000]\tLoss: 3.5594\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [1280/50000]\tLoss: 3.7918\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [1408/50000]\tLoss: 3.7493\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [1536/50000]\tLoss: 3.6377\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [1664/50000]\tLoss: 3.8803\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [1792/50000]\tLoss: 3.7277\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [1920/50000]\tLoss: 3.9402\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [2048/50000]\tLoss: 3.7864\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [2176/50000]\tLoss: 3.5353\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [2304/50000]\tLoss: 3.6953\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [2432/50000]\tLoss: 3.9878\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [2560/50000]\tLoss: 3.8115\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [2688/50000]\tLoss: 3.7240\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [2816/50000]\tLoss: 3.7894\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [2944/50000]\tLoss: 3.8426\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [3072/50000]\tLoss: 3.7378\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [3200/50000]\tLoss: 3.8210\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [3328/50000]\tLoss: 3.8241\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [3456/50000]\tLoss: 3.6947\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [3584/50000]\tLoss: 3.9726\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [3712/50000]\tLoss: 3.8768\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [3840/50000]\tLoss: 3.7509\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [3968/50000]\tLoss: 3.9651\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [4096/50000]\tLoss: 3.6756\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [4224/50000]\tLoss: 3.8205\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [4352/50000]\tLoss: 4.0555\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [4480/50000]\tLoss: 3.8025\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [4608/50000]\tLoss: 3.7088\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [4736/50000]\tLoss: 3.7933\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [4864/50000]\tLoss: 3.8311\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [4992/50000]\tLoss: 3.8014\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [5120/50000]\tLoss: 4.0217\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [5248/50000]\tLoss: 3.8281\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [5376/50000]\tLoss: 3.8596\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [5504/50000]\tLoss: 3.6559\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [5632/50000]\tLoss: 3.7807\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [5760/50000]\tLoss: 3.5587\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [5888/50000]\tLoss: 3.7123\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [6016/50000]\tLoss: 3.8275\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [6144/50000]\tLoss: 3.6556\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [6272/50000]\tLoss: 3.7333\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [6400/50000]\tLoss: 3.6506\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [6528/50000]\tLoss: 3.8245\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [6656/50000]\tLoss: 3.8077\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [6784/50000]\tLoss: 3.7533\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [6912/50000]\tLoss: 3.6921\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [7040/50000]\tLoss: 3.5882\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [7168/50000]\tLoss: 3.6841\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [7296/50000]\tLoss: 3.8432\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [7424/50000]\tLoss: 3.8195\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [7552/50000]\tLoss: 3.5561\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [7680/50000]\tLoss: 3.7371\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [7808/50000]\tLoss: 3.7641\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [7936/50000]\tLoss: 3.6592\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [8064/50000]\tLoss: 3.6738\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [8192/50000]\tLoss: 3.6921\tLR: 0.100000\n",
      "start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [8320/50000]\tLoss: 3.4196\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [8448/50000]\tLoss: 3.2847\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [8576/50000]\tLoss: 3.7479\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [8704/50000]\tLoss: 3.8264\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [8832/50000]\tLoss: 3.8865\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [8960/50000]\tLoss: 3.7353\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [9088/50000]\tLoss: 3.5706\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [9216/50000]\tLoss: 3.7604\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [9344/50000]\tLoss: 3.6543\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [9472/50000]\tLoss: 3.6137\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [9600/50000]\tLoss: 3.6695\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [9728/50000]\tLoss: 3.7087\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [9856/50000]\tLoss: 3.8474\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [9984/50000]\tLoss: 3.6483\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [10112/50000]\tLoss: 3.7157\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [10240/50000]\tLoss: 3.5732\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [10368/50000]\tLoss: 3.7254\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [10496/50000]\tLoss: 3.7855\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [10624/50000]\tLoss: 3.6967\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [10752/50000]\tLoss: 3.8036\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [10880/50000]\tLoss: 3.5804\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [11008/50000]\tLoss: 3.6166\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [11136/50000]\tLoss: 3.7067\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [11264/50000]\tLoss: 3.6476\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [11392/50000]\tLoss: 3.6474\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [11520/50000]\tLoss: 3.6126\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [11648/50000]\tLoss: 3.7580\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [11776/50000]\tLoss: 3.7971\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [11904/50000]\tLoss: 3.6427\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [12032/50000]\tLoss: 3.6106\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [12160/50000]\tLoss: 3.5587\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [12288/50000]\tLoss: 3.8480\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [12416/50000]\tLoss: 3.7061\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [12544/50000]\tLoss: 3.6369\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [12672/50000]\tLoss: 3.7484\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [12800/50000]\tLoss: 3.8241\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [12928/50000]\tLoss: 3.6611\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [13056/50000]\tLoss: 3.7950\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [13184/50000]\tLoss: 3.6396\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [13312/50000]\tLoss: 3.6940\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [13440/50000]\tLoss: 3.6281\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [13568/50000]\tLoss: 3.5525\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [13696/50000]\tLoss: 3.7004\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [13824/50000]\tLoss: 3.8857\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [13952/50000]\tLoss: 3.5596\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [14080/50000]\tLoss: 3.5440\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [14208/50000]\tLoss: 3.8337\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [14336/50000]\tLoss: 3.7223\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [14464/50000]\tLoss: 3.5038\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [14592/50000]\tLoss: 3.6786\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [14720/50000]\tLoss: 3.5016\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [14848/50000]\tLoss: 3.4889\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [14976/50000]\tLoss: 3.8824\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [15104/50000]\tLoss: 3.4725\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [15232/50000]\tLoss: 3.4796\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [15360/50000]\tLoss: 3.6317\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [15488/50000]\tLoss: 3.6100\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [15616/50000]\tLoss: 3.5525\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [15744/50000]\tLoss: 3.6694\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [15872/50000]\tLoss: 3.5837\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [16000/50000]\tLoss: 3.6693\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [16128/50000]\tLoss: 3.6284\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [16256/50000]\tLoss: 3.9047\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [16384/50000]\tLoss: 3.4619\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [16512/50000]\tLoss: 3.5638\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [16640/50000]\tLoss: 3.6744\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [16768/50000]\tLoss: 3.6445\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [16896/50000]\tLoss: 3.6481\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [17024/50000]\tLoss: 3.5931\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [17152/50000]\tLoss: 3.7815\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [17280/50000]\tLoss: 3.7263\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [17408/50000]\tLoss: 3.6291\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [17536/50000]\tLoss: 3.6133\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [17664/50000]\tLoss: 3.7980\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [17792/50000]\tLoss: 3.7887\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [17920/50000]\tLoss: 3.8844\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [18048/50000]\tLoss: 3.3222\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [18176/50000]\tLoss: 3.6178\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [18304/50000]\tLoss: 3.4620\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [18432/50000]\tLoss: 3.4809\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [18560/50000]\tLoss: 3.3901\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [18688/50000]\tLoss: 3.7855\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [18816/50000]\tLoss: 3.5570\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [18944/50000]\tLoss: 3.6525\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [19072/50000]\tLoss: 3.6082\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [19200/50000]\tLoss: 3.6957\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [19328/50000]\tLoss: 3.5461\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [19456/50000]\tLoss: 3.6822\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [19584/50000]\tLoss: 3.7284\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [19712/50000]\tLoss: 3.4558\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [19840/50000]\tLoss: 3.5030\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [19968/50000]\tLoss: 3.7315\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [20096/50000]\tLoss: 3.5349\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [20224/50000]\tLoss: 3.6392\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [20352/50000]\tLoss: 3.7545\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [20480/50000]\tLoss: 3.6771\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [20608/50000]\tLoss: 3.6891\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [20736/50000]\tLoss: 3.4117\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [20864/50000]\tLoss: 3.6699\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [20992/50000]\tLoss: 3.5386\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [21120/50000]\tLoss: 3.6814\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [21248/50000]\tLoss: 3.4110\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [21376/50000]\tLoss: 3.4194\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [21504/50000]\tLoss: 3.5388\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [21632/50000]\tLoss: 3.4783\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [21760/50000]\tLoss: 3.6076\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [21888/50000]\tLoss: 3.6111\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [22016/50000]\tLoss: 3.3923\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [22144/50000]\tLoss: 3.4400\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [22272/50000]\tLoss: 3.6960\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [22400/50000]\tLoss: 3.6704\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [22528/50000]\tLoss: 3.8437\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [22656/50000]\tLoss: 3.6306\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [22784/50000]\tLoss: 3.6404\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [22912/50000]\tLoss: 3.6674\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [23040/50000]\tLoss: 3.5269\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [23168/50000]\tLoss: 3.6582\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [23296/50000]\tLoss: 3.4981\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [23424/50000]\tLoss: 3.6069\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [23552/50000]\tLoss: 3.5743\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [23680/50000]\tLoss: 3.4414\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [23808/50000]\tLoss: 3.6181\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [23936/50000]\tLoss: 3.7288\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [24064/50000]\tLoss: 3.3899\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [24192/50000]\tLoss: 3.4307\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [24320/50000]\tLoss: 3.6571\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [24448/50000]\tLoss: 3.6334\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [24576/50000]\tLoss: 3.4431\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [24704/50000]\tLoss: 3.4186\tLR: 0.100000\n",
      "start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [24832/50000]\tLoss: 3.4292\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [24960/50000]\tLoss: 3.3104\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [25088/50000]\tLoss: 3.6835\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [25216/50000]\tLoss: 3.4154\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [25344/50000]\tLoss: 3.4947\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [25472/50000]\tLoss: 3.5742\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [25600/50000]\tLoss: 3.3285\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [25728/50000]\tLoss: 3.6773\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [25856/50000]\tLoss: 3.3978\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [25984/50000]\tLoss: 3.5107\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [26112/50000]\tLoss: 3.5474\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [26240/50000]\tLoss: 3.7578\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [26368/50000]\tLoss: 3.3968\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [26496/50000]\tLoss: 3.4923\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [26624/50000]\tLoss: 3.6455\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [26752/50000]\tLoss: 3.4302\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [26880/50000]\tLoss: 3.4782\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [27008/50000]\tLoss: 3.4289\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [27136/50000]\tLoss: 3.4450\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [27264/50000]\tLoss: 3.6074\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [27392/50000]\tLoss: 3.5921\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [27520/50000]\tLoss: 3.5391\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [27648/50000]\tLoss: 3.4297\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [27776/50000]\tLoss: 3.6446\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [27904/50000]\tLoss: 3.1978\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [28032/50000]\tLoss: 3.5561\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [28160/50000]\tLoss: 3.5821\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [28288/50000]\tLoss: 3.6232\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [28416/50000]\tLoss: 3.7274\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [28544/50000]\tLoss: 3.3311\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [28672/50000]\tLoss: 3.3899\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [28800/50000]\tLoss: 3.4152\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [28928/50000]\tLoss: 3.3713\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [29056/50000]\tLoss: 3.3151\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [29184/50000]\tLoss: 3.6633\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [29312/50000]\tLoss: 3.4978\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [29440/50000]\tLoss: 3.6424\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [29568/50000]\tLoss: 3.5352\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [29696/50000]\tLoss: 3.5547\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [29824/50000]\tLoss: 3.5228\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [29952/50000]\tLoss: 3.7401\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [30080/50000]\tLoss: 3.6437\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [30208/50000]\tLoss: 3.5559\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [30336/50000]\tLoss: 3.4794\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [30464/50000]\tLoss: 3.4364\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [30592/50000]\tLoss: 3.4890\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [30720/50000]\tLoss: 3.3637\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [30848/50000]\tLoss: 3.6878\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [30976/50000]\tLoss: 3.5856\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [31104/50000]\tLoss: 3.5781\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [31232/50000]\tLoss: 3.5894\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [31360/50000]\tLoss: 3.5178\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [31488/50000]\tLoss: 3.3951\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [31616/50000]\tLoss: 3.3558\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [31744/50000]\tLoss: 3.7217\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [31872/50000]\tLoss: 3.5091\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [32000/50000]\tLoss: 3.6440\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [32128/50000]\tLoss: 3.5369\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [32256/50000]\tLoss: 3.3192\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [32384/50000]\tLoss: 3.4603\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [32512/50000]\tLoss: 3.2945\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [32640/50000]\tLoss: 3.6477\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [32768/50000]\tLoss: 3.4857\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [32896/50000]\tLoss: 3.5049\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [33024/50000]\tLoss: 3.6146\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [33152/50000]\tLoss: 3.3108\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [33280/50000]\tLoss: 3.5197\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [33408/50000]\tLoss: 3.3462\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [33536/50000]\tLoss: 3.3737\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [33664/50000]\tLoss: 3.4456\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [33792/50000]\tLoss: 3.4360\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [33920/50000]\tLoss: 3.5780\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [34048/50000]\tLoss: 3.4909\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [34176/50000]\tLoss: 3.4715\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [34304/50000]\tLoss: 3.4631\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [34432/50000]\tLoss: 3.2349\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [34560/50000]\tLoss: 3.4394\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [34688/50000]\tLoss: 3.5970\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [34816/50000]\tLoss: 3.5578\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [34944/50000]\tLoss: 3.2847\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [35072/50000]\tLoss: 3.3686\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [35200/50000]\tLoss: 3.4218\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [35328/50000]\tLoss: 3.5655\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [35456/50000]\tLoss: 3.4393\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [35584/50000]\tLoss: 3.6870\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [35712/50000]\tLoss: 3.5658\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [35840/50000]\tLoss: 3.5838\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [35968/50000]\tLoss: 3.4416\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [36096/50000]\tLoss: 3.5522\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [36224/50000]\tLoss: 3.3750\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [36352/50000]\tLoss: 3.2977\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [36480/50000]\tLoss: 3.3782\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [36608/50000]\tLoss: 3.4371\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [36736/50000]\tLoss: 3.4990\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [36864/50000]\tLoss: 3.5586\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [36992/50000]\tLoss: 3.2767\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [37120/50000]\tLoss: 3.5187\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [37248/50000]\tLoss: 3.4023\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [37376/50000]\tLoss: 3.3662\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [37504/50000]\tLoss: 3.6894\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [37632/50000]\tLoss: 3.4448\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [37760/50000]\tLoss: 3.4133\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [37888/50000]\tLoss: 3.4611\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [38016/50000]\tLoss: 3.4467\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [38144/50000]\tLoss: 3.3609\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [38272/50000]\tLoss: 3.6951\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [38400/50000]\tLoss: 3.3643\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [38528/50000]\tLoss: 3.2761\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [38656/50000]\tLoss: 3.2796\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [38784/50000]\tLoss: 3.3903\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [38912/50000]\tLoss: 3.3739\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [39040/50000]\tLoss: 3.3218\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [39168/50000]\tLoss: 3.2041\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [39296/50000]\tLoss: 3.1563\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [39424/50000]\tLoss: 3.3668\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [39552/50000]\tLoss: 3.3168\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [39680/50000]\tLoss: 3.3018\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [39808/50000]\tLoss: 3.3863\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [39936/50000]\tLoss: 3.5430\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [40064/50000]\tLoss: 3.3210\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [40192/50000]\tLoss: 3.3954\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [40320/50000]\tLoss: 3.2685\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [40448/50000]\tLoss: 3.3931\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [40576/50000]\tLoss: 3.1806\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [40704/50000]\tLoss: 3.2915\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [40832/50000]\tLoss: 3.5245\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [40960/50000]\tLoss: 3.4619\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [41088/50000]\tLoss: 3.1880\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [41216/50000]\tLoss: 3.5299\tLR: 0.100000\n",
      "start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 [41344/50000]\tLoss: 3.2753\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [41472/50000]\tLoss: 3.3701\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [41600/50000]\tLoss: 3.5047\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [41728/50000]\tLoss: 3.3786\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [41856/50000]\tLoss: 3.3848\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [41984/50000]\tLoss: 3.3592\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [42112/50000]\tLoss: 3.4269\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [42240/50000]\tLoss: 3.5172\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [42368/50000]\tLoss: 3.4500\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [42496/50000]\tLoss: 3.4548\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [42624/50000]\tLoss: 3.4555\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [42752/50000]\tLoss: 3.4472\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [42880/50000]\tLoss: 3.2848\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [43008/50000]\tLoss: 3.2001\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [43136/50000]\tLoss: 3.3864\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [43264/50000]\tLoss: 3.5348\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [43392/50000]\tLoss: 3.3618\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [43520/50000]\tLoss: 3.2238\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [43648/50000]\tLoss: 3.4317\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [43776/50000]\tLoss: 3.3598\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [43904/50000]\tLoss: 3.0879\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [44032/50000]\tLoss: 3.3278\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [44160/50000]\tLoss: 3.1760\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [44288/50000]\tLoss: 3.4161\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [44416/50000]\tLoss: 3.0256\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [44544/50000]\tLoss: 3.3555\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [44672/50000]\tLoss: 3.1387\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [44800/50000]\tLoss: 3.4001\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [44928/50000]\tLoss: 3.3471\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [45056/50000]\tLoss: 3.4542\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [45184/50000]\tLoss: 3.3440\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [45312/50000]\tLoss: 3.3640\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [45440/50000]\tLoss: 3.2434\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [45568/50000]\tLoss: 3.4581\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [45696/50000]\tLoss: 3.2336\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [45824/50000]\tLoss: 3.4511\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [45952/50000]\tLoss: 3.2937\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [46080/50000]\tLoss: 3.4966\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [46208/50000]\tLoss: 3.3943\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [46336/50000]\tLoss: 3.2395\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [46464/50000]\tLoss: 3.3918\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [46592/50000]\tLoss: 3.1802\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [46720/50000]\tLoss: 3.6058\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [46848/50000]\tLoss: 3.3900\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [46976/50000]\tLoss: 3.4029\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [47104/50000]\tLoss: 3.2459\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [47232/50000]\tLoss: 3.6411\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [47360/50000]\tLoss: 3.3011\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [47488/50000]\tLoss: 3.4488\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [47616/50000]\tLoss: 3.4796\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [47744/50000]\tLoss: 3.3963\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [47872/50000]\tLoss: 3.3962\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [48000/50000]\tLoss: 3.4677\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [48128/50000]\tLoss: 3.3025\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [48256/50000]\tLoss: 3.4376\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [48384/50000]\tLoss: 3.4412\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [48512/50000]\tLoss: 3.2918\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [48640/50000]\tLoss: 3.5707\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [48768/50000]\tLoss: 3.4199\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [48896/50000]\tLoss: 3.2195\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [49024/50000]\tLoss: 3.1210\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [49152/50000]\tLoss: 3.5868\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [49280/50000]\tLoss: 3.4113\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [49408/50000]\tLoss: 3.0672\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [49536/50000]\tLoss: 3.2745\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [49664/50000]\tLoss: 3.3397\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [49792/50000]\tLoss: 3.1237\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [49920/50000]\tLoss: 3.4341\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 2 [50000/50000]\tLoss: 3.3845\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [128/50000]\tLoss: 3.3887\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [256/50000]\tLoss: 3.2509\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [384/50000]\tLoss: 3.1400\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [512/50000]\tLoss: 3.1828\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [640/50000]\tLoss: 3.3618\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [768/50000]\tLoss: 3.4412\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [896/50000]\tLoss: 3.1140\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [1024/50000]\tLoss: 3.4826\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [1152/50000]\tLoss: 3.4158\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [1280/50000]\tLoss: 3.4090\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [1408/50000]\tLoss: 3.2854\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [1536/50000]\tLoss: 3.0859\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [1664/50000]\tLoss: 3.5771\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [1792/50000]\tLoss: 3.4908\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [1920/50000]\tLoss: 3.3456\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [2048/50000]\tLoss: 3.3605\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [2176/50000]\tLoss: 3.4441\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [2304/50000]\tLoss: 3.1436\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [2432/50000]\tLoss: 3.4438\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [2560/50000]\tLoss: 3.2105\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [2688/50000]\tLoss: 3.3680\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [2816/50000]\tLoss: 3.2257\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [2944/50000]\tLoss: 3.2610\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [3072/50000]\tLoss: 3.1814\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [3200/50000]\tLoss: 3.2230\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [3328/50000]\tLoss: 3.2345\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [3456/50000]\tLoss: 3.4270\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [3584/50000]\tLoss: 3.0055\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [3712/50000]\tLoss: 3.2826\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [3840/50000]\tLoss: 3.3376\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [3968/50000]\tLoss: 3.0021\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [4096/50000]\tLoss: 3.2189\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [4224/50000]\tLoss: 3.0592\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [4352/50000]\tLoss: 3.5201\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [4480/50000]\tLoss: 3.4068\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [4608/50000]\tLoss: 3.2677\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [4736/50000]\tLoss: 3.1626\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [4864/50000]\tLoss: 3.1638\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [4992/50000]\tLoss: 3.3795\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [5120/50000]\tLoss: 3.2395\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [5248/50000]\tLoss: 3.2412\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [5376/50000]\tLoss: 3.5361\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [5504/50000]\tLoss: 3.1868\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [5632/50000]\tLoss: 3.2918\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [5760/50000]\tLoss: 3.1758\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [5888/50000]\tLoss: 3.2958\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [6016/50000]\tLoss: 3.3512\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [6144/50000]\tLoss: 3.3239\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [6272/50000]\tLoss: 3.4469\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [6400/50000]\tLoss: 3.1581\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [6528/50000]\tLoss: 3.2498\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [6656/50000]\tLoss: 3.3313\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [6784/50000]\tLoss: 3.2237\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [6912/50000]\tLoss: 3.3536\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [7040/50000]\tLoss: 3.1553\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [7168/50000]\tLoss: 3.2525\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [7296/50000]\tLoss: 3.2199\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [7424/50000]\tLoss: 3.2795\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [7552/50000]\tLoss: 3.4085\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [7680/50000]\tLoss: 3.1066\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [7808/50000]\tLoss: 3.5075\tLR: 0.100000\n",
      "start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [7936/50000]\tLoss: 3.5455\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [8064/50000]\tLoss: 3.4352\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [8192/50000]\tLoss: 3.2088\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [8320/50000]\tLoss: 3.2500\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [8448/50000]\tLoss: 3.0834\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [8576/50000]\tLoss: 2.9137\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [8704/50000]\tLoss: 3.3169\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [8832/50000]\tLoss: 3.1892\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [8960/50000]\tLoss: 3.3555\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [9088/50000]\tLoss: 3.1894\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [9216/50000]\tLoss: 2.9062\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [9344/50000]\tLoss: 3.1698\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [9472/50000]\tLoss: 3.1829\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [9600/50000]\tLoss: 3.1199\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [9728/50000]\tLoss: 3.0327\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [9856/50000]\tLoss: 3.2670\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [9984/50000]\tLoss: 3.2574\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [10112/50000]\tLoss: 3.1749\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [10240/50000]\tLoss: 3.3332\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [10368/50000]\tLoss: 3.4468\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [10496/50000]\tLoss: 3.4015\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [10624/50000]\tLoss: 3.1509\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [10752/50000]\tLoss: 2.9890\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [10880/50000]\tLoss: 3.2968\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [11008/50000]\tLoss: 3.1869\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [11136/50000]\tLoss: 3.2235\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [11264/50000]\tLoss: 2.9592\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [11392/50000]\tLoss: 3.1978\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [11520/50000]\tLoss: 3.0038\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [11648/50000]\tLoss: 3.1198\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [11776/50000]\tLoss: 3.1544\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [11904/50000]\tLoss: 3.2368\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [12032/50000]\tLoss: 2.9705\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [12160/50000]\tLoss: 3.0180\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [12288/50000]\tLoss: 3.0529\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [12416/50000]\tLoss: 2.9533\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [12544/50000]\tLoss: 3.5623\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [12672/50000]\tLoss: 3.0908\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [12800/50000]\tLoss: 3.1232\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [12928/50000]\tLoss: 3.1599\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [13056/50000]\tLoss: 3.0595\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [13184/50000]\tLoss: 3.0994\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [13312/50000]\tLoss: 3.3896\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [13440/50000]\tLoss: 3.2876\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [13568/50000]\tLoss: 3.1838\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [13696/50000]\tLoss: 3.4860\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [13824/50000]\tLoss: 2.9038\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [13952/50000]\tLoss: 3.1553\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [14080/50000]\tLoss: 3.1354\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [14208/50000]\tLoss: 3.3786\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [14336/50000]\tLoss: 3.2245\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [14464/50000]\tLoss: 3.3029\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [14592/50000]\tLoss: 3.1447\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [14720/50000]\tLoss: 3.3556\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [14848/50000]\tLoss: 3.1590\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [14976/50000]\tLoss: 3.0711\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [15104/50000]\tLoss: 3.0881\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [15232/50000]\tLoss: 3.3475\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [15360/50000]\tLoss: 3.3251\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [15488/50000]\tLoss: 3.0047\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [15616/50000]\tLoss: 3.3882\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [15744/50000]\tLoss: 3.1848\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [15872/50000]\tLoss: 3.2431\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [16000/50000]\tLoss: 3.1671\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [16128/50000]\tLoss: 3.2347\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [16256/50000]\tLoss: 3.1306\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [16384/50000]\tLoss: 3.2557\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [16512/50000]\tLoss: 3.0227\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [16640/50000]\tLoss: 3.1603\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [16768/50000]\tLoss: 3.0727\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [16896/50000]\tLoss: 3.2678\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [17024/50000]\tLoss: 2.9459\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [17152/50000]\tLoss: 3.1048\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [17280/50000]\tLoss: 3.1646\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [17408/50000]\tLoss: 3.1029\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [17536/50000]\tLoss: 3.0882\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [17664/50000]\tLoss: 3.2124\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [17792/50000]\tLoss: 3.1178\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [17920/50000]\tLoss: 3.0541\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [18048/50000]\tLoss: 3.2194\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [18176/50000]\tLoss: 3.1184\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [18304/50000]\tLoss: 3.1279\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [18432/50000]\tLoss: 3.1717\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [18560/50000]\tLoss: 3.1553\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [18688/50000]\tLoss: 3.2611\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [18816/50000]\tLoss: 3.1874\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [18944/50000]\tLoss: 3.2823\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [19072/50000]\tLoss: 3.1135\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [19200/50000]\tLoss: 3.3601\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [19328/50000]\tLoss: 3.3751\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [19456/50000]\tLoss: 3.2184\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [19584/50000]\tLoss: 3.0954\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [19712/50000]\tLoss: 3.3614\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [19840/50000]\tLoss: 3.3015\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [19968/50000]\tLoss: 2.9928\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [20096/50000]\tLoss: 3.1198\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [20224/50000]\tLoss: 3.3015\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [20352/50000]\tLoss: 3.2053\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [20480/50000]\tLoss: 2.9523\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [20608/50000]\tLoss: 3.0152\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [20736/50000]\tLoss: 3.0098\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [20864/50000]\tLoss: 3.0478\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [20992/50000]\tLoss: 3.1364\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [21120/50000]\tLoss: 3.2360\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [21248/50000]\tLoss: 2.7339\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [21376/50000]\tLoss: 3.3125\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [21504/50000]\tLoss: 3.0946\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [21632/50000]\tLoss: 3.0624\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [21760/50000]\tLoss: 3.1451\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [21888/50000]\tLoss: 3.3649\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [22016/50000]\tLoss: 3.2826\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [22144/50000]\tLoss: 3.0349\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [22272/50000]\tLoss: 3.1128\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [22400/50000]\tLoss: 3.0174\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [22528/50000]\tLoss: 3.1185\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [22656/50000]\tLoss: 2.8103\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [22784/50000]\tLoss: 3.1459\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [22912/50000]\tLoss: 3.1456\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [23040/50000]\tLoss: 3.2065\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [23168/50000]\tLoss: 2.9387\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [23296/50000]\tLoss: 3.2135\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [23424/50000]\tLoss: 3.1131\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [23552/50000]\tLoss: 3.1334\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [23680/50000]\tLoss: 3.1484\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [23808/50000]\tLoss: 2.8865\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [23936/50000]\tLoss: 3.2178\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [24064/50000]\tLoss: 3.0505\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [24192/50000]\tLoss: 3.3281\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [24320/50000]\tLoss: 3.0359\tLR: 0.100000\n",
      "start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [24448/50000]\tLoss: 3.2690\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [24576/50000]\tLoss: 3.0059\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [24704/50000]\tLoss: 3.1659\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [24832/50000]\tLoss: 3.1172\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [24960/50000]\tLoss: 3.1681\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [25088/50000]\tLoss: 3.1528\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [25216/50000]\tLoss: 3.1339\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [25344/50000]\tLoss: 3.3069\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [25472/50000]\tLoss: 3.1323\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [25600/50000]\tLoss: 3.0596\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [25728/50000]\tLoss: 2.9496\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [25856/50000]\tLoss: 3.1098\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [25984/50000]\tLoss: 3.3980\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [26112/50000]\tLoss: 3.0986\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [26240/50000]\tLoss: 3.2771\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [26368/50000]\tLoss: 3.3539\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [26496/50000]\tLoss: 3.1224\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [26624/50000]\tLoss: 3.2117\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [26752/50000]\tLoss: 3.2038\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [26880/50000]\tLoss: 3.0579\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [27008/50000]\tLoss: 2.9616\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [27136/50000]\tLoss: 3.1787\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [27264/50000]\tLoss: 3.0327\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [27392/50000]\tLoss: 3.1167\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [27520/50000]\tLoss: 2.8663\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [27648/50000]\tLoss: 3.1462\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [27776/50000]\tLoss: 2.9609\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [27904/50000]\tLoss: 3.4062\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [28032/50000]\tLoss: 2.9513\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [28160/50000]\tLoss: 3.2671\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [28288/50000]\tLoss: 2.8972\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [28416/50000]\tLoss: 3.1008\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [28544/50000]\tLoss: 3.0391\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [28672/50000]\tLoss: 3.1505\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [28800/50000]\tLoss: 3.0738\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [28928/50000]\tLoss: 2.8761\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [29056/50000]\tLoss: 3.1609\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [29184/50000]\tLoss: 2.9301\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [29312/50000]\tLoss: 2.9885\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [29440/50000]\tLoss: 2.8110\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [29568/50000]\tLoss: 3.3243\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [29696/50000]\tLoss: 3.2513\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [29824/50000]\tLoss: 2.9140\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [29952/50000]\tLoss: 3.2968\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [30080/50000]\tLoss: 2.9084\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [30208/50000]\tLoss: 3.2792\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [30336/50000]\tLoss: 3.0803\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [30464/50000]\tLoss: 3.4556\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [30592/50000]\tLoss: 3.2587\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [30720/50000]\tLoss: 3.0016\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [30848/50000]\tLoss: 3.2527\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [30976/50000]\tLoss: 3.1587\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [31104/50000]\tLoss: 3.3024\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [31232/50000]\tLoss: 3.0872\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [31360/50000]\tLoss: 2.9858\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [31488/50000]\tLoss: 3.3792\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [31616/50000]\tLoss: 3.2413\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [31744/50000]\tLoss: 3.4735\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [31872/50000]\tLoss: 3.0073\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [32000/50000]\tLoss: 3.1230\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [32128/50000]\tLoss: 2.8569\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [32256/50000]\tLoss: 3.1153\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [32384/50000]\tLoss: 3.3795\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [32512/50000]\tLoss: 3.2156\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [32640/50000]\tLoss: 3.1306\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [32768/50000]\tLoss: 2.9322\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [32896/50000]\tLoss: 3.3856\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [33024/50000]\tLoss: 3.0432\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [33152/50000]\tLoss: 2.9431\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [33280/50000]\tLoss: 3.1686\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [33408/50000]\tLoss: 2.9778\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [33536/50000]\tLoss: 3.0899\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [33664/50000]\tLoss: 3.1774\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [33792/50000]\tLoss: 2.9852\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [33920/50000]\tLoss: 2.8179\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [34048/50000]\tLoss: 2.9976\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [34176/50000]\tLoss: 3.1461\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [34304/50000]\tLoss: 3.0420\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [34432/50000]\tLoss: 2.9766\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [34560/50000]\tLoss: 3.1394\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [34688/50000]\tLoss: 3.2533\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [34816/50000]\tLoss: 3.1423\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [34944/50000]\tLoss: 2.8810\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [35072/50000]\tLoss: 3.3701\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [35200/50000]\tLoss: 3.3528\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [35328/50000]\tLoss: 3.1044\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [35456/50000]\tLoss: 2.9708\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [35584/50000]\tLoss: 3.1941\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [35712/50000]\tLoss: 3.0660\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [35840/50000]\tLoss: 3.0269\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [35968/50000]\tLoss: 3.0173\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [36096/50000]\tLoss: 2.8811\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [36224/50000]\tLoss: 3.1860\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [36352/50000]\tLoss: 3.1330\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [36480/50000]\tLoss: 3.0333\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [36608/50000]\tLoss: 2.9481\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [36736/50000]\tLoss: 2.9683\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [36864/50000]\tLoss: 3.0494\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [36992/50000]\tLoss: 2.8601\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [37120/50000]\tLoss: 3.0100\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [37248/50000]\tLoss: 2.9493\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [37376/50000]\tLoss: 2.8643\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [37504/50000]\tLoss: 3.0987\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [37632/50000]\tLoss: 2.9956\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [37760/50000]\tLoss: 3.0132\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [37888/50000]\tLoss: 2.8867\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [38016/50000]\tLoss: 3.2811\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [38144/50000]\tLoss: 3.0410\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [38272/50000]\tLoss: 3.2837\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [38400/50000]\tLoss: 3.1941\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [38528/50000]\tLoss: 3.0771\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [38656/50000]\tLoss: 3.1530\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [38784/50000]\tLoss: 3.0996\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [38912/50000]\tLoss: 3.1106\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [39040/50000]\tLoss: 2.7903\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [39168/50000]\tLoss: 3.1550\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [39296/50000]\tLoss: 3.1207\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [39424/50000]\tLoss: 3.1739\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [39552/50000]\tLoss: 3.0424\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [39680/50000]\tLoss: 3.1431\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [39808/50000]\tLoss: 2.9365\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [39936/50000]\tLoss: 3.1195\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [40064/50000]\tLoss: 2.9621\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [40192/50000]\tLoss: 3.0627\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [40320/50000]\tLoss: 3.0753\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [40448/50000]\tLoss: 2.8746\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [40576/50000]\tLoss: 3.0660\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [40704/50000]\tLoss: 2.8564\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [40832/50000]\tLoss: 2.8198\tLR: 0.100000\n",
      "start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 [40960/50000]\tLoss: 2.8683\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [41088/50000]\tLoss: 3.0779\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [41216/50000]\tLoss: 2.7954\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [41344/50000]\tLoss: 2.8430\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [41472/50000]\tLoss: 2.9897\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [41600/50000]\tLoss: 3.0877\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [41728/50000]\tLoss: 2.9299\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [41856/50000]\tLoss: 3.0629\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [41984/50000]\tLoss: 3.1042\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [42112/50000]\tLoss: 3.1813\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [42240/50000]\tLoss: 3.0854\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [42368/50000]\tLoss: 3.1898\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [42496/50000]\tLoss: 2.8447\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [42624/50000]\tLoss: 2.8436\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [42752/50000]\tLoss: 2.9661\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [42880/50000]\tLoss: 3.1152\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [43008/50000]\tLoss: 3.2535\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [43136/50000]\tLoss: 2.7733\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [43264/50000]\tLoss: 3.3184\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [43392/50000]\tLoss: 2.9841\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [43520/50000]\tLoss: 2.8525\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [43648/50000]\tLoss: 3.1067\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [43776/50000]\tLoss: 2.8858\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [43904/50000]\tLoss: 3.0789\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [44032/50000]\tLoss: 3.1979\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [44160/50000]\tLoss: 3.0624\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [44288/50000]\tLoss: 2.9100\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [44416/50000]\tLoss: 2.9744\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [44544/50000]\tLoss: 3.2710\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [44672/50000]\tLoss: 3.1340\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [44800/50000]\tLoss: 3.0239\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [44928/50000]\tLoss: 3.1353\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [45056/50000]\tLoss: 2.8654\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [45184/50000]\tLoss: 3.0093\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [45312/50000]\tLoss: 2.8541\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [45440/50000]\tLoss: 2.9852\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [45568/50000]\tLoss: 3.0665\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [45696/50000]\tLoss: 2.7236\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [45824/50000]\tLoss: 2.9357\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [45952/50000]\tLoss: 3.0981\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [46080/50000]\tLoss: 3.0030\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [46208/50000]\tLoss: 3.0079\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [46336/50000]\tLoss: 2.8272\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [46464/50000]\tLoss: 3.1326\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [46592/50000]\tLoss: 3.0242\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [46720/50000]\tLoss: 2.9798\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [46848/50000]\tLoss: 2.9161\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [46976/50000]\tLoss: 3.0238\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [47104/50000]\tLoss: 3.1298\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [47232/50000]\tLoss: 2.8026\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [47360/50000]\tLoss: 2.9909\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [47488/50000]\tLoss: 3.0472\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [47616/50000]\tLoss: 2.9950\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [47744/50000]\tLoss: 3.1195\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [47872/50000]\tLoss: 3.1062\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [48000/50000]\tLoss: 3.0903\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [48128/50000]\tLoss: 3.1705\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [48256/50000]\tLoss: 2.7379\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [48384/50000]\tLoss: 2.9716\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [48512/50000]\tLoss: 3.1151\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [48640/50000]\tLoss: 2.9706\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [48768/50000]\tLoss: 2.9415\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [48896/50000]\tLoss: 3.0208\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [49024/50000]\tLoss: 3.0618\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [49152/50000]\tLoss: 3.1118\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [49280/50000]\tLoss: 2.9536\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [49408/50000]\tLoss: 2.9123\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [49536/50000]\tLoss: 3.1307\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [49664/50000]\tLoss: 3.0613\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [49792/50000]\tLoss: 3.2341\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [49920/50000]\tLoss: 2.9533\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 3 [50000/50000]\tLoss: 2.7202\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [128/50000]\tLoss: 2.9182\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [256/50000]\tLoss: 2.7812\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [384/50000]\tLoss: 3.0238\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [512/50000]\tLoss: 2.8359\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [640/50000]\tLoss: 2.9292\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [768/50000]\tLoss: 3.0312\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [896/50000]\tLoss: 3.0168\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [1024/50000]\tLoss: 2.8738\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [1152/50000]\tLoss: 2.9396\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [1280/50000]\tLoss: 3.0636\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [1408/50000]\tLoss: 3.2149\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [1536/50000]\tLoss: 2.9415\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [1664/50000]\tLoss: 2.6221\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [1792/50000]\tLoss: 2.8397\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [1920/50000]\tLoss: 2.8436\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [2048/50000]\tLoss: 2.8294\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [2176/50000]\tLoss: 2.9799\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [2304/50000]\tLoss: 2.8921\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [2432/50000]\tLoss: 2.8904\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [2560/50000]\tLoss: 2.9802\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [2688/50000]\tLoss: 2.6932\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [2816/50000]\tLoss: 2.7975\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [2944/50000]\tLoss: 2.7017\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [3072/50000]\tLoss: 2.9251\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [3200/50000]\tLoss: 3.0853\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [3328/50000]\tLoss: 2.9375\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [3456/50000]\tLoss: 3.0580\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [3584/50000]\tLoss: 2.9335\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [3712/50000]\tLoss: 3.0419\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [3840/50000]\tLoss: 2.9113\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [3968/50000]\tLoss: 2.7551\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [4096/50000]\tLoss: 2.9372\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [4224/50000]\tLoss: 2.6425\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [4352/50000]\tLoss: 3.1216\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [4480/50000]\tLoss: 2.7428\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [4608/50000]\tLoss: 2.9220\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [4736/50000]\tLoss: 2.9535\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [4864/50000]\tLoss: 3.0046\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [4992/50000]\tLoss: 2.7891\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [5120/50000]\tLoss: 3.0873\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [5248/50000]\tLoss: 3.0021\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [5376/50000]\tLoss: 2.8801\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [5504/50000]\tLoss: 2.8310\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [5632/50000]\tLoss: 2.8405\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [5760/50000]\tLoss: 2.9102\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [5888/50000]\tLoss: 3.0922\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [6016/50000]\tLoss: 3.1092\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [6144/50000]\tLoss: 2.9238\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [6272/50000]\tLoss: 2.7773\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [6400/50000]\tLoss: 2.8129\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [6528/50000]\tLoss: 2.6559\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [6656/50000]\tLoss: 2.9451\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [6784/50000]\tLoss: 2.5124\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [6912/50000]\tLoss: 2.8943\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [7040/50000]\tLoss: 2.9767\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [7168/50000]\tLoss: 3.0754\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [7296/50000]\tLoss: 2.9336\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [7424/50000]\tLoss: 2.6911\tLR: 0.100000\n",
      "start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [7552/50000]\tLoss: 2.8877\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [7680/50000]\tLoss: 2.9376\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [7808/50000]\tLoss: 3.2932\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [7936/50000]\tLoss: 3.1361\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [8064/50000]\tLoss: 2.7486\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [8192/50000]\tLoss: 2.5825\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [8320/50000]\tLoss: 2.6273\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [8448/50000]\tLoss: 3.0576\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [8576/50000]\tLoss: 2.7458\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [8704/50000]\tLoss: 2.9270\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [8832/50000]\tLoss: 2.9165\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [8960/50000]\tLoss: 2.9224\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [9088/50000]\tLoss: 3.0894\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [9216/50000]\tLoss: 2.9388\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [9344/50000]\tLoss: 3.0582\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [9472/50000]\tLoss: 3.0055\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [9600/50000]\tLoss: 3.0033\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [9728/50000]\tLoss: 2.8566\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [9856/50000]\tLoss: 2.8272\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [9984/50000]\tLoss: 2.8844\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [10112/50000]\tLoss: 3.0069\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [10240/50000]\tLoss: 3.1115\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [10368/50000]\tLoss: 2.9705\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [10496/50000]\tLoss: 2.9358\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [10624/50000]\tLoss: 3.0617\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [10752/50000]\tLoss: 3.1199\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [10880/50000]\tLoss: 2.8952\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [11008/50000]\tLoss: 2.7866\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [11136/50000]\tLoss: 2.9587\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [11264/50000]\tLoss: 3.0543\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [11392/50000]\tLoss: 2.7623\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [11520/50000]\tLoss: 2.8273\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [11648/50000]\tLoss: 3.0696\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [11776/50000]\tLoss: 3.0057\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [11904/50000]\tLoss: 2.9704\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [12032/50000]\tLoss: 2.7393\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [12160/50000]\tLoss: 2.8545\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [12288/50000]\tLoss: 3.0275\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [12416/50000]\tLoss: 3.0880\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [12544/50000]\tLoss: 2.8512\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [12672/50000]\tLoss: 3.0279\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [12800/50000]\tLoss: 2.7976\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [12928/50000]\tLoss: 2.9283\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [13056/50000]\tLoss: 3.0087\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [13184/50000]\tLoss: 2.7262\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [13312/50000]\tLoss: 2.7704\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [13440/50000]\tLoss: 2.9917\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [13568/50000]\tLoss: 2.8163\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [13696/50000]\tLoss: 2.8473\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [13824/50000]\tLoss: 2.8996\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [13952/50000]\tLoss: 2.9094\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [14080/50000]\tLoss: 2.8742\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [14208/50000]\tLoss: 2.8088\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [14336/50000]\tLoss: 2.9288\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [14464/50000]\tLoss: 2.7403\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [14592/50000]\tLoss: 3.0078\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [14720/50000]\tLoss: 2.7292\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [14848/50000]\tLoss: 2.9794\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [14976/50000]\tLoss: 2.8363\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [15104/50000]\tLoss: 2.7355\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [15232/50000]\tLoss: 2.8746\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [15360/50000]\tLoss: 3.1161\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [15488/50000]\tLoss: 2.9710\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [15616/50000]\tLoss: 2.7991\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [15744/50000]\tLoss: 2.9128\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [15872/50000]\tLoss: 2.8467\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [16000/50000]\tLoss: 3.0708\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [16128/50000]\tLoss: 2.8190\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [16256/50000]\tLoss: 2.9597\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [16384/50000]\tLoss: 2.9238\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [16512/50000]\tLoss: 2.8242\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [16640/50000]\tLoss: 2.9738\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [16768/50000]\tLoss: 2.8390\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [16896/50000]\tLoss: 3.0428\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [17024/50000]\tLoss: 2.7530\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [17152/50000]\tLoss: 2.9233\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [17280/50000]\tLoss: 2.9114\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [17408/50000]\tLoss: 2.7814\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [17536/50000]\tLoss: 2.7108\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [17664/50000]\tLoss: 3.0899\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [17792/50000]\tLoss: 2.9637\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [17920/50000]\tLoss: 2.9704\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [18048/50000]\tLoss: 2.8506\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [18176/50000]\tLoss: 2.7322\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [18304/50000]\tLoss: 2.5595\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [18432/50000]\tLoss: 2.7886\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [18560/50000]\tLoss: 2.9314\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [18688/50000]\tLoss: 2.6560\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [18816/50000]\tLoss: 3.0015\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [18944/50000]\tLoss: 3.1445\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [19072/50000]\tLoss: 2.8628\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [19200/50000]\tLoss: 2.8285\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [19328/50000]\tLoss: 3.1121\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [19456/50000]\tLoss: 2.9701\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [19584/50000]\tLoss: 2.8628\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [19712/50000]\tLoss: 2.6394\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [19840/50000]\tLoss: 2.6530\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [19968/50000]\tLoss: 2.7072\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [20096/50000]\tLoss: 2.7017\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [20224/50000]\tLoss: 2.7706\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [20352/50000]\tLoss: 2.7996\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [20480/50000]\tLoss: 2.7330\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [20608/50000]\tLoss: 2.8970\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [20736/50000]\tLoss: 2.6400\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [20864/50000]\tLoss: 2.9063\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [20992/50000]\tLoss: 2.9427\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [21120/50000]\tLoss: 2.8628\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [21248/50000]\tLoss: 2.8587\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [21376/50000]\tLoss: 2.9830\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [21504/50000]\tLoss: 2.8631\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [21632/50000]\tLoss: 3.1335\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [21760/50000]\tLoss: 2.8195\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [21888/50000]\tLoss: 2.5847\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [22016/50000]\tLoss: 2.8036\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [22144/50000]\tLoss: 2.7878\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [22272/50000]\tLoss: 2.8066\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [22400/50000]\tLoss: 2.6768\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [22528/50000]\tLoss: 2.6550\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [22656/50000]\tLoss: 3.0515\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [22784/50000]\tLoss: 2.8459\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [22912/50000]\tLoss: 2.9905\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [23040/50000]\tLoss: 2.9528\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [23168/50000]\tLoss: 2.8378\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [23296/50000]\tLoss: 2.7487\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [23424/50000]\tLoss: 2.6978\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [23552/50000]\tLoss: 2.6772\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [23680/50000]\tLoss: 2.5354\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [23808/50000]\tLoss: 2.8229\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [23936/50000]\tLoss: 2.8725\tLR: 0.100000\n",
      "start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [24064/50000]\tLoss: 2.8710\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [24192/50000]\tLoss: 2.8124\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [24320/50000]\tLoss: 2.8168\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [24448/50000]\tLoss: 2.8483\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [24576/50000]\tLoss: 2.6250\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [24704/50000]\tLoss: 2.9954\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [24832/50000]\tLoss: 2.8468\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [24960/50000]\tLoss: 2.6512\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [25088/50000]\tLoss: 2.9574\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [25216/50000]\tLoss: 2.8663\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [25344/50000]\tLoss: 2.8826\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [25472/50000]\tLoss: 2.9933\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [25600/50000]\tLoss: 2.6744\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [25728/50000]\tLoss: 3.0510\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [25856/50000]\tLoss: 2.8141\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [25984/50000]\tLoss: 2.8458\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [26112/50000]\tLoss: 2.9364\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [26240/50000]\tLoss: 2.8338\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [26368/50000]\tLoss: 2.6197\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [26496/50000]\tLoss: 3.1016\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [26624/50000]\tLoss: 2.8769\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [26752/50000]\tLoss: 3.1089\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [26880/50000]\tLoss: 2.6438\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [27008/50000]\tLoss: 2.8371\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [27136/50000]\tLoss: 3.0960\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [27264/50000]\tLoss: 2.6945\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [27392/50000]\tLoss: 2.8736\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [27520/50000]\tLoss: 3.0210\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [27648/50000]\tLoss: 2.7163\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [27776/50000]\tLoss: 2.7009\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [27904/50000]\tLoss: 2.8416\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [28032/50000]\tLoss: 2.8869\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [28160/50000]\tLoss: 2.8747\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [28288/50000]\tLoss: 2.7112\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [28416/50000]\tLoss: 3.0360\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [28544/50000]\tLoss: 2.7232\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [28672/50000]\tLoss: 2.6301\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [28800/50000]\tLoss: 2.9693\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [28928/50000]\tLoss: 2.9961\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [29056/50000]\tLoss: 2.8745\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [29184/50000]\tLoss: 2.8952\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [29312/50000]\tLoss: 2.9024\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [29440/50000]\tLoss: 2.8491\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [29568/50000]\tLoss: 2.6648\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [29696/50000]\tLoss: 2.7300\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [29824/50000]\tLoss: 2.9060\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [29952/50000]\tLoss: 3.0500\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [30080/50000]\tLoss: 2.8836\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [30208/50000]\tLoss: 3.0253\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [30336/50000]\tLoss: 2.6536\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [30464/50000]\tLoss: 2.9396\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [30592/50000]\tLoss: 2.8831\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [30720/50000]\tLoss: 2.8975\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [30848/50000]\tLoss: 3.0840\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [30976/50000]\tLoss: 2.5184\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [31104/50000]\tLoss: 2.7105\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [31232/50000]\tLoss: 2.8113\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [31360/50000]\tLoss: 2.6860\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [31488/50000]\tLoss: 2.6109\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [31616/50000]\tLoss: 2.7368\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [31744/50000]\tLoss: 3.0151\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [31872/50000]\tLoss: 2.7813\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [32000/50000]\tLoss: 2.5212\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [32128/50000]\tLoss: 2.6969\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [32256/50000]\tLoss: 2.8026\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [32384/50000]\tLoss: 2.6702\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [32512/50000]\tLoss: 2.8885\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [32640/50000]\tLoss: 2.7202\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [32768/50000]\tLoss: 2.8265\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [32896/50000]\tLoss: 2.8333\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [33024/50000]\tLoss: 2.8275\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [33152/50000]\tLoss: 2.7301\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [33280/50000]\tLoss: 2.9541\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [33408/50000]\tLoss: 2.8286\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [33536/50000]\tLoss: 2.8116\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [33664/50000]\tLoss: 2.9555\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [33792/50000]\tLoss: 2.6310\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [33920/50000]\tLoss: 2.7723\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [34048/50000]\tLoss: 2.7191\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [34176/50000]\tLoss: 2.9363\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [34304/50000]\tLoss: 2.7691\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [34432/50000]\tLoss: 2.6147\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [34560/50000]\tLoss: 2.8760\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [34688/50000]\tLoss: 2.8012\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [34816/50000]\tLoss: 2.7430\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [34944/50000]\tLoss: 2.8935\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [35072/50000]\tLoss: 2.6083\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [35200/50000]\tLoss: 2.6415\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [35328/50000]\tLoss: 2.9667\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [35456/50000]\tLoss: 2.8864\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [35584/50000]\tLoss: 2.6182\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [35712/50000]\tLoss: 2.7577\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [35840/50000]\tLoss: 3.1239\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [35968/50000]\tLoss: 2.9453\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [36096/50000]\tLoss: 2.7184\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [36224/50000]\tLoss: 2.7644\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [36352/50000]\tLoss: 2.7742\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [36480/50000]\tLoss: 2.7657\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [36608/50000]\tLoss: 2.9724\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [36736/50000]\tLoss: 2.7194\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [36864/50000]\tLoss: 2.6932\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [36992/50000]\tLoss: 2.8283\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [37120/50000]\tLoss: 3.1919\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [37248/50000]\tLoss: 2.8688\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [37376/50000]\tLoss: 2.8184\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [37504/50000]\tLoss: 2.6143\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [37632/50000]\tLoss: 2.6206\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [37760/50000]\tLoss: 2.6960\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [37888/50000]\tLoss: 2.6581\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [38016/50000]\tLoss: 2.6637\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [38144/50000]\tLoss: 2.5661\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [38272/50000]\tLoss: 2.6036\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [38400/50000]\tLoss: 2.8476\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [38528/50000]\tLoss: 2.7560\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [38656/50000]\tLoss: 2.5406\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [38784/50000]\tLoss: 2.9746\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [38912/50000]\tLoss: 2.7488\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [39040/50000]\tLoss: 3.0079\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [39168/50000]\tLoss: 2.7409\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [39296/50000]\tLoss: 2.8654\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [39424/50000]\tLoss: 2.8526\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [39552/50000]\tLoss: 2.9188\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [39680/50000]\tLoss: 2.6618\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [39808/50000]\tLoss: 2.8025\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [39936/50000]\tLoss: 2.8338\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [40064/50000]\tLoss: 2.6854\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [40192/50000]\tLoss: 2.6845\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [40320/50000]\tLoss: 2.8205\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [40448/50000]\tLoss: 2.8573\tLR: 0.100000\n",
      "start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 [40576/50000]\tLoss: 2.8411\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [40704/50000]\tLoss: 2.7784\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [40832/50000]\tLoss: 2.6545\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [40960/50000]\tLoss: 2.6468\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [41088/50000]\tLoss: 2.8048\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [41216/50000]\tLoss: 2.9786\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [41344/50000]\tLoss: 2.5618\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [41472/50000]\tLoss: 2.7349\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [41600/50000]\tLoss: 2.9070\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [41728/50000]\tLoss: 2.6887\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [41856/50000]\tLoss: 2.6155\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [41984/50000]\tLoss: 2.8079\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [42112/50000]\tLoss: 2.6336\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [42240/50000]\tLoss: 2.6540\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [42368/50000]\tLoss: 2.8730\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [42496/50000]\tLoss: 2.8058\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [42624/50000]\tLoss: 2.7363\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [42752/50000]\tLoss: 2.6523\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [42880/50000]\tLoss: 2.5210\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [43008/50000]\tLoss: 2.7399\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [43136/50000]\tLoss: 2.9853\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [43264/50000]\tLoss: 2.9600\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [43392/50000]\tLoss: 2.6730\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [43520/50000]\tLoss: 2.7602\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [43648/50000]\tLoss: 3.0523\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [43776/50000]\tLoss: 2.6406\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [43904/50000]\tLoss: 2.8650\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [44032/50000]\tLoss: 2.9659\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [44160/50000]\tLoss: 2.8067\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [44288/50000]\tLoss: 2.8661\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [44416/50000]\tLoss: 3.0615\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [44544/50000]\tLoss: 2.8135\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [44672/50000]\tLoss: 3.0231\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [44800/50000]\tLoss: 2.6484\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [44928/50000]\tLoss: 2.6419\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [45056/50000]\tLoss: 2.9168\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [45184/50000]\tLoss: 2.6384\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [45312/50000]\tLoss: 2.8955\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [45440/50000]\tLoss: 2.6246\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [45568/50000]\tLoss: 2.7593\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [45696/50000]\tLoss: 2.6538\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [45824/50000]\tLoss: 2.8805\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [45952/50000]\tLoss: 2.6721\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [46080/50000]\tLoss: 2.6430\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [46208/50000]\tLoss: 2.8345\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [46336/50000]\tLoss: 2.6729\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [46464/50000]\tLoss: 3.1102\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [46592/50000]\tLoss: 2.6466\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [46720/50000]\tLoss: 2.6461\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [46848/50000]\tLoss: 2.6477\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [46976/50000]\tLoss: 2.8928\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [47104/50000]\tLoss: 2.7216\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [47232/50000]\tLoss: 2.9720\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [47360/50000]\tLoss: 2.8363\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [47488/50000]\tLoss: 2.5915\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [47616/50000]\tLoss: 2.8659\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [47744/50000]\tLoss: 2.7767\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [47872/50000]\tLoss: 2.9829\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [48000/50000]\tLoss: 3.0660\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [48128/50000]\tLoss: 2.6458\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [48256/50000]\tLoss: 2.7853\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [48384/50000]\tLoss: 2.9835\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [48512/50000]\tLoss: 2.5400\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [48640/50000]\tLoss: 2.6527\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [48768/50000]\tLoss: 2.7265\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [48896/50000]\tLoss: 2.5687\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [49024/50000]\tLoss: 2.6323\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [49152/50000]\tLoss: 2.6164\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [49280/50000]\tLoss: 2.6108\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [49408/50000]\tLoss: 2.6130\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [49536/50000]\tLoss: 2.9108\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [49664/50000]\tLoss: 2.7100\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [49792/50000]\tLoss: 2.6696\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [49920/50000]\tLoss: 2.5801\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 4 [50000/50000]\tLoss: 2.6358\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [128/50000]\tLoss: 2.6695\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [256/50000]\tLoss: 2.7777\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [384/50000]\tLoss: 2.7063\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [512/50000]\tLoss: 2.5881\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [640/50000]\tLoss: 2.5262\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [768/50000]\tLoss: 2.5576\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [896/50000]\tLoss: 2.5814\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [1024/50000]\tLoss: 2.7823\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [1152/50000]\tLoss: 2.7858\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [1280/50000]\tLoss: 2.8454\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [1408/50000]\tLoss: 2.6662\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [1536/50000]\tLoss: 2.7585\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [1664/50000]\tLoss: 2.5081\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [1792/50000]\tLoss: 2.6949\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [1920/50000]\tLoss: 2.5301\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [2048/50000]\tLoss: 2.6805\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [2176/50000]\tLoss: 2.6180\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [2304/50000]\tLoss: 2.6443\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [2432/50000]\tLoss: 2.4941\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [2560/50000]\tLoss: 2.5725\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [2688/50000]\tLoss: 2.7863\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [2816/50000]\tLoss: 2.8399\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [2944/50000]\tLoss: 2.9024\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [3072/50000]\tLoss: 2.6900\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [3200/50000]\tLoss: 2.8394\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [3328/50000]\tLoss: 3.0767\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [3456/50000]\tLoss: 2.6668\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [3584/50000]\tLoss: 2.5287\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [3712/50000]\tLoss: 2.7769\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [3840/50000]\tLoss: 2.7144\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [3968/50000]\tLoss: 2.7950\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [4096/50000]\tLoss: 2.5785\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [4224/50000]\tLoss: 2.4508\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [4352/50000]\tLoss: 2.7243\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [4480/50000]\tLoss: 2.9716\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [4608/50000]\tLoss: 2.3285\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [4736/50000]\tLoss: 2.5178\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [4864/50000]\tLoss: 2.4802\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [4992/50000]\tLoss: 2.4134\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [5120/50000]\tLoss: 2.8876\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [5248/50000]\tLoss: 2.5092\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [5376/50000]\tLoss: 2.4792\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [5504/50000]\tLoss: 2.7422\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [5632/50000]\tLoss: 2.7920\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [5760/50000]\tLoss: 2.6912\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [5888/50000]\tLoss: 2.4622\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [6016/50000]\tLoss: 2.8657\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [6144/50000]\tLoss: 2.7884\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [6272/50000]\tLoss: 2.8060\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [6400/50000]\tLoss: 2.4559\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [6528/50000]\tLoss: 2.6802\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [6656/50000]\tLoss: 2.8658\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [6784/50000]\tLoss: 2.5225\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [6912/50000]\tLoss: 2.3899\tLR: 0.100000\n",
      "start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 5 [7040/50000]\tLoss: 2.6510\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [7168/50000]\tLoss: 2.8825\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [7296/50000]\tLoss: 2.5860\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [7424/50000]\tLoss: 2.6051\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [7552/50000]\tLoss: 2.6728\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [7680/50000]\tLoss: 2.6309\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [7808/50000]\tLoss: 2.7713\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [7936/50000]\tLoss: 2.6268\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [8064/50000]\tLoss: 2.5467\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [8192/50000]\tLoss: 2.9341\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [8320/50000]\tLoss: 2.9059\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [8448/50000]\tLoss: 2.7750\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [8576/50000]\tLoss: 2.4703\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [8704/50000]\tLoss: 2.4900\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [8832/50000]\tLoss: 2.8228\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [8960/50000]\tLoss: 2.5713\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [9088/50000]\tLoss: 2.5814\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [9216/50000]\tLoss: 2.6211\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [9344/50000]\tLoss: 2.6510\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [9472/50000]\tLoss: 2.5742\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [9600/50000]\tLoss: 2.6447\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [9728/50000]\tLoss: 2.6796\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [9856/50000]\tLoss: 2.9129\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [9984/50000]\tLoss: 2.6453\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [10112/50000]\tLoss: 2.5939\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [10240/50000]\tLoss: 2.8650\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [10368/50000]\tLoss: 2.5742\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [10496/50000]\tLoss: 2.5554\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [10624/50000]\tLoss: 2.5978\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [10752/50000]\tLoss: 2.4460\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [10880/50000]\tLoss: 2.6856\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [11008/50000]\tLoss: 2.7039\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [11136/50000]\tLoss: 2.7209\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [11264/50000]\tLoss: 2.7200\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [11392/50000]\tLoss: 2.5688\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [11520/50000]\tLoss: 2.5399\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [11648/50000]\tLoss: 2.5356\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [11776/50000]\tLoss: 2.6219\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [11904/50000]\tLoss: 2.4888\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [12032/50000]\tLoss: 2.6015\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [12160/50000]\tLoss: 2.7046\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [12288/50000]\tLoss: 2.5282\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [12416/50000]\tLoss: 2.6644\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [12544/50000]\tLoss: 2.6335\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [12672/50000]\tLoss: 2.6437\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [12800/50000]\tLoss: 2.6359\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [12928/50000]\tLoss: 2.5347\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [13056/50000]\tLoss: 2.7264\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [13184/50000]\tLoss: 2.3720\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [13312/50000]\tLoss: 2.8239\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [13440/50000]\tLoss: 2.9653\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [13568/50000]\tLoss: 2.8550\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [13696/50000]\tLoss: 2.6054\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [13824/50000]\tLoss: 2.8290\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [13952/50000]\tLoss: 2.4834\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [14080/50000]\tLoss: 2.4552\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [14208/50000]\tLoss: 2.6829\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [14336/50000]\tLoss: 2.7264\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [14464/50000]\tLoss: 2.6124\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [14592/50000]\tLoss: 2.6654\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [14720/50000]\tLoss: 2.9172\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [14848/50000]\tLoss: 2.6767\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [14976/50000]\tLoss: 2.5096\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [15104/50000]\tLoss: 2.7822\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [15232/50000]\tLoss: 2.4986\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [15360/50000]\tLoss: 2.7191\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [15488/50000]\tLoss: 2.6602\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [15616/50000]\tLoss: 2.6510\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [15744/50000]\tLoss: 2.5549\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [15872/50000]\tLoss: 2.8616\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [16000/50000]\tLoss: 2.6235\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [16128/50000]\tLoss: 2.5854\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [16256/50000]\tLoss: 2.5976\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [16384/50000]\tLoss: 2.5194\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [16512/50000]\tLoss: 2.6846\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [16640/50000]\tLoss: 2.8806\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [16768/50000]\tLoss: 2.4047\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [16896/50000]\tLoss: 2.5970\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [17024/50000]\tLoss: 2.7648\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [17152/50000]\tLoss: 2.7615\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [17280/50000]\tLoss: 2.4326\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [17408/50000]\tLoss: 2.6066\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [17536/50000]\tLoss: 2.4958\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [17664/50000]\tLoss: 2.7545\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [17792/50000]\tLoss: 2.7984\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [17920/50000]\tLoss: 2.7723\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [18048/50000]\tLoss: 2.4294\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [18176/50000]\tLoss: 2.7493\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [18304/50000]\tLoss: 2.6468\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [18432/50000]\tLoss: 2.5969\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [18560/50000]\tLoss: 2.7675\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [18688/50000]\tLoss: 2.8449\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [18816/50000]\tLoss: 2.5658\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [18944/50000]\tLoss: 2.5799\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [19072/50000]\tLoss: 2.6919\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [19200/50000]\tLoss: 2.5927\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [19328/50000]\tLoss: 2.7067\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [19456/50000]\tLoss: 2.9723\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [19584/50000]\tLoss: 2.3584\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [19712/50000]\tLoss: 2.4877\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [19840/50000]\tLoss: 2.6573\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [19968/50000]\tLoss: 2.7515\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [20096/50000]\tLoss: 2.6940\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [20224/50000]\tLoss: 2.5016\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [20352/50000]\tLoss: 2.5049\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [20480/50000]\tLoss: 2.6320\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [20608/50000]\tLoss: 2.4834\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [20736/50000]\tLoss: 2.6201\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [20864/50000]\tLoss: 2.6254\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [20992/50000]\tLoss: 2.7566\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [21120/50000]\tLoss: 2.3167\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [21248/50000]\tLoss: 2.4924\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [21376/50000]\tLoss: 2.5647\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [21504/50000]\tLoss: 2.5933\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [21632/50000]\tLoss: 2.5964\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [21760/50000]\tLoss: 2.6307\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [21888/50000]\tLoss: 2.4626\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [22016/50000]\tLoss: 2.4911\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [22144/50000]\tLoss: 2.7035\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [22272/50000]\tLoss: 2.7283\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [22400/50000]\tLoss: 2.7399\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [22528/50000]\tLoss: 2.6923\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [22656/50000]\tLoss: 2.7571\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [22784/50000]\tLoss: 2.5972\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [22912/50000]\tLoss: 2.6335\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [23040/50000]\tLoss: 2.4477\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [23168/50000]\tLoss: 2.6802\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [23296/50000]\tLoss: 2.8285\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [23424/50000]\tLoss: 2.7500\tLR: 0.100000\n",
      "start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 5 [23552/50000]\tLoss: 2.5463\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [23680/50000]\tLoss: 2.7007\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [23808/50000]\tLoss: 2.5706\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [23936/50000]\tLoss: 2.6530\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [24064/50000]\tLoss: 2.6851\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [24192/50000]\tLoss: 2.5804\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [24320/50000]\tLoss: 2.6992\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [24448/50000]\tLoss: 2.5157\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [24576/50000]\tLoss: 2.8598\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [24704/50000]\tLoss: 2.6140\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [24832/50000]\tLoss: 2.9450\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [24960/50000]\tLoss: 2.4952\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [25088/50000]\tLoss: 2.6650\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [25216/50000]\tLoss: 2.6270\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [25344/50000]\tLoss: 2.4579\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [25472/50000]\tLoss: 2.4751\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [25600/50000]\tLoss: 2.6760\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [25728/50000]\tLoss: 2.5045\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [25856/50000]\tLoss: 2.7922\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [25984/50000]\tLoss: 2.6686\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [26112/50000]\tLoss: 2.6524\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [26240/50000]\tLoss: 2.6462\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [26368/50000]\tLoss: 2.4850\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [26496/50000]\tLoss: 2.6966\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [26624/50000]\tLoss: 2.6279\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [26752/50000]\tLoss: 2.5441\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [26880/50000]\tLoss: 2.7579\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [27008/50000]\tLoss: 2.7501\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [27136/50000]\tLoss: 2.4094\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [27264/50000]\tLoss: 2.5819\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [27392/50000]\tLoss: 2.4647\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [27520/50000]\tLoss: 2.7102\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [27648/50000]\tLoss: 2.9445\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [27776/50000]\tLoss: 2.6767\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [27904/50000]\tLoss: 2.4989\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [28032/50000]\tLoss: 2.5511\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [28160/50000]\tLoss: 2.7063\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [28288/50000]\tLoss: 2.6432\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [28416/50000]\tLoss: 2.5461\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [28544/50000]\tLoss: 2.3622\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [28672/50000]\tLoss: 2.5638\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [28800/50000]\tLoss: 2.5633\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [28928/50000]\tLoss: 2.6723\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [29056/50000]\tLoss: 2.4511\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [29184/50000]\tLoss: 2.8480\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [29312/50000]\tLoss: 2.6671\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [29440/50000]\tLoss: 2.6084\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [29568/50000]\tLoss: 2.4216\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [29696/50000]\tLoss: 2.5655\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [29824/50000]\tLoss: 2.3802\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [29952/50000]\tLoss: 2.7960\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [30080/50000]\tLoss: 2.5525\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [30208/50000]\tLoss: 2.5644\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [30336/50000]\tLoss: 2.4569\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [30464/50000]\tLoss: 2.6056\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [30592/50000]\tLoss: 2.6955\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [30720/50000]\tLoss: 2.7461\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [30848/50000]\tLoss: 2.6372\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [30976/50000]\tLoss: 2.5241\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [31104/50000]\tLoss: 2.6411\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [31232/50000]\tLoss: 2.5586\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [31360/50000]\tLoss: 2.6670\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [31488/50000]\tLoss: 2.6524\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [31616/50000]\tLoss: 2.5220\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [31744/50000]\tLoss: 2.7343\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [31872/50000]\tLoss: 2.5471\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [32000/50000]\tLoss: 2.5896\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [32128/50000]\tLoss: 2.7931\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [32256/50000]\tLoss: 2.3370\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [32384/50000]\tLoss: 2.7497\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [32512/50000]\tLoss: 2.7837\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [32640/50000]\tLoss: 2.6008\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [32768/50000]\tLoss: 2.3791\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [32896/50000]\tLoss: 2.4860\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [33024/50000]\tLoss: 2.5888\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [33152/50000]\tLoss: 2.4072\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [33280/50000]\tLoss: 2.3935\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [33408/50000]\tLoss: 2.7763\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [33536/50000]\tLoss: 2.6205\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [33664/50000]\tLoss: 2.9123\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [33792/50000]\tLoss: 2.4196\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [33920/50000]\tLoss: 2.7025\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [34048/50000]\tLoss: 2.5346\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [34176/50000]\tLoss: 2.3265\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [34304/50000]\tLoss: 2.6709\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [34432/50000]\tLoss: 2.5567\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [34560/50000]\tLoss: 2.3715\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [34688/50000]\tLoss: 2.7461\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [34816/50000]\tLoss: 3.0580\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [34944/50000]\tLoss: 2.6940\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [35072/50000]\tLoss: 2.6458\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [35200/50000]\tLoss: 2.4286\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [35328/50000]\tLoss: 2.5698\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [35456/50000]\tLoss: 2.6128\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [35584/50000]\tLoss: 2.7275\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [35712/50000]\tLoss: 2.5617\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [35840/50000]\tLoss: 2.6005\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [35968/50000]\tLoss: 2.3822\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [36096/50000]\tLoss: 2.5888\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [36224/50000]\tLoss: 2.6591\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [36352/50000]\tLoss: 2.6207\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [36480/50000]\tLoss: 2.6410\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [36608/50000]\tLoss: 2.5360\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [36736/50000]\tLoss: 2.4605\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [36864/50000]\tLoss: 2.4078\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [36992/50000]\tLoss: 2.2906\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [37120/50000]\tLoss: 2.6196\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [37248/50000]\tLoss: 2.5422\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [37376/50000]\tLoss: 2.6004\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [37504/50000]\tLoss: 2.3112\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [37632/50000]\tLoss: 2.5049\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [37760/50000]\tLoss: 2.4576\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [37888/50000]\tLoss: 2.5647\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [38016/50000]\tLoss: 2.6902\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [38144/50000]\tLoss: 2.6184\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [38272/50000]\tLoss: 2.6191\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [38400/50000]\tLoss: 2.6935\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [38528/50000]\tLoss: 2.5131\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [38656/50000]\tLoss: 2.7772\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [38784/50000]\tLoss: 2.7716\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [38912/50000]\tLoss: 2.4824\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [39040/50000]\tLoss: 2.5162\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [39168/50000]\tLoss: 2.6503\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [39296/50000]\tLoss: 2.6165\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [39424/50000]\tLoss: 2.6115\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [39552/50000]\tLoss: 2.6725\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [39680/50000]\tLoss: 2.5270\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [39808/50000]\tLoss: 2.5387\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [39936/50000]\tLoss: 2.7642\tLR: 0.100000\n",
      "start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 5 [40064/50000]\tLoss: 2.4874\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [40192/50000]\tLoss: 2.4055\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [40320/50000]\tLoss: 2.5133\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [40448/50000]\tLoss: 2.7078\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [40576/50000]\tLoss: 2.3703\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [40704/50000]\tLoss: 2.4731\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [40832/50000]\tLoss: 2.4861\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [40960/50000]\tLoss: 2.8350\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [41088/50000]\tLoss: 2.6752\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [41216/50000]\tLoss: 2.4704\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [41344/50000]\tLoss: 2.6862\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [41472/50000]\tLoss: 2.5425\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [41600/50000]\tLoss: 2.6562\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [41728/50000]\tLoss: 2.5413\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [41856/50000]\tLoss: 2.4913\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [41984/50000]\tLoss: 2.5072\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [42112/50000]\tLoss: 2.8160\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [42240/50000]\tLoss: 2.6869\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [42368/50000]\tLoss: 2.7288\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [42496/50000]\tLoss: 2.9242\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [42624/50000]\tLoss: 2.4215\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [42752/50000]\tLoss: 2.5221\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [42880/50000]\tLoss: 2.6191\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [43008/50000]\tLoss: 2.6521\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [43136/50000]\tLoss: 2.6650\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [43264/50000]\tLoss: 2.6005\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [43392/50000]\tLoss: 2.7319\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [43520/50000]\tLoss: 2.8449\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [43648/50000]\tLoss: 2.7119\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [43776/50000]\tLoss: 2.7270\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [43904/50000]\tLoss: 2.6185\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [44032/50000]\tLoss: 2.7938\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [44160/50000]\tLoss: 2.7064\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [44288/50000]\tLoss: 2.6457\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [44416/50000]\tLoss: 2.6794\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [44544/50000]\tLoss: 2.7777\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [44672/50000]\tLoss: 2.3833\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [44800/50000]\tLoss: 2.6161\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [44928/50000]\tLoss: 2.3810\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [45056/50000]\tLoss: 2.4811\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [45184/50000]\tLoss: 2.6309\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [45312/50000]\tLoss: 2.5466\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [45440/50000]\tLoss: 2.4670\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [45568/50000]\tLoss: 2.5043\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [45696/50000]\tLoss: 2.7107\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [45824/50000]\tLoss: 2.5412\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [45952/50000]\tLoss: 2.4460\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [46080/50000]\tLoss: 2.9284\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [46208/50000]\tLoss: 2.6790\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [46336/50000]\tLoss: 2.6632\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [46464/50000]\tLoss: 2.3785\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [46592/50000]\tLoss: 2.5073\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [46720/50000]\tLoss: 2.7345\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [46848/50000]\tLoss: 2.6081\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [46976/50000]\tLoss: 2.5477\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [47104/50000]\tLoss: 2.4568\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [47232/50000]\tLoss: 2.8280\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [47360/50000]\tLoss: 2.5731\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [47488/50000]\tLoss: 2.6396\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [47616/50000]\tLoss: 2.7406\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [47744/50000]\tLoss: 2.4297\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [47872/50000]\tLoss: 2.7029\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [48000/50000]\tLoss: 2.5514\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [48128/50000]\tLoss: 2.5813\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [48256/50000]\tLoss: 2.5007\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [48384/50000]\tLoss: 2.2004\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [48512/50000]\tLoss: 2.7615\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [48640/50000]\tLoss: 2.6303\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [48768/50000]\tLoss: 2.5246\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [48896/50000]\tLoss: 2.8044\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [49024/50000]\tLoss: 2.3012\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [49152/50000]\tLoss: 2.5749\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [49280/50000]\tLoss: 2.2819\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [49408/50000]\tLoss: 2.4867\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [49536/50000]\tLoss: 2.8708\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [49664/50000]\tLoss: 2.7191\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [49792/50000]\tLoss: 2.5692\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [49920/50000]\tLoss: 2.7402\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 5 [50000/50000]\tLoss: 2.5835\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [128/50000]\tLoss: 2.4139\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [256/50000]\tLoss: 2.2790\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [384/50000]\tLoss: 2.3531\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [512/50000]\tLoss: 2.2913\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [640/50000]\tLoss: 2.2949\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [768/50000]\tLoss: 2.7703\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [896/50000]\tLoss: 2.2636\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [1024/50000]\tLoss: 2.6195\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [1152/50000]\tLoss: 2.3102\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [1280/50000]\tLoss: 2.1944\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [1408/50000]\tLoss: 2.4117\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [1536/50000]\tLoss: 2.4193\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [1664/50000]\tLoss: 2.3310\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [1792/50000]\tLoss: 2.1376\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [1920/50000]\tLoss: 2.2876\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [2048/50000]\tLoss: 2.6344\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [2176/50000]\tLoss: 2.7280\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [2304/50000]\tLoss: 2.6424\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [2432/50000]\tLoss: 2.5022\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [2560/50000]\tLoss: 2.6008\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [2688/50000]\tLoss: 2.6268\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [2816/50000]\tLoss: 2.3898\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [2944/50000]\tLoss: 2.5129\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [3072/50000]\tLoss: 2.3888\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [3200/50000]\tLoss: 2.6844\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [3328/50000]\tLoss: 2.5981\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [3456/50000]\tLoss: 2.5124\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [3584/50000]\tLoss: 2.6960\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [3712/50000]\tLoss: 2.5683\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [3840/50000]\tLoss: 2.2671\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [3968/50000]\tLoss: 2.6287\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [4096/50000]\tLoss: 2.5236\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [4224/50000]\tLoss: 2.5114\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [4352/50000]\tLoss: 2.7765\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [4480/50000]\tLoss: 2.5530\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [4608/50000]\tLoss: 2.4319\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [4736/50000]\tLoss: 2.5531\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [4864/50000]\tLoss: 2.2657\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [4992/50000]\tLoss: 2.3383\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [5120/50000]\tLoss: 2.1863\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [5248/50000]\tLoss: 2.6305\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [5376/50000]\tLoss: 2.7700\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [5504/50000]\tLoss: 2.4273\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [5632/50000]\tLoss: 2.6405\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [5760/50000]\tLoss: 2.4353\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [5888/50000]\tLoss: 2.4254\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [6016/50000]\tLoss: 2.3704\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [6144/50000]\tLoss: 2.4600\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [6272/50000]\tLoss: 2.2851\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [6400/50000]\tLoss: 2.5399\tLR: 0.100000\n",
      "start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 6 [6528/50000]\tLoss: 2.5739\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [6656/50000]\tLoss: 2.4999\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [6784/50000]\tLoss: 2.5938\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [6912/50000]\tLoss: 2.3523\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [7040/50000]\tLoss: 2.4366\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [7168/50000]\tLoss: 2.6160\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [7296/50000]\tLoss: 2.5680\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [7424/50000]\tLoss: 2.5611\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [7552/50000]\tLoss: 2.5245\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [7680/50000]\tLoss: 2.4255\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [7808/50000]\tLoss: 2.4307\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [7936/50000]\tLoss: 2.2740\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [8064/50000]\tLoss: 2.4414\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [8192/50000]\tLoss: 2.3130\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [8320/50000]\tLoss: 2.3667\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [8448/50000]\tLoss: 2.2768\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [8576/50000]\tLoss: 2.5451\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [8704/50000]\tLoss: 2.4354\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [8832/50000]\tLoss: 2.2504\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [8960/50000]\tLoss: 2.5569\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [9088/50000]\tLoss: 2.4306\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [9216/50000]\tLoss: 2.7279\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [9344/50000]\tLoss: 2.4681\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [9472/50000]\tLoss: 2.3316\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [9600/50000]\tLoss: 2.4010\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [9728/50000]\tLoss: 2.3479\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [9856/50000]\tLoss: 2.3383\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [9984/50000]\tLoss: 2.2674\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [10112/50000]\tLoss: 2.4314\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [10240/50000]\tLoss: 2.5771\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [10368/50000]\tLoss: 2.4355\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [10496/50000]\tLoss: 2.6568\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [10624/50000]\tLoss: 2.3142\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [10752/50000]\tLoss: 2.4486\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [10880/50000]\tLoss: 2.7786\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [11008/50000]\tLoss: 2.4724\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [11136/50000]\tLoss: 2.4887\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [11264/50000]\tLoss: 2.5099\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [11392/50000]\tLoss: 2.5180\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [11520/50000]\tLoss: 2.5140\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [11648/50000]\tLoss: 2.3555\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [11776/50000]\tLoss: 2.3239\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [11904/50000]\tLoss: 2.7078\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [12032/50000]\tLoss: 2.4678\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [12160/50000]\tLoss: 2.3923\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [12288/50000]\tLoss: 2.6319\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [12416/50000]\tLoss: 2.3338\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [12544/50000]\tLoss: 2.5321\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [12672/50000]\tLoss: 2.7196\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [12800/50000]\tLoss: 2.6665\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [12928/50000]\tLoss: 2.6229\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [13056/50000]\tLoss: 2.5058\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [13184/50000]\tLoss: 2.4159\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [13312/50000]\tLoss: 2.2630\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [13440/50000]\tLoss: 2.5704\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [13568/50000]\tLoss: 2.4944\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [13696/50000]\tLoss: 2.5962\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [13824/50000]\tLoss: 2.3419\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [13952/50000]\tLoss: 2.4782\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [14080/50000]\tLoss: 2.5441\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [14208/50000]\tLoss: 2.3067\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [14336/50000]\tLoss: 2.4822\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [14464/50000]\tLoss: 2.3012\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [14592/50000]\tLoss: 2.6895\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [14720/50000]\tLoss: 2.5204\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [14848/50000]\tLoss: 2.7241\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [14976/50000]\tLoss: 2.7174\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [15104/50000]\tLoss: 2.5485\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [15232/50000]\tLoss: 2.3856\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [15360/50000]\tLoss: 2.3784\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [15488/50000]\tLoss: 2.4012\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [15616/50000]\tLoss: 2.5340\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [15744/50000]\tLoss: 2.4934\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [15872/50000]\tLoss: 2.2602\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [16000/50000]\tLoss: 2.6069\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [16128/50000]\tLoss: 2.0447\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [16256/50000]\tLoss: 2.3516\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [16384/50000]\tLoss: 2.5355\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [16512/50000]\tLoss: 2.3359\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [16640/50000]\tLoss: 2.4006\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [16768/50000]\tLoss: 2.6643\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [16896/50000]\tLoss: 2.5896\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [17024/50000]\tLoss: 2.7520\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [17152/50000]\tLoss: 2.6633\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [17280/50000]\tLoss: 2.2058\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [17408/50000]\tLoss: 2.6359\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [17536/50000]\tLoss: 2.4162\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [17664/50000]\tLoss: 2.2954\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [17792/50000]\tLoss: 2.3279\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [17920/50000]\tLoss: 2.4998\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [18048/50000]\tLoss: 2.7422\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [18176/50000]\tLoss: 2.4006\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [18304/50000]\tLoss: 2.7521\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [18432/50000]\tLoss: 2.6373\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [18560/50000]\tLoss: 2.3541\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [18688/50000]\tLoss: 2.4162\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [18816/50000]\tLoss: 2.2821\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [18944/50000]\tLoss: 2.4128\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [19072/50000]\tLoss: 2.4537\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [19200/50000]\tLoss: 2.3742\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [19328/50000]\tLoss: 2.8089\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [19456/50000]\tLoss: 2.2959\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [19584/50000]\tLoss: 2.2854\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [19712/50000]\tLoss: 2.4366\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [19840/50000]\tLoss: 2.4272\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [19968/50000]\tLoss: 2.5877\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [20096/50000]\tLoss: 2.6010\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [20224/50000]\tLoss: 2.6438\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [20352/50000]\tLoss: 2.3745\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [20480/50000]\tLoss: 2.5306\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [20608/50000]\tLoss: 2.5004\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [20736/50000]\tLoss: 2.6225\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [20864/50000]\tLoss: 2.3880\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [20992/50000]\tLoss: 2.5395\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [21120/50000]\tLoss: 2.6027\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [21248/50000]\tLoss: 2.4763\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [21376/50000]\tLoss: 2.5144\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [21504/50000]\tLoss: 2.4588\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [21632/50000]\tLoss: 2.4878\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [21760/50000]\tLoss: 2.5439\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [21888/50000]\tLoss: 2.4640\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [22016/50000]\tLoss: 2.5105\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [22144/50000]\tLoss: 2.3052\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [22272/50000]\tLoss: 2.6635\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [22400/50000]\tLoss: 2.4677\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [22528/50000]\tLoss: 2.6004\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [22656/50000]\tLoss: 2.6124\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [22784/50000]\tLoss: 2.5996\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [22912/50000]\tLoss: 2.5183\tLR: 0.100000\n",
      "start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 6 [23040/50000]\tLoss: 2.4519\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [23168/50000]\tLoss: 2.4306\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [23296/50000]\tLoss: 2.5694\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [23424/50000]\tLoss: 2.6083\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [23552/50000]\tLoss: 2.5271\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [23680/50000]\tLoss: 2.3569\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [23808/50000]\tLoss: 2.3862\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [23936/50000]\tLoss: 2.6541\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [24064/50000]\tLoss: 2.4871\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [24192/50000]\tLoss: 2.4517\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [24320/50000]\tLoss: 2.3923\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [24448/50000]\tLoss: 2.4800\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [24576/50000]\tLoss: 2.5169\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [24704/50000]\tLoss: 2.6266\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [24832/50000]\tLoss: 2.3361\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [24960/50000]\tLoss: 2.4481\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [25088/50000]\tLoss: 2.5102\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [25216/50000]\tLoss: 2.2572\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [25344/50000]\tLoss: 2.5721\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [25472/50000]\tLoss: 2.4421\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [25600/50000]\tLoss: 2.4194\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [25728/50000]\tLoss: 2.4515\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [25856/50000]\tLoss: 2.2986\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [25984/50000]\tLoss: 2.4504\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [26112/50000]\tLoss: 2.6736\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [26240/50000]\tLoss: 2.4522\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [26368/50000]\tLoss: 2.4965\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [26496/50000]\tLoss: 2.4758\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [26624/50000]\tLoss: 2.5815\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [26752/50000]\tLoss: 2.2558\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [26880/50000]\tLoss: 2.5915\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [27008/50000]\tLoss: 2.6544\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [27136/50000]\tLoss: 2.2598\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [27264/50000]\tLoss: 2.4026\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [27392/50000]\tLoss: 2.5281\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [27520/50000]\tLoss: 2.2795\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [27648/50000]\tLoss: 2.5566\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [27776/50000]\tLoss: 2.6550\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [27904/50000]\tLoss: 2.5729\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [28032/50000]\tLoss: 2.4701\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [28160/50000]\tLoss: 2.5852\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [28288/50000]\tLoss: 2.3496\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [28416/50000]\tLoss: 2.5829\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [28544/50000]\tLoss: 2.5297\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [28672/50000]\tLoss: 2.4440\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [28800/50000]\tLoss: 2.4827\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [28928/50000]\tLoss: 2.4142\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [29056/50000]\tLoss: 2.4678\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [29184/50000]\tLoss: 2.3073\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [29312/50000]\tLoss: 2.6054\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [29440/50000]\tLoss: 2.3140\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [29568/50000]\tLoss: 2.5408\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [29696/50000]\tLoss: 2.4893\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [29824/50000]\tLoss: 2.5208\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [29952/50000]\tLoss: 2.5243\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [30080/50000]\tLoss: 2.5936\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [30208/50000]\tLoss: 2.6031\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [30336/50000]\tLoss: 2.5868\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [30464/50000]\tLoss: 2.5606\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [30592/50000]\tLoss: 2.4033\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [30720/50000]\tLoss: 2.3625\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [30848/50000]\tLoss: 2.2981\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [30976/50000]\tLoss: 2.5927\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [31104/50000]\tLoss: 2.6484\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [31232/50000]\tLoss: 2.3800\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [31360/50000]\tLoss: 2.4265\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [31488/50000]\tLoss: 2.4181\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [31616/50000]\tLoss: 2.3477\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [31744/50000]\tLoss: 2.5662\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [31872/50000]\tLoss: 2.3712\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [32000/50000]\tLoss: 2.3596\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [32128/50000]\tLoss: 2.2914\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [32256/50000]\tLoss: 2.4396\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [32384/50000]\tLoss: 2.2322\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [32512/50000]\tLoss: 2.3889\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [32640/50000]\tLoss: 2.4417\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [32768/50000]\tLoss: 2.4149\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [32896/50000]\tLoss: 2.5004\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [33024/50000]\tLoss: 2.2241\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [33152/50000]\tLoss: 2.4107\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [33280/50000]\tLoss: 2.5081\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [33408/50000]\tLoss: 2.5724\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [33536/50000]\tLoss: 2.2964\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [33664/50000]\tLoss: 2.5942\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [33792/50000]\tLoss: 2.4951\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [33920/50000]\tLoss: 2.4188\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [34048/50000]\tLoss: 2.4688\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [34176/50000]\tLoss: 2.5701\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [34304/50000]\tLoss: 2.4930\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [34432/50000]\tLoss: 2.5631\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [34560/50000]\tLoss: 2.5015\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [34688/50000]\tLoss: 3.0326\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [34816/50000]\tLoss: 2.5993\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [34944/50000]\tLoss: 2.5265\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [35072/50000]\tLoss: 2.5469\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [35200/50000]\tLoss: 2.7930\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [35328/50000]\tLoss: 2.6727\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [35456/50000]\tLoss: 2.5654\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [35584/50000]\tLoss: 2.4830\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [35712/50000]\tLoss: 2.4781\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [35840/50000]\tLoss: 2.6939\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [35968/50000]\tLoss: 2.4863\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [36096/50000]\tLoss: 2.3358\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [36224/50000]\tLoss: 2.5937\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [36352/50000]\tLoss: 2.5308\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [36480/50000]\tLoss: 2.5123\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [36608/50000]\tLoss: 2.5213\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [36736/50000]\tLoss: 2.6681\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [36864/50000]\tLoss: 2.4898\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [36992/50000]\tLoss: 2.5926\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [37120/50000]\tLoss: 2.5419\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [37248/50000]\tLoss: 2.4321\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [37376/50000]\tLoss: 2.4808\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [37504/50000]\tLoss: 2.2464\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [37632/50000]\tLoss: 2.3257\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [37760/50000]\tLoss: 2.4696\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [37888/50000]\tLoss: 2.5301\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [38016/50000]\tLoss: 2.3682\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [38144/50000]\tLoss: 2.4405\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [38272/50000]\tLoss: 2.4772\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [38400/50000]\tLoss: 2.4010\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [38528/50000]\tLoss: 2.2694\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [38656/50000]\tLoss: 2.5834\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [38784/50000]\tLoss: 2.4948\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [38912/50000]\tLoss: 2.4141\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [39040/50000]\tLoss: 2.4579\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [39168/50000]\tLoss: 2.4113\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [39296/50000]\tLoss: 2.5139\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [39424/50000]\tLoss: 2.7213\tLR: 0.100000\n",
      "start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 6 [39552/50000]\tLoss: 2.6925\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [39680/50000]\tLoss: 2.1679\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [39808/50000]\tLoss: 2.5472\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [39936/50000]\tLoss: 2.5910\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [40064/50000]\tLoss: 2.3453\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [40192/50000]\tLoss: 2.3652\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [40320/50000]\tLoss: 2.7461\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [40448/50000]\tLoss: 2.3932\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [40576/50000]\tLoss: 2.5613\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [40704/50000]\tLoss: 2.6124\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [40832/50000]\tLoss: 2.5704\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [40960/50000]\tLoss: 2.3686\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [41088/50000]\tLoss: 2.4202\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [41216/50000]\tLoss: 2.4318\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [41344/50000]\tLoss: 2.4606\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [41472/50000]\tLoss: 2.6664\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [41600/50000]\tLoss: 2.2961\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [41728/50000]\tLoss: 2.6135\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [41856/50000]\tLoss: 2.3405\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [41984/50000]\tLoss: 2.2772\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [42112/50000]\tLoss: 2.8327\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [42240/50000]\tLoss: 2.5075\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [42368/50000]\tLoss: 2.6037\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [42496/50000]\tLoss: 2.4754\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [42624/50000]\tLoss: 2.6871\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [42752/50000]\tLoss: 2.4637\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [42880/50000]\tLoss: 2.1573\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [43008/50000]\tLoss: 2.4370\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [43136/50000]\tLoss: 2.4971\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [43264/50000]\tLoss: 2.5710\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [43392/50000]\tLoss: 2.1928\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [43520/50000]\tLoss: 2.2986\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [43648/50000]\tLoss: 2.4807\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [43776/50000]\tLoss: 2.4191\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [43904/50000]\tLoss: 2.5820\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [44032/50000]\tLoss: 2.2128\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [44160/50000]\tLoss: 2.5489\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [44288/50000]\tLoss: 2.4351\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [44416/50000]\tLoss: 2.4286\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [44544/50000]\tLoss: 2.4495\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [44672/50000]\tLoss: 2.3989\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [44800/50000]\tLoss: 2.7697\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [44928/50000]\tLoss: 2.3263\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [45056/50000]\tLoss: 2.2999\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [45184/50000]\tLoss: 2.4123\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [45312/50000]\tLoss: 2.5177\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [45440/50000]\tLoss: 2.3454\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [45568/50000]\tLoss: 2.3997\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [45696/50000]\tLoss: 2.1460\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [45824/50000]\tLoss: 2.4399\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [45952/50000]\tLoss: 2.5492\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [46080/50000]\tLoss: 2.3329\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [46208/50000]\tLoss: 2.4963\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [46336/50000]\tLoss: 2.3685\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [46464/50000]\tLoss: 2.4302\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [46592/50000]\tLoss: 2.3092\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [46720/50000]\tLoss: 2.5648\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [46848/50000]\tLoss: 2.3835\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [46976/50000]\tLoss: 2.2634\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [47104/50000]\tLoss: 2.2216\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [47232/50000]\tLoss: 2.3276\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [47360/50000]\tLoss: 2.4560\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [47488/50000]\tLoss: 2.5085\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [47616/50000]\tLoss: 2.7191\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [47744/50000]\tLoss: 2.5295\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [47872/50000]\tLoss: 2.4441\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [48000/50000]\tLoss: 2.2712\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [48128/50000]\tLoss: 2.5668\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [48256/50000]\tLoss: 2.4541\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [48384/50000]\tLoss: 2.3621\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [48512/50000]\tLoss: 2.6475\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [48640/50000]\tLoss: 2.1425\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [48768/50000]\tLoss: 2.4628\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [48896/50000]\tLoss: 2.5347\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [49024/50000]\tLoss: 2.4083\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [49152/50000]\tLoss: 2.7164\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [49280/50000]\tLoss: 2.3320\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [49408/50000]\tLoss: 2.3842\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [49536/50000]\tLoss: 2.4796\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [49664/50000]\tLoss: 2.4715\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [49792/50000]\tLoss: 2.7011\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [49920/50000]\tLoss: 2.4873\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 6 [50000/50000]\tLoss: 2.1865\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [128/50000]\tLoss: 2.5781\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [256/50000]\tLoss: 2.4310\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [384/50000]\tLoss: 2.1800\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [512/50000]\tLoss: 2.4559\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [640/50000]\tLoss: 2.2383\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [768/50000]\tLoss: 2.3173\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [896/50000]\tLoss: 2.3511\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [1024/50000]\tLoss: 2.1897\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [1152/50000]\tLoss: 2.0175\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [1280/50000]\tLoss: 2.5019\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [1408/50000]\tLoss: 2.4722\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [1536/50000]\tLoss: 2.4489\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [1664/50000]\tLoss: 2.4639\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [1792/50000]\tLoss: 2.4700\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [1920/50000]\tLoss: 2.4785\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [2048/50000]\tLoss: 2.2784\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [2176/50000]\tLoss: 1.9957\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [2304/50000]\tLoss: 2.3542\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [2432/50000]\tLoss: 2.3321\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [2560/50000]\tLoss: 2.2134\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [2688/50000]\tLoss: 2.3351\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [2816/50000]\tLoss: 2.3018\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [2944/50000]\tLoss: 2.3665\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [3072/50000]\tLoss: 2.6921\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [3200/50000]\tLoss: 2.4269\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [3328/50000]\tLoss: 2.3519\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [3456/50000]\tLoss: 2.3852\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [3584/50000]\tLoss: 2.2941\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [3712/50000]\tLoss: 2.2548\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [3840/50000]\tLoss: 2.3219\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [3968/50000]\tLoss: 2.1117\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [4096/50000]\tLoss: 2.7352\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [4224/50000]\tLoss: 2.5055\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [4352/50000]\tLoss: 2.2965\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [4480/50000]\tLoss: 2.3357\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [4608/50000]\tLoss: 2.4277\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [4736/50000]\tLoss: 2.3553\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [4864/50000]\tLoss: 2.4818\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [4992/50000]\tLoss: 2.1592\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [5120/50000]\tLoss: 2.1505\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [5248/50000]\tLoss: 2.1909\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [5376/50000]\tLoss: 2.4478\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [5504/50000]\tLoss: 2.4851\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [5632/50000]\tLoss: 2.5557\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [5760/50000]\tLoss: 2.2167\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [5888/50000]\tLoss: 2.6176\tLR: 0.100000\n",
      "start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 7 [6016/50000]\tLoss: 2.4591\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [6144/50000]\tLoss: 2.1911\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [6272/50000]\tLoss: 2.3485\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [6400/50000]\tLoss: 2.2408\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [6528/50000]\tLoss: 2.0421\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [6656/50000]\tLoss: 2.4070\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [6784/50000]\tLoss: 2.4513\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [6912/50000]\tLoss: 2.1854\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [7040/50000]\tLoss: 2.3650\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [7168/50000]\tLoss: 2.4344\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [7296/50000]\tLoss: 2.4191\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [7424/50000]\tLoss: 2.4820\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [7552/50000]\tLoss: 2.4300\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [7680/50000]\tLoss: 2.4982\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [7808/50000]\tLoss: 2.5105\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [7936/50000]\tLoss: 2.5911\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [8064/50000]\tLoss: 2.6791\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [8192/50000]\tLoss: 2.3609\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [8320/50000]\tLoss: 2.6180\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [8448/50000]\tLoss: 2.2683\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [8576/50000]\tLoss: 2.2440\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [8704/50000]\tLoss: 2.5621\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [8832/50000]\tLoss: 2.4506\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [8960/50000]\tLoss: 2.4303\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [9088/50000]\tLoss: 2.3769\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [9216/50000]\tLoss: 2.5141\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [9344/50000]\tLoss: 2.3911\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [9472/50000]\tLoss: 2.4216\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [9600/50000]\tLoss: 2.4439\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [9728/50000]\tLoss: 2.2934\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [9856/50000]\tLoss: 2.8402\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [9984/50000]\tLoss: 2.1981\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [10112/50000]\tLoss: 2.5704\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [10240/50000]\tLoss: 2.4965\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [10368/50000]\tLoss: 2.2632\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [10496/50000]\tLoss: 2.3479\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [10624/50000]\tLoss: 2.4528\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [10752/50000]\tLoss: 2.6772\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [10880/50000]\tLoss: 2.4404\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [11008/50000]\tLoss: 2.5067\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [11136/50000]\tLoss: 2.5614\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [11264/50000]\tLoss: 2.2347\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [11392/50000]\tLoss: 2.5610\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [11520/50000]\tLoss: 2.4331\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [11648/50000]\tLoss: 2.3015\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [11776/50000]\tLoss: 2.5102\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [11904/50000]\tLoss: 2.3564\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [12032/50000]\tLoss: 2.3103\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [12160/50000]\tLoss: 2.2986\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [12288/50000]\tLoss: 2.6323\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [12416/50000]\tLoss: 2.7247\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [12544/50000]\tLoss: 2.7065\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [12672/50000]\tLoss: 2.3412\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [12800/50000]\tLoss: 2.6095\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [12928/50000]\tLoss: 2.1966\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [13056/50000]\tLoss: 2.2952\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [13184/50000]\tLoss: 2.4213\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [13312/50000]\tLoss: 2.5516\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [13440/50000]\tLoss: 2.2442\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [13568/50000]\tLoss: 2.5929\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [13696/50000]\tLoss: 2.4815\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [13824/50000]\tLoss: 2.6358\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [13952/50000]\tLoss: 2.2775\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [14080/50000]\tLoss: 2.5030\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [14208/50000]\tLoss: 2.2656\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [14336/50000]\tLoss: 2.3808\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [14464/50000]\tLoss: 2.3081\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [14592/50000]\tLoss: 2.2415\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [14720/50000]\tLoss: 2.4147\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [14848/50000]\tLoss: 2.3698\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [14976/50000]\tLoss: 2.8533\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [15104/50000]\tLoss: 2.1195\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [15232/50000]\tLoss: 2.1494\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [15360/50000]\tLoss: 2.5078\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [15488/50000]\tLoss: 2.5450\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [15616/50000]\tLoss: 2.6199\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [15744/50000]\tLoss: 2.4781\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [15872/50000]\tLoss: 2.2790\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [16000/50000]\tLoss: 2.7641\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [16128/50000]\tLoss: 2.5580\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [16256/50000]\tLoss: 2.2799\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [16384/50000]\tLoss: 2.5048\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [16512/50000]\tLoss: 2.3996\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [16640/50000]\tLoss: 2.2416\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [16768/50000]\tLoss: 2.5114\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [16896/50000]\tLoss: 2.0991\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [17024/50000]\tLoss: 2.5230\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [17152/50000]\tLoss: 2.3482\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [17280/50000]\tLoss: 2.4091\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [17408/50000]\tLoss: 2.2407\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [17536/50000]\tLoss: 2.4507\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [17664/50000]\tLoss: 2.4790\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [17792/50000]\tLoss: 2.1051\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [17920/50000]\tLoss: 2.4093\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [18048/50000]\tLoss: 2.3434\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [18176/50000]\tLoss: 2.3781\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [18304/50000]\tLoss: 2.1782\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [18432/50000]\tLoss: 2.7881\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [18560/50000]\tLoss: 2.3372\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [18688/50000]\tLoss: 2.2341\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [18816/50000]\tLoss: 2.3839\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [18944/50000]\tLoss: 2.3887\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [19072/50000]\tLoss: 2.3581\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [19200/50000]\tLoss: 2.5148\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [19328/50000]\tLoss: 2.4477\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [19456/50000]\tLoss: 2.5732\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [19584/50000]\tLoss: 2.5572\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [19712/50000]\tLoss: 2.3371\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [19840/50000]\tLoss: 2.7861\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [19968/50000]\tLoss: 2.7351\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [20096/50000]\tLoss: 2.3062\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [20224/50000]\tLoss: 2.2599\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [20352/50000]\tLoss: 2.6626\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [20480/50000]\tLoss: 2.4922\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [20608/50000]\tLoss: 2.4550\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [20736/50000]\tLoss: 2.4286\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [20864/50000]\tLoss: 2.3879\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [20992/50000]\tLoss: 2.4043\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [21120/50000]\tLoss: 2.5114\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [21248/50000]\tLoss: 2.2501\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [21376/50000]\tLoss: 2.1040\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [21504/50000]\tLoss: 2.5638\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [21632/50000]\tLoss: 2.3993\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [21760/50000]\tLoss: 2.1899\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [21888/50000]\tLoss: 2.2593\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [22016/50000]\tLoss: 2.5661\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [22144/50000]\tLoss: 2.1486\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [22272/50000]\tLoss: 2.1452\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [22400/50000]\tLoss: 2.9395\tLR: 0.100000\n",
      "start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 7 [22528/50000]\tLoss: 2.6117\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [22656/50000]\tLoss: 2.4131\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [22784/50000]\tLoss: 2.3859\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [22912/50000]\tLoss: 2.3647\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [23040/50000]\tLoss: 2.3788\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [23168/50000]\tLoss: 2.3277\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [23296/50000]\tLoss: 2.2172\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [23424/50000]\tLoss: 2.2164\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [23552/50000]\tLoss: 2.5636\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [23680/50000]\tLoss: 2.4414\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [23808/50000]\tLoss: 2.5544\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [23936/50000]\tLoss: 2.4256\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [24064/50000]\tLoss: 2.5907\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [24192/50000]\tLoss: 2.3479\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [24320/50000]\tLoss: 2.3794\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [24448/50000]\tLoss: 2.4888\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [24576/50000]\tLoss: 2.4839\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [24704/50000]\tLoss: 2.6751\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [24832/50000]\tLoss: 2.4021\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [24960/50000]\tLoss: 2.1939\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [25088/50000]\tLoss: 2.3886\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [25216/50000]\tLoss: 2.6011\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [25344/50000]\tLoss: 2.5035\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [25472/50000]\tLoss: 2.1454\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [25600/50000]\tLoss: 2.3255\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [25728/50000]\tLoss: 2.3595\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [25856/50000]\tLoss: 2.2313\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [25984/50000]\tLoss: 2.0958\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [26112/50000]\tLoss: 2.0810\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [26240/50000]\tLoss: 2.4314\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [26368/50000]\tLoss: 2.5123\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [26496/50000]\tLoss: 2.3272\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [26624/50000]\tLoss: 2.4644\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [26752/50000]\tLoss: 2.2671\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [26880/50000]\tLoss: 2.2797\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [27008/50000]\tLoss: 2.1162\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [27136/50000]\tLoss: 2.3319\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [27264/50000]\tLoss: 2.3914\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [27392/50000]\tLoss: 2.2281\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [27520/50000]\tLoss: 2.4707\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [27648/50000]\tLoss: 2.3015\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [27776/50000]\tLoss: 2.3243\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [27904/50000]\tLoss: 2.5319\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [28032/50000]\tLoss: 2.2015\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [28160/50000]\tLoss: 2.4970\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [28288/50000]\tLoss: 2.4469\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [28416/50000]\tLoss: 2.2302\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [28544/50000]\tLoss: 2.4523\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [28672/50000]\tLoss: 2.2254\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [28800/50000]\tLoss: 2.5537\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [28928/50000]\tLoss: 2.6678\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [29056/50000]\tLoss: 2.6783\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [29184/50000]\tLoss: 2.1901\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [29312/50000]\tLoss: 2.4231\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [29440/50000]\tLoss: 2.2896\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [29568/50000]\tLoss: 2.2273\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [29696/50000]\tLoss: 2.3330\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [29824/50000]\tLoss: 2.0740\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [29952/50000]\tLoss: 2.3210\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [30080/50000]\tLoss: 2.3654\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [30208/50000]\tLoss: 2.3810\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [30336/50000]\tLoss: 2.3836\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [30464/50000]\tLoss: 2.3273\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [30592/50000]\tLoss: 2.4267\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [30720/50000]\tLoss: 2.5403\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [30848/50000]\tLoss: 2.5101\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [30976/50000]\tLoss: 2.3039\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [31104/50000]\tLoss: 2.5843\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [31232/50000]\tLoss: 2.5725\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [31360/50000]\tLoss: 2.3826\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [31488/50000]\tLoss: 2.4908\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [31616/50000]\tLoss: 2.3450\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [31744/50000]\tLoss: 2.3517\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [31872/50000]\tLoss: 2.1917\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [32000/50000]\tLoss: 2.4369\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [32128/50000]\tLoss: 2.4168\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [32256/50000]\tLoss: 2.2756\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [32384/50000]\tLoss: 2.0869\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [32512/50000]\tLoss: 2.1896\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [32640/50000]\tLoss: 2.0978\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [32768/50000]\tLoss: 2.4602\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [32896/50000]\tLoss: 2.2638\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [33024/50000]\tLoss: 2.0671\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [33152/50000]\tLoss: 2.0768\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [33280/50000]\tLoss: 2.5418\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [33408/50000]\tLoss: 2.1782\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [33536/50000]\tLoss: 2.1628\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [33664/50000]\tLoss: 2.1831\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [33792/50000]\tLoss: 2.1637\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [33920/50000]\tLoss: 2.2686\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [34048/50000]\tLoss: 2.1827\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [34176/50000]\tLoss: 2.0976\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [34304/50000]\tLoss: 2.3837\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [34432/50000]\tLoss: 2.2104\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [34560/50000]\tLoss: 2.3048\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [34688/50000]\tLoss: 2.5462\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [34816/50000]\tLoss: 2.2091\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [34944/50000]\tLoss: 2.2306\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [35072/50000]\tLoss: 2.3098\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [35200/50000]\tLoss: 2.4938\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [35328/50000]\tLoss: 2.4881\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [35456/50000]\tLoss: 2.4192\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [35584/50000]\tLoss: 2.4592\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [35712/50000]\tLoss: 2.4771\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [35840/50000]\tLoss: 2.3296\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [35968/50000]\tLoss: 2.2816\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [36096/50000]\tLoss: 2.0832\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [36224/50000]\tLoss: 2.2661\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [36352/50000]\tLoss: 2.1024\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [36480/50000]\tLoss: 2.5063\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [36608/50000]\tLoss: 2.2872\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [36736/50000]\tLoss: 2.4160\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [36864/50000]\tLoss: 2.4930\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [36992/50000]\tLoss: 2.3292\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [37120/50000]\tLoss: 2.0746\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [37248/50000]\tLoss: 2.3602\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [37376/50000]\tLoss: 2.3729\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [37504/50000]\tLoss: 2.5197\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [37632/50000]\tLoss: 2.2945\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [37760/50000]\tLoss: 2.2013\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [37888/50000]\tLoss: 2.1728\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [38016/50000]\tLoss: 2.4398\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [38144/50000]\tLoss: 2.2328\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [38272/50000]\tLoss: 2.4556\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [38400/50000]\tLoss: 2.1146\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [38528/50000]\tLoss: 2.1999\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [38656/50000]\tLoss: 2.2513\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [38784/50000]\tLoss: 2.4184\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [38912/50000]\tLoss: 2.3956\tLR: 0.100000\n",
      "start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 7 [39040/50000]\tLoss: 2.3620\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [39168/50000]\tLoss: 2.3826\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [39296/50000]\tLoss: 2.2410\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [39424/50000]\tLoss: 2.2996\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [39552/50000]\tLoss: 2.3089\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [39680/50000]\tLoss: 2.5045\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [39808/50000]\tLoss: 2.0483\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [39936/50000]\tLoss: 2.2004\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [40064/50000]\tLoss: 2.2736\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [40192/50000]\tLoss: 2.1145\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [40320/50000]\tLoss: 2.3064\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [40448/50000]\tLoss: 2.3306\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [40576/50000]\tLoss: 2.1758\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [40704/50000]\tLoss: 2.3730\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [40832/50000]\tLoss: 2.2944\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [40960/50000]\tLoss: 2.1782\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [41088/50000]\tLoss: 2.4205\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [41216/50000]\tLoss: 2.0211\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [41344/50000]\tLoss: 2.3128\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [41472/50000]\tLoss: 2.2775\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [41600/50000]\tLoss: 2.2694\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [41728/50000]\tLoss: 2.1484\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [41856/50000]\tLoss: 2.0912\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [41984/50000]\tLoss: 2.4821\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [42112/50000]\tLoss: 2.4350\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [42240/50000]\tLoss: 2.6201\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [42368/50000]\tLoss: 2.5126\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [42496/50000]\tLoss: 2.1864\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [42624/50000]\tLoss: 2.2588\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [42752/50000]\tLoss: 2.2800\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [42880/50000]\tLoss: 2.3643\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [43008/50000]\tLoss: 2.3094\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [43136/50000]\tLoss: 2.3307\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [43264/50000]\tLoss: 2.3000\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [43392/50000]\tLoss: 2.3858\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [43520/50000]\tLoss: 1.9982\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [43648/50000]\tLoss: 2.5791\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [43776/50000]\tLoss: 2.3822\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [43904/50000]\tLoss: 2.2780\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [44032/50000]\tLoss: 2.4885\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [44160/50000]\tLoss: 2.2974\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [44288/50000]\tLoss: 2.1906\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [44416/50000]\tLoss: 2.4505\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [44544/50000]\tLoss: 2.1264\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [44672/50000]\tLoss: 2.3784\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [44800/50000]\tLoss: 2.4216\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [44928/50000]\tLoss: 2.5846\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [45056/50000]\tLoss: 2.3744\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [45184/50000]\tLoss: 2.7356\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [45312/50000]\tLoss: 2.3557\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [45440/50000]\tLoss: 2.5916\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [45568/50000]\tLoss: 2.3131\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [45696/50000]\tLoss: 2.5879\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [45824/50000]\tLoss: 2.2826\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [45952/50000]\tLoss: 2.4998\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [46080/50000]\tLoss: 2.3603\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [46208/50000]\tLoss: 2.2912\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [46336/50000]\tLoss: 2.3621\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [46464/50000]\tLoss: 2.7372\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [46592/50000]\tLoss: 2.3542\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [46720/50000]\tLoss: 2.4989\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [46848/50000]\tLoss: 2.1725\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [46976/50000]\tLoss: 2.5932\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [47104/50000]\tLoss: 2.4230\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [47232/50000]\tLoss: 2.6116\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [47360/50000]\tLoss: 2.2400\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [47488/50000]\tLoss: 2.4913\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [47616/50000]\tLoss: 2.3363\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [47744/50000]\tLoss: 2.4217\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [47872/50000]\tLoss: 2.3368\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [48000/50000]\tLoss: 2.5011\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [48128/50000]\tLoss: 2.3935\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [48256/50000]\tLoss: 2.2240\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [48384/50000]\tLoss: 2.2284\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [48512/50000]\tLoss: 2.4529\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [48640/50000]\tLoss: 2.2933\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [48768/50000]\tLoss: 2.2621\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [48896/50000]\tLoss: 2.2840\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [49024/50000]\tLoss: 2.5617\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [49152/50000]\tLoss: 2.4263\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [49280/50000]\tLoss: 2.4433\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [49408/50000]\tLoss: 2.2956\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [49536/50000]\tLoss: 2.4066\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [49664/50000]\tLoss: 2.2845\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [49792/50000]\tLoss: 2.4922\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [49920/50000]\tLoss: 2.8525\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 7 [50000/50000]\tLoss: 2.2791\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [128/50000]\tLoss: 2.2038\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [256/50000]\tLoss: 2.3497\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [384/50000]\tLoss: 2.4084\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [512/50000]\tLoss: 2.3197\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [640/50000]\tLoss: 2.1031\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [768/50000]\tLoss: 2.2521\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [896/50000]\tLoss: 2.2983\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [1024/50000]\tLoss: 2.3081\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [1152/50000]\tLoss: 2.1161\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [1280/50000]\tLoss: 2.4994\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [1408/50000]\tLoss: 2.2565\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [1536/50000]\tLoss: 2.1774\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [1664/50000]\tLoss: 2.2407\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [1792/50000]\tLoss: 2.3561\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [1920/50000]\tLoss: 2.4393\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [2048/50000]\tLoss: 2.2328\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [2176/50000]\tLoss: 2.3826\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [2304/50000]\tLoss: 2.5062\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [2432/50000]\tLoss: 2.3117\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [2560/50000]\tLoss: 2.3061\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [2688/50000]\tLoss: 2.5668\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [2816/50000]\tLoss: 2.3216\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [2944/50000]\tLoss: 2.4259\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [3072/50000]\tLoss: 2.4949\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [3200/50000]\tLoss: 2.3246\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [3328/50000]\tLoss: 2.4193\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [3456/50000]\tLoss: 2.2424\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [3584/50000]\tLoss: 2.3835\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [3712/50000]\tLoss: 2.3036\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [3840/50000]\tLoss: 2.5181\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [3968/50000]\tLoss: 2.2826\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [4096/50000]\tLoss: 2.4174\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [4224/50000]\tLoss: 2.0895\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [4352/50000]\tLoss: 2.3572\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [4480/50000]\tLoss: 2.1388\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [4608/50000]\tLoss: 2.3244\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [4736/50000]\tLoss: 2.3438\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [4864/50000]\tLoss: 2.4935\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [4992/50000]\tLoss: 2.6014\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [5120/50000]\tLoss: 2.2171\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [5248/50000]\tLoss: 2.5259\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [5376/50000]\tLoss: 2.2652\tLR: 0.100000\n",
      "start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 8 [5504/50000]\tLoss: 2.2539\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [5632/50000]\tLoss: 2.3694\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [5760/50000]\tLoss: 2.2869\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [5888/50000]\tLoss: 2.4184\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [6016/50000]\tLoss: 2.4012\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [6144/50000]\tLoss: 2.2855\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [6272/50000]\tLoss: 2.4169\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [6400/50000]\tLoss: 2.1621\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [6528/50000]\tLoss: 2.3151\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [6656/50000]\tLoss: 2.1822\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [6784/50000]\tLoss: 1.9710\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [6912/50000]\tLoss: 2.5301\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [7040/50000]\tLoss: 2.2042\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [7168/50000]\tLoss: 2.1805\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [7296/50000]\tLoss: 2.3132\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [7424/50000]\tLoss: 2.2051\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [7552/50000]\tLoss: 2.0879\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [7680/50000]\tLoss: 2.5106\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [7808/50000]\tLoss: 2.2137\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [7936/50000]\tLoss: 2.2822\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [8064/50000]\tLoss: 1.9643\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [8192/50000]\tLoss: 2.1429\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [8320/50000]\tLoss: 2.0718\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [8448/50000]\tLoss: 2.0768\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [8576/50000]\tLoss: 2.4605\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [8704/50000]\tLoss: 2.5136\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [8832/50000]\tLoss: 2.1537\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [8960/50000]\tLoss: 2.1439\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [9088/50000]\tLoss: 2.4513\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [9216/50000]\tLoss: 2.1450\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [9344/50000]\tLoss: 2.5176\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [9472/50000]\tLoss: 2.3612\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [9600/50000]\tLoss: 2.2188\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [9728/50000]\tLoss: 2.2186\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [9856/50000]\tLoss: 2.1591\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [9984/50000]\tLoss: 2.2452\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [10112/50000]\tLoss: 2.1110\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [10240/50000]\tLoss: 2.0955\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [10368/50000]\tLoss: 2.0649\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [10496/50000]\tLoss: 2.2234\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [10624/50000]\tLoss: 2.3517\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [10752/50000]\tLoss: 2.6471\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [10880/50000]\tLoss: 2.2501\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [11008/50000]\tLoss: 2.1496\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [11136/50000]\tLoss: 2.5561\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [11264/50000]\tLoss: 2.2758\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [11392/50000]\tLoss: 2.2550\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [11520/50000]\tLoss: 1.8867\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [11648/50000]\tLoss: 2.4506\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [11776/50000]\tLoss: 2.1245\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [11904/50000]\tLoss: 2.3048\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [12032/50000]\tLoss: 2.4211\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [12160/50000]\tLoss: 2.4581\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [12288/50000]\tLoss: 2.2538\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [12416/50000]\tLoss: 2.2835\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [12544/50000]\tLoss: 2.4654\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [12672/50000]\tLoss: 2.4314\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [12800/50000]\tLoss: 2.2533\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [12928/50000]\tLoss: 2.0477\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [13056/50000]\tLoss: 2.1773\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [13184/50000]\tLoss: 2.2595\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [13312/50000]\tLoss: 2.3412\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [13440/50000]\tLoss: 2.3838\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [13568/50000]\tLoss: 2.0281\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [13696/50000]\tLoss: 2.1253\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [13824/50000]\tLoss: 2.4450\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [13952/50000]\tLoss: 2.4456\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [14080/50000]\tLoss: 2.1245\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [14208/50000]\tLoss: 2.2972\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [14336/50000]\tLoss: 2.2506\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [14464/50000]\tLoss: 2.3105\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [14592/50000]\tLoss: 2.3471\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [14720/50000]\tLoss: 2.2531\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [14848/50000]\tLoss: 2.5156\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [14976/50000]\tLoss: 2.1725\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [15104/50000]\tLoss: 2.6057\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [15232/50000]\tLoss: 2.5091\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [15360/50000]\tLoss: 2.2653\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [15488/50000]\tLoss: 2.3899\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [15616/50000]\tLoss: 2.3975\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [15744/50000]\tLoss: 2.2547\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [15872/50000]\tLoss: 2.3753\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [16000/50000]\tLoss: 2.5498\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [16128/50000]\tLoss: 2.4093\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [16256/50000]\tLoss: 2.4644\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [16384/50000]\tLoss: 2.4217\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [16512/50000]\tLoss: 2.2916\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [16640/50000]\tLoss: 2.2802\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [16768/50000]\tLoss: 2.4599\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [16896/50000]\tLoss: 2.1012\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [17024/50000]\tLoss: 2.1831\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [17152/50000]\tLoss: 2.1034\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [17280/50000]\tLoss: 2.4642\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [17408/50000]\tLoss: 2.4460\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [17536/50000]\tLoss: 2.2223\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [17664/50000]\tLoss: 2.4666\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [17792/50000]\tLoss: 2.0995\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [17920/50000]\tLoss: 2.3296\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [18048/50000]\tLoss: 2.5325\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [18176/50000]\tLoss: 1.9682\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [18304/50000]\tLoss: 2.3013\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [18432/50000]\tLoss: 2.3118\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [18560/50000]\tLoss: 2.2299\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [18688/50000]\tLoss: 2.3118\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [18816/50000]\tLoss: 2.3583\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [18944/50000]\tLoss: 2.2358\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [19072/50000]\tLoss: 2.3732\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [19200/50000]\tLoss: 2.4372\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [19328/50000]\tLoss: 2.1721\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [19456/50000]\tLoss: 2.3295\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [19584/50000]\tLoss: 2.2854\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [19712/50000]\tLoss: 2.3520\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [19840/50000]\tLoss: 2.1761\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [19968/50000]\tLoss: 2.1932\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [20096/50000]\tLoss: 2.4095\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [20224/50000]\tLoss: 2.1952\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [20352/50000]\tLoss: 2.3582\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [20480/50000]\tLoss: 2.0855\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [20608/50000]\tLoss: 2.3645\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [20736/50000]\tLoss: 2.4399\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [20864/50000]\tLoss: 2.3366\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [20992/50000]\tLoss: 2.3714\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [21120/50000]\tLoss: 2.1219\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [21248/50000]\tLoss: 2.2561\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [21376/50000]\tLoss: 2.4694\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [21504/50000]\tLoss: 2.3717\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [21632/50000]\tLoss: 2.2236\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [21760/50000]\tLoss: 2.2833\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [21888/50000]\tLoss: 2.3593\tLR: 0.100000\n",
      "start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 8 [22016/50000]\tLoss: 2.2609\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [22144/50000]\tLoss: 2.5295\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [22272/50000]\tLoss: 2.4042\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [22400/50000]\tLoss: 2.2007\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [22528/50000]\tLoss: 2.2847\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [22656/50000]\tLoss: 2.4098\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [22784/50000]\tLoss: 2.1907\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [22912/50000]\tLoss: 2.3050\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [23040/50000]\tLoss: 2.3716\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [23168/50000]\tLoss: 2.4245\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [23296/50000]\tLoss: 2.5238\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [23424/50000]\tLoss: 2.3967\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [23552/50000]\tLoss: 2.3219\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [23680/50000]\tLoss: 2.1635\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [23808/50000]\tLoss: 2.4142\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [23936/50000]\tLoss: 2.2054\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [24064/50000]\tLoss: 2.0609\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [24192/50000]\tLoss: 2.3041\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [24320/50000]\tLoss: 2.1488\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [24448/50000]\tLoss: 2.4585\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [24576/50000]\tLoss: 2.3822\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [24704/50000]\tLoss: 2.1580\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [24832/50000]\tLoss: 2.3788\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [24960/50000]\tLoss: 2.0650\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [25088/50000]\tLoss: 2.6671\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [25216/50000]\tLoss: 2.6106\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [25344/50000]\tLoss: 2.4425\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [25472/50000]\tLoss: 2.1631\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [25600/50000]\tLoss: 2.1428\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [25728/50000]\tLoss: 2.2003\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [25856/50000]\tLoss: 2.1279\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [25984/50000]\tLoss: 2.3597\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [26112/50000]\tLoss: 2.2112\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [26240/50000]\tLoss: 2.5308\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [26368/50000]\tLoss: 2.2768\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [26496/50000]\tLoss: 2.1828\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [26624/50000]\tLoss: 2.2084\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [26752/50000]\tLoss: 2.5404\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [26880/50000]\tLoss: 2.4009\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [27008/50000]\tLoss: 2.2600\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [27136/50000]\tLoss: 2.0504\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [27264/50000]\tLoss: 2.4017\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [27392/50000]\tLoss: 2.4174\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [27520/50000]\tLoss: 2.4334\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [27648/50000]\tLoss: 2.1997\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [27776/50000]\tLoss: 2.3307\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [27904/50000]\tLoss: 2.2663\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [28032/50000]\tLoss: 2.3213\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [28160/50000]\tLoss: 2.4437\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [28288/50000]\tLoss: 2.2902\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [28416/50000]\tLoss: 2.3159\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [28544/50000]\tLoss: 2.0811\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [28672/50000]\tLoss: 2.2836\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [28800/50000]\tLoss: 2.1264\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [28928/50000]\tLoss: 2.7048\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [29056/50000]\tLoss: 2.2555\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [29184/50000]\tLoss: 2.1547\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [29312/50000]\tLoss: 2.5819\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [29440/50000]\tLoss: 2.2564\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [29568/50000]\tLoss: 2.2909\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [29696/50000]\tLoss: 2.2700\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [29824/50000]\tLoss: 2.2352\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [29952/50000]\tLoss: 2.0108\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [30080/50000]\tLoss: 2.8186\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [30208/50000]\tLoss: 2.6574\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [30336/50000]\tLoss: 2.1862\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [30464/50000]\tLoss: 2.4538\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [30592/50000]\tLoss: 2.4529\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [30720/50000]\tLoss: 2.3399\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [30848/50000]\tLoss: 2.4368\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [30976/50000]\tLoss: 2.4433\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [31104/50000]\tLoss: 2.2010\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [31232/50000]\tLoss: 2.3626\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [31360/50000]\tLoss: 2.4304\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [31488/50000]\tLoss: 2.3619\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [31616/50000]\tLoss: 2.2008\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [31744/50000]\tLoss: 2.2052\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [31872/50000]\tLoss: 2.2980\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [32000/50000]\tLoss: 2.3162\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [32128/50000]\tLoss: 1.9701\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [32256/50000]\tLoss: 2.2955\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [32384/50000]\tLoss: 2.3419\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [32512/50000]\tLoss: 2.3635\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [32640/50000]\tLoss: 2.4691\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [32768/50000]\tLoss: 2.1221\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [32896/50000]\tLoss: 2.1809\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [33024/50000]\tLoss: 2.2256\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [33152/50000]\tLoss: 1.9697\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [33280/50000]\tLoss: 2.2952\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [33408/50000]\tLoss: 2.1898\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [33536/50000]\tLoss: 2.0812\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [33664/50000]\tLoss: 2.3979\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [33792/50000]\tLoss: 2.2430\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [33920/50000]\tLoss: 2.1510\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [34048/50000]\tLoss: 2.0842\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [34176/50000]\tLoss: 2.2277\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [34304/50000]\tLoss: 2.2561\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [34432/50000]\tLoss: 2.2033\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [34560/50000]\tLoss: 2.2243\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [34688/50000]\tLoss: 2.4550\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [34816/50000]\tLoss: 2.0806\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [34944/50000]\tLoss: 2.2037\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [35072/50000]\tLoss: 2.5611\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [35200/50000]\tLoss: 2.5278\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [35328/50000]\tLoss: 2.0152\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [35456/50000]\tLoss: 2.4940\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [35584/50000]\tLoss: 2.3283\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [35712/50000]\tLoss: 2.5417\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [35840/50000]\tLoss: 2.2947\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [35968/50000]\tLoss: 2.3200\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [36096/50000]\tLoss: 2.4396\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [36224/50000]\tLoss: 2.1485\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [36352/50000]\tLoss: 2.4102\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [36480/50000]\tLoss: 2.2626\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [36608/50000]\tLoss: 2.2627\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [36736/50000]\tLoss: 2.5115\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [36864/50000]\tLoss: 2.2488\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [36992/50000]\tLoss: 2.2190\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [37120/50000]\tLoss: 2.1723\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [37248/50000]\tLoss: 2.0424\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [37376/50000]\tLoss: 2.0087\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [37504/50000]\tLoss: 2.4252\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [37632/50000]\tLoss: 2.3042\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [37760/50000]\tLoss: 2.1596\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [37888/50000]\tLoss: 2.5505\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [38016/50000]\tLoss: 2.2345\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [38144/50000]\tLoss: 2.4960\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [38272/50000]\tLoss: 2.2649\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [38400/50000]\tLoss: 2.4755\tLR: 0.100000\n",
      "start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 8 [38528/50000]\tLoss: 2.4618\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [38656/50000]\tLoss: 2.3359\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [38784/50000]\tLoss: 2.2896\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [38912/50000]\tLoss: 2.3914\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [39040/50000]\tLoss: 2.5000\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [39168/50000]\tLoss: 2.0966\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [39296/50000]\tLoss: 2.2395\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [39424/50000]\tLoss: 2.2482\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [39552/50000]\tLoss: 2.3883\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [39680/50000]\tLoss: 2.4456\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [39808/50000]\tLoss: 2.2681\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [39936/50000]\tLoss: 2.0676\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [40064/50000]\tLoss: 2.0624\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [40192/50000]\tLoss: 2.5896\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [40320/50000]\tLoss: 2.2683\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [40448/50000]\tLoss: 2.3828\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [40576/50000]\tLoss: 2.5397\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [40704/50000]\tLoss: 2.2629\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [40832/50000]\tLoss: 2.3766\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [40960/50000]\tLoss: 2.3426\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [41088/50000]\tLoss: 2.2470\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [41216/50000]\tLoss: 2.0391\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [41344/50000]\tLoss: 2.0835\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [41472/50000]\tLoss: 2.0979\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [41600/50000]\tLoss: 2.2440\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [41728/50000]\tLoss: 2.4584\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [41856/50000]\tLoss: 2.2638\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [41984/50000]\tLoss: 2.2026\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [42112/50000]\tLoss: 2.2076\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [42240/50000]\tLoss: 2.0986\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [42368/50000]\tLoss: 2.0352\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [42496/50000]\tLoss: 2.1727\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [42624/50000]\tLoss: 2.2628\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [42752/50000]\tLoss: 2.1690\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [42880/50000]\tLoss: 2.2157\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [43008/50000]\tLoss: 2.3019\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [43136/50000]\tLoss: 2.2790\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [43264/50000]\tLoss: 2.2776\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [43392/50000]\tLoss: 2.1946\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [43520/50000]\tLoss: 2.2764\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [43648/50000]\tLoss: 2.2832\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [43776/50000]\tLoss: 2.2031\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [43904/50000]\tLoss: 2.0724\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [44032/50000]\tLoss: 1.9817\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [44160/50000]\tLoss: 2.2462\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [44288/50000]\tLoss: 2.4682\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [44416/50000]\tLoss: 2.2061\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [44544/50000]\tLoss: 2.0419\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [44672/50000]\tLoss: 2.0861\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [44800/50000]\tLoss: 2.1634\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [44928/50000]\tLoss: 2.3417\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [45056/50000]\tLoss: 2.4641\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [45184/50000]\tLoss: 2.2446\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [45312/50000]\tLoss: 2.2533\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [45440/50000]\tLoss: 2.5990\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [45568/50000]\tLoss: 2.3051\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [45696/50000]\tLoss: 2.2898\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [45824/50000]\tLoss: 2.4371\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [45952/50000]\tLoss: 2.1956\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [46080/50000]\tLoss: 2.5853\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [46208/50000]\tLoss: 2.2698\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [46336/50000]\tLoss: 2.2768\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [46464/50000]\tLoss: 2.0387\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [46592/50000]\tLoss: 2.2275\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [46720/50000]\tLoss: 2.0687\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [46848/50000]\tLoss: 2.0986\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [46976/50000]\tLoss: 2.4837\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [47104/50000]\tLoss: 2.5390\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [47232/50000]\tLoss: 2.6120\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [47360/50000]\tLoss: 2.0980\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [47488/50000]\tLoss: 2.2282\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [47616/50000]\tLoss: 2.4804\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [47744/50000]\tLoss: 2.2243\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [47872/50000]\tLoss: 2.1831\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [48000/50000]\tLoss: 2.1843\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [48128/50000]\tLoss: 2.1868\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [48256/50000]\tLoss: 2.5124\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [48384/50000]\tLoss: 2.3496\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [48512/50000]\tLoss: 2.2905\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [48640/50000]\tLoss: 2.0862\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [48768/50000]\tLoss: 2.3974\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [48896/50000]\tLoss: 2.0936\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [49024/50000]\tLoss: 2.2801\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [49152/50000]\tLoss: 2.0468\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [49280/50000]\tLoss: 2.1821\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [49408/50000]\tLoss: 2.3475\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [49536/50000]\tLoss: 1.9261\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [49664/50000]\tLoss: 2.3649\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [49792/50000]\tLoss: 2.3439\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [49920/50000]\tLoss: 2.1607\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 8 [50000/50000]\tLoss: 2.5968\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [128/50000]\tLoss: 2.2100\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [256/50000]\tLoss: 2.0639\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [384/50000]\tLoss: 2.2717\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [512/50000]\tLoss: 2.2984\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [640/50000]\tLoss: 2.4261\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [768/50000]\tLoss: 2.3052\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [896/50000]\tLoss: 2.1408\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [1024/50000]\tLoss: 2.2850\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [1152/50000]\tLoss: 2.3377\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [1280/50000]\tLoss: 2.3675\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [1408/50000]\tLoss: 2.4128\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [1536/50000]\tLoss: 2.5332\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [1664/50000]\tLoss: 2.2965\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [1792/50000]\tLoss: 2.4209\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [1920/50000]\tLoss: 2.3946\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [2048/50000]\tLoss: 2.0261\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [2176/50000]\tLoss: 2.2147\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [2304/50000]\tLoss: 2.4222\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [2432/50000]\tLoss: 2.1934\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [2560/50000]\tLoss: 2.1440\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [2688/50000]\tLoss: 2.0106\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [2816/50000]\tLoss: 2.2522\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [2944/50000]\tLoss: 1.8038\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [3072/50000]\tLoss: 1.9697\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [3200/50000]\tLoss: 2.1620\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [3328/50000]\tLoss: 2.3651\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [3456/50000]\tLoss: 2.2890\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [3584/50000]\tLoss: 2.2539\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [3712/50000]\tLoss: 2.2944\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [3840/50000]\tLoss: 2.2604\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [3968/50000]\tLoss: 2.3055\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [4096/50000]\tLoss: 2.3191\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [4224/50000]\tLoss: 2.3439\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [4352/50000]\tLoss: 2.6638\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [4480/50000]\tLoss: 2.5708\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [4608/50000]\tLoss: 2.0567\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [4736/50000]\tLoss: 2.3148\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [4864/50000]\tLoss: 2.2176\tLR: 0.100000\n",
      "start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 9 [4992/50000]\tLoss: 2.3334\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [5120/50000]\tLoss: 2.2692\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [5248/50000]\tLoss: 2.2286\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [5376/50000]\tLoss: 2.2865\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [5504/50000]\tLoss: 2.0142\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [5632/50000]\tLoss: 2.3238\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [5760/50000]\tLoss: 2.3680\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [5888/50000]\tLoss: 2.2904\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [6016/50000]\tLoss: 2.3743\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [6144/50000]\tLoss: 2.3717\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [6272/50000]\tLoss: 2.2726\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [6400/50000]\tLoss: 1.9936\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [6528/50000]\tLoss: 2.1845\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [6656/50000]\tLoss: 2.3108\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [6784/50000]\tLoss: 2.1152\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [6912/50000]\tLoss: 2.3890\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [7040/50000]\tLoss: 2.3108\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [7168/50000]\tLoss: 2.1835\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [7296/50000]\tLoss: 2.4356\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [7424/50000]\tLoss: 2.3958\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [7552/50000]\tLoss: 2.3714\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [7680/50000]\tLoss: 2.1409\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [7808/50000]\tLoss: 2.0657\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [7936/50000]\tLoss: 2.2188\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [8064/50000]\tLoss: 2.2003\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [8192/50000]\tLoss: 2.2301\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [8320/50000]\tLoss: 1.8935\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [8448/50000]\tLoss: 1.9352\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [8576/50000]\tLoss: 2.2430\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [8704/50000]\tLoss: 2.4139\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [8832/50000]\tLoss: 2.3166\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [8960/50000]\tLoss: 2.2009\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [9088/50000]\tLoss: 2.2775\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [9216/50000]\tLoss: 2.3362\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [9344/50000]\tLoss: 2.2848\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [9472/50000]\tLoss: 2.2129\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [9600/50000]\tLoss: 1.9741\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [9728/50000]\tLoss: 1.9372\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [9856/50000]\tLoss: 1.9365\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [9984/50000]\tLoss: 2.2525\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [10112/50000]\tLoss: 2.2618\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [10240/50000]\tLoss: 2.1662\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [10368/50000]\tLoss: 2.4401\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [10496/50000]\tLoss: 2.0843\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [10624/50000]\tLoss: 2.0448\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [10752/50000]\tLoss: 2.1414\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [10880/50000]\tLoss: 2.3239\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [11008/50000]\tLoss: 2.3167\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [11136/50000]\tLoss: 2.2136\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [11264/50000]\tLoss: 2.1419\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [11392/50000]\tLoss: 2.1288\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [11520/50000]\tLoss: 2.3358\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [11648/50000]\tLoss: 2.2726\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [11776/50000]\tLoss: 2.3856\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [11904/50000]\tLoss: 2.1359\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [12032/50000]\tLoss: 2.3182\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [12160/50000]\tLoss: 1.9838\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [12288/50000]\tLoss: 2.2830\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [12416/50000]\tLoss: 2.1874\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [12544/50000]\tLoss: 2.3185\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [12672/50000]\tLoss: 2.5817\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [12800/50000]\tLoss: 2.2271\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [12928/50000]\tLoss: 2.0200\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [13056/50000]\tLoss: 2.2068\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [13184/50000]\tLoss: 2.1895\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [13312/50000]\tLoss: 2.4924\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [13440/50000]\tLoss: 2.2619\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [13568/50000]\tLoss: 2.2138\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [13696/50000]\tLoss: 2.1738\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [13824/50000]\tLoss: 2.2243\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [13952/50000]\tLoss: 2.3036\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [14080/50000]\tLoss: 2.1770\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [14208/50000]\tLoss: 2.1666\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [14336/50000]\tLoss: 2.0119\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [14464/50000]\tLoss: 2.0391\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [14592/50000]\tLoss: 2.5490\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [14720/50000]\tLoss: 2.0449\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [14848/50000]\tLoss: 2.4008\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [14976/50000]\tLoss: 2.0940\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [15104/50000]\tLoss: 2.2048\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [15232/50000]\tLoss: 2.2555\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [15360/50000]\tLoss: 2.2508\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [15488/50000]\tLoss: 2.1877\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [15616/50000]\tLoss: 2.3029\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [15744/50000]\tLoss: 2.1218\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [15872/50000]\tLoss: 2.3500\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [16000/50000]\tLoss: 2.1937\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [16128/50000]\tLoss: 2.4420\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [16256/50000]\tLoss: 2.3308\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [16384/50000]\tLoss: 2.2322\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [16512/50000]\tLoss: 2.4909\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [16640/50000]\tLoss: 2.2623\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [16768/50000]\tLoss: 2.3809\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [16896/50000]\tLoss: 2.4123\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [17024/50000]\tLoss: 2.3708\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [17152/50000]\tLoss: 1.9564\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [17280/50000]\tLoss: 2.3389\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [17408/50000]\tLoss: 2.0265\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [17536/50000]\tLoss: 2.5046\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [17664/50000]\tLoss: 2.1535\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [17792/50000]\tLoss: 2.1569\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [17920/50000]\tLoss: 2.3849\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [18048/50000]\tLoss: 2.2780\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [18176/50000]\tLoss: 2.3113\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [18304/50000]\tLoss: 2.3559\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [18432/50000]\tLoss: 2.0964\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [18560/50000]\tLoss: 2.2094\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [18688/50000]\tLoss: 2.2308\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [18816/50000]\tLoss: 2.1323\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [18944/50000]\tLoss: 2.2895\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [19072/50000]\tLoss: 2.3802\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [19200/50000]\tLoss: 2.6398\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [19328/50000]\tLoss: 2.3200\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [19456/50000]\tLoss: 2.0712\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [19584/50000]\tLoss: 2.1760\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [19712/50000]\tLoss: 2.4955\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [19840/50000]\tLoss: 2.0025\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [19968/50000]\tLoss: 2.2183\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [20096/50000]\tLoss: 2.4267\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [20224/50000]\tLoss: 2.1940\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [20352/50000]\tLoss: 2.2989\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [20480/50000]\tLoss: 2.1102\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [20608/50000]\tLoss: 2.3820\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [20736/50000]\tLoss: 2.3251\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [20864/50000]\tLoss: 2.3471\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [20992/50000]\tLoss: 2.1557\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [21120/50000]\tLoss: 2.2822\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [21248/50000]\tLoss: 2.1953\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [21376/50000]\tLoss: 2.1733\tLR: 0.100000\n",
      "start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 9 [21504/50000]\tLoss: 2.1284\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [21632/50000]\tLoss: 2.0749\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [21760/50000]\tLoss: 2.1101\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [21888/50000]\tLoss: 2.2763\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [22016/50000]\tLoss: 2.2559\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [22144/50000]\tLoss: 2.3432\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [22272/50000]\tLoss: 2.4113\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [22400/50000]\tLoss: 2.3744\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [22528/50000]\tLoss: 2.1315\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [22656/50000]\tLoss: 2.1769\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [22784/50000]\tLoss: 2.0637\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [22912/50000]\tLoss: 2.5026\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [23040/50000]\tLoss: 2.1384\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [23168/50000]\tLoss: 2.5123\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [23296/50000]\tLoss: 2.3666\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [23424/50000]\tLoss: 2.0361\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [23552/50000]\tLoss: 2.1308\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [23680/50000]\tLoss: 2.1175\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [23808/50000]\tLoss: 2.2602\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [23936/50000]\tLoss: 2.3700\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [24064/50000]\tLoss: 2.0811\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [24192/50000]\tLoss: 2.4620\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [24320/50000]\tLoss: 2.0579\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [24448/50000]\tLoss: 2.4832\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [24576/50000]\tLoss: 2.1703\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [24704/50000]\tLoss: 2.1753\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [24832/50000]\tLoss: 2.1689\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [24960/50000]\tLoss: 2.3470\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [25088/50000]\tLoss: 2.5089\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [25216/50000]\tLoss: 2.3580\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [25344/50000]\tLoss: 2.3138\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [25472/50000]\tLoss: 2.1855\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [25600/50000]\tLoss: 2.2460\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [25728/50000]\tLoss: 2.1256\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [25856/50000]\tLoss: 2.2718\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [25984/50000]\tLoss: 2.0601\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [26112/50000]\tLoss: 2.3494\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [26240/50000]\tLoss: 2.4880\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [26368/50000]\tLoss: 2.3023\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [26496/50000]\tLoss: 2.2141\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [26624/50000]\tLoss: 2.2910\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [26752/50000]\tLoss: 2.3819\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [26880/50000]\tLoss: 2.2416\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [27008/50000]\tLoss: 2.1051\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [27136/50000]\tLoss: 2.0700\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [27264/50000]\tLoss: 2.2676\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [27392/50000]\tLoss: 2.1274\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [27520/50000]\tLoss: 2.0420\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [27648/50000]\tLoss: 2.5104\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [27776/50000]\tLoss: 2.2319\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [27904/50000]\tLoss: 2.3562\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [28032/50000]\tLoss: 2.0392\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [28160/50000]\tLoss: 2.2871\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [28288/50000]\tLoss: 2.3154\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [28416/50000]\tLoss: 2.3061\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [28544/50000]\tLoss: 2.0366\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [28672/50000]\tLoss: 2.2227\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [28800/50000]\tLoss: 2.0328\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [28928/50000]\tLoss: 2.0573\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [29056/50000]\tLoss: 1.9880\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [29184/50000]\tLoss: 2.0289\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [29312/50000]\tLoss: 2.0944\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [29440/50000]\tLoss: 2.3926\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [29568/50000]\tLoss: 2.2435\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [29696/50000]\tLoss: 2.0832\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [29824/50000]\tLoss: 2.2452\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [29952/50000]\tLoss: 2.1056\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [30080/50000]\tLoss: 2.1698\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [30208/50000]\tLoss: 2.2527\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [30336/50000]\tLoss: 2.2037\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [30464/50000]\tLoss: 2.3216\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [30592/50000]\tLoss: 2.0803\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [30720/50000]\tLoss: 2.0953\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [30848/50000]\tLoss: 2.3797\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [30976/50000]\tLoss: 2.3943\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [31104/50000]\tLoss: 2.2728\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [31232/50000]\tLoss: 2.1727\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [31360/50000]\tLoss: 2.0996\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [31488/50000]\tLoss: 2.1407\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [31616/50000]\tLoss: 2.3428\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [31744/50000]\tLoss: 1.9840\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [31872/50000]\tLoss: 2.3385\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [32000/50000]\tLoss: 2.0855\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [32128/50000]\tLoss: 2.2944\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [32256/50000]\tLoss: 2.1854\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [32384/50000]\tLoss: 2.3392\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [32512/50000]\tLoss: 2.2071\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [32640/50000]\tLoss: 2.3640\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [32768/50000]\tLoss: 2.2632\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [32896/50000]\tLoss: 2.2609\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [33024/50000]\tLoss: 2.0086\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [33152/50000]\tLoss: 2.2157\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [33280/50000]\tLoss: 2.3290\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [33408/50000]\tLoss: 2.1672\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [33536/50000]\tLoss: 2.1499\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [33664/50000]\tLoss: 2.1118\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [33792/50000]\tLoss: 2.1195\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [33920/50000]\tLoss: 2.3279\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [34048/50000]\tLoss: 2.4592\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [34176/50000]\tLoss: 2.2269\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [34304/50000]\tLoss: 1.9800\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [34432/50000]\tLoss: 2.2548\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [34560/50000]\tLoss: 2.2109\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [34688/50000]\tLoss: 2.4399\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [34816/50000]\tLoss: 2.2823\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [34944/50000]\tLoss: 2.2106\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [35072/50000]\tLoss: 2.2989\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [35200/50000]\tLoss: 2.3275\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [35328/50000]\tLoss: 2.5710\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [35456/50000]\tLoss: 2.0849\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [35584/50000]\tLoss: 2.2022\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [35712/50000]\tLoss: 2.0846\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [35840/50000]\tLoss: 1.9983\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [35968/50000]\tLoss: 2.1847\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [36096/50000]\tLoss: 2.3932\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [36224/50000]\tLoss: 2.4404\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [36352/50000]\tLoss: 2.2516\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [36480/50000]\tLoss: 2.3889\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [36608/50000]\tLoss: 2.2203\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [36736/50000]\tLoss: 2.1652\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [36864/50000]\tLoss: 2.2650\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [36992/50000]\tLoss: 2.2012\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [37120/50000]\tLoss: 1.9063\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [37248/50000]\tLoss: 2.0675\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [37376/50000]\tLoss: 2.4477\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [37504/50000]\tLoss: 2.1988\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [37632/50000]\tLoss: 2.1614\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [37760/50000]\tLoss: 2.0284\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [37888/50000]\tLoss: 2.1990\tLR: 0.100000\n",
      "start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 9 [38016/50000]\tLoss: 2.1794\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [38144/50000]\tLoss: 2.0034\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [38272/50000]\tLoss: 1.9220\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [38400/50000]\tLoss: 2.0620\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [38528/50000]\tLoss: 2.1572\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [38656/50000]\tLoss: 2.3200\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [38784/50000]\tLoss: 2.4750\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [38912/50000]\tLoss: 2.4286\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [39040/50000]\tLoss: 2.3397\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [39168/50000]\tLoss: 2.2500\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [39296/50000]\tLoss: 2.2188\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [39424/50000]\tLoss: 2.1259\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [39552/50000]\tLoss: 2.2087\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [39680/50000]\tLoss: 2.2523\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [39808/50000]\tLoss: 2.5027\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [39936/50000]\tLoss: 2.1544\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [40064/50000]\tLoss: 2.3178\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [40192/50000]\tLoss: 2.4728\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [40320/50000]\tLoss: 2.2851\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [40448/50000]\tLoss: 2.2412\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [40576/50000]\tLoss: 2.2388\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [40704/50000]\tLoss: 2.2583\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [40832/50000]\tLoss: 2.2048\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [40960/50000]\tLoss: 2.0110\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [41088/50000]\tLoss: 2.3348\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [41216/50000]\tLoss: 2.4459\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [41344/50000]\tLoss: 2.3166\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [41472/50000]\tLoss: 2.4312\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [41600/50000]\tLoss: 2.1188\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [41728/50000]\tLoss: 2.1670\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [41856/50000]\tLoss: 2.1894\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [41984/50000]\tLoss: 2.1103\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [42112/50000]\tLoss: 2.1359\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [42240/50000]\tLoss: 2.2139\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [42368/50000]\tLoss: 2.0650\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [42496/50000]\tLoss: 2.2604\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [42624/50000]\tLoss: 2.2490\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [42752/50000]\tLoss: 2.0267\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [42880/50000]\tLoss: 1.9617\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [43008/50000]\tLoss: 2.4109\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [43136/50000]\tLoss: 2.3881\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [43264/50000]\tLoss: 2.2085\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [43392/50000]\tLoss: 2.0788\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [43520/50000]\tLoss: 2.2750\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [43648/50000]\tLoss: 2.2573\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [43776/50000]\tLoss: 2.0216\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [43904/50000]\tLoss: 2.7595\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [44032/50000]\tLoss: 2.1942\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [44160/50000]\tLoss: 2.3796\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [44288/50000]\tLoss: 2.2311\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [44416/50000]\tLoss: 2.2166\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [44544/50000]\tLoss: 2.5318\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [44672/50000]\tLoss: 2.2215\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [44800/50000]\tLoss: 2.3088\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [44928/50000]\tLoss: 2.0517\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [45056/50000]\tLoss: 2.4188\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [45184/50000]\tLoss: 2.3148\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [45312/50000]\tLoss: 2.0929\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [45440/50000]\tLoss: 2.3587\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [45568/50000]\tLoss: 2.2183\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [45696/50000]\tLoss: 2.3479\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [45824/50000]\tLoss: 2.4528\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [45952/50000]\tLoss: 2.1198\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [46080/50000]\tLoss: 2.1525\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [46208/50000]\tLoss: 2.0570\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [46336/50000]\tLoss: 2.1691\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [46464/50000]\tLoss: 2.2832\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [46592/50000]\tLoss: 2.4396\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [46720/50000]\tLoss: 2.1707\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [46848/50000]\tLoss: 2.0311\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [46976/50000]\tLoss: 2.1218\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [47104/50000]\tLoss: 2.1482\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [47232/50000]\tLoss: 2.1874\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [47360/50000]\tLoss: 2.3041\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [47488/50000]\tLoss: 2.3429\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [47616/50000]\tLoss: 2.0339\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [47744/50000]\tLoss: 1.8419\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [47872/50000]\tLoss: 2.1146\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [48000/50000]\tLoss: 2.3614\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [48128/50000]\tLoss: 2.2684\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [48256/50000]\tLoss: 2.2197\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [48384/50000]\tLoss: 2.2723\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [48512/50000]\tLoss: 2.3810\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [48640/50000]\tLoss: 2.0652\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [48768/50000]\tLoss: 2.1390\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [48896/50000]\tLoss: 2.0842\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [49024/50000]\tLoss: 2.2993\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [49152/50000]\tLoss: 2.0742\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [49280/50000]\tLoss: 2.1321\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [49408/50000]\tLoss: 2.1478\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [49536/50000]\tLoss: 2.0961\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [49664/50000]\tLoss: 2.2817\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [49792/50000]\tLoss: 2.5054\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [49920/50000]\tLoss: 2.2281\tLR: 0.100000\n",
      "start\n",
      "Training Epoch: 9 [50000/50000]\tLoss: 2.3883\tLR: 0.100000\n",
      "Trained in:  2525.1176977157593\n"
     ]
    }
   ],
   "source": [
    "net = nasnet()\n",
    "net = net.cuda()\n",
    "k = '1'\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "train_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[60, 120, 160], gamma=0.2) #learning rate decay\n",
    "iter_per_epoch = len(trainloader)\n",
    "warmup_scheduler = WarmUpLR(optimizer, iter_per_epoch * w)\n",
    "checkpoint_path = os.path.join('checkpoint', 'nasnet', k)\n",
    "\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "checkpoint_path = os.path.join(checkpoint_path, '{net}-{epoch}-{type}.pth')\n",
    "\n",
    "best_acc = 0.0\n",
    "start = time.time()\n",
    "for epoch in range(1,max_epochs):\n",
    "    if epoch > 1:\n",
    "         train_scheduler.step(epoch)\n",
    "    train(epoch)\n",
    "end = time.time()\n",
    "print('Trained in: ',str(end-start))\n",
    "#acc = eval_training(epoch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.08529496192932 minutes\n"
     ]
    }
   ],
   "source": [
    "print((end-start)/60,'minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Results</h3>\n",
    "Trained for up to 9 epochs on Kaggle. Time taken ~ 42 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    }
   ],
   "source": [
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': net.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, 'weights_9ep.pth')\n",
    "print('saved')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
